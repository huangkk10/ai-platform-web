"""
å–®ä¸€æ¸¬è©¦æ¡ˆä¾‹å¤šåŸ·è¡Œç·’ç‰ˆæœ¬æ¸¬è©¦å™¨ (Single Case Multithread Tester)
================================================================

ç”¨æ–¼å°å–®ä¸€æ¸¬è©¦æ¡ˆä¾‹ï¼Œä½¿ç”¨å¤šåŸ·è¡Œç·’ä¸¦è¡Œæ¸¬è©¦æŒ‡å®šçš„æœå°‹ç‰ˆæœ¬ã€‚

ä½¿ç”¨å ´æ™¯ï¼š
- å¿«é€Ÿæ¸¬è©¦å–®ä¸€å•é¡Œåœ¨å¤šå€‹ç‰ˆæœ¬çš„è¡¨ç¾
- éœ€è¦ç”¨æˆ¶é¸æ“‡ç‰¹å®šç‰ˆæœ¬æ¸¬è©¦
- éœ€è¦ä¸¦è¡ŒåŸ·è¡Œä»¥åŠ é€Ÿæ¸¬è©¦

æ™‚é–“å„ªå‹¢ï¼ˆç›¸æ¯”é †åºåŸ·è¡Œï¼‰ï¼š
- 3 å€‹ç‰ˆæœ¬ï¼š~9 ç§’ â†’ ~3 ç§’ (3x åŠ é€Ÿ)
- 5 å€‹ç‰ˆæœ¬ï¼š~15 ç§’ â†’ ~5 ç§’ (3x åŠ é€Ÿ)
- 10 å€‹ç‰ˆæœ¬ï¼š~30 ç§’ â†’ ~10 ç§’ (3x åŠ é€Ÿ)
"""

import logging
from datetime import datetime
from typing import List, Dict, Any, Optional
from decimal import Decimal
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

logger = logging.getLogger(__name__)

# åŸ·è¡Œç·’æœ¬åœ°å­˜å„²ï¼Œç”¨æ–¼ Django è³‡æ–™åº«é€£æ¥ç®¡ç†
_thread_local = threading.local()


class SingleCaseMultithreadTester:
    """å–®ä¸€æ¸¬è©¦æ¡ˆä¾‹å¤šåŸ·è¡Œç·’ç‰ˆæœ¬æ¸¬è©¦å™¨"""
    
    def __init__(
        self, 
        test_case_id: int, 
        version_ids: List[int], 
        max_workers: int = 3,
        verbose: bool = False
    ):
        """
        åˆå§‹åŒ–æ¸¬è©¦å™¨
        
        Args:
            test_case_id: æ¸¬è©¦æ¡ˆä¾‹ ID
            version_ids: è¦æ¸¬è©¦çš„ç‰ˆæœ¬ ID åˆ—è¡¨ï¼ˆå¿…é ˆæŒ‡å®šï¼‰
            max_workers: æœ€å¤§ä¸¦è¡ŒåŸ·è¡Œç·’æ•¸ï¼ˆé è¨­ 3ï¼Œå»ºè­° 1-5ï¼‰
            verbose: æ˜¯å¦è¼¸å‡ºè©³ç´°æ—¥èªŒ
        """
        if not version_ids:
            raise ValueError("version_ids ä¸èƒ½ç‚ºç©ºï¼Œå¿…é ˆæŒ‡å®šè¦æ¸¬è©¦çš„ç‰ˆæœ¬")
        
        self.test_case_id = test_case_id
        self.version_ids = version_ids
        self.max_workers = min(max_workers, len(version_ids), 5)  # é™åˆ¶æœ€å¤§ 5 å€‹åŸ·è¡Œç·’
        self.verbose = verbose
        self.test_case = None
        self.versions = []
        
        # åŸ·è¡Œç·’å®‰å…¨çš„çµæœæ”¶é›†
        self._results_lock = threading.Lock()
        self._results = []
        self._test_run_ids = []
    
    def run_test(self) -> Dict[str, Any]:
        """
        åŸ·è¡Œå¤šåŸ·è¡Œç·’æ¸¬è©¦
        
        Returns:
            Dict: æ¸¬è©¦çµæœ
                {
                    'success': bool,
                    'test_case': {...},
                    'results': [
                        {
                            'version_id': int,
                            'version_name': str,
                            'strategy_type': str,
                            'metrics': {
                                'precision': float,
                                'recall': float,
                                'f1_score': float
                            },
                            'response_time': float,
                            'matched_keywords': List[str],
                            'total_keywords': int,
                            'status': 'success' | 'error',
                            'test_run_id': int (if success)
                        },
                        ...
                    ],
                    'summary': {
                        'total_versions': int,
                        'successful_tests': int,
                        'failed_tests': int,
                        'best_version': {...},
                        'avg_response_time': float,
                        'total_execution_time': float,
                        'test_run_ids': List[int],
                        'max_workers_used': int
                    }
                }
        """
        try:
            self._log(f"\n{'='*80}")
            self._log(f"ğŸš€ é–‹å§‹å¤šåŸ·è¡Œç·’ç‰ˆæœ¬æ¸¬è©¦")
            self._log(f"{'='*80}")
            
            # 1. æº–å‚™æ¸¬è©¦æ¡ˆä¾‹å’Œç‰ˆæœ¬
            self._log(f"ğŸ“‹ æº–å‚™æ¸¬è©¦æ¡ˆä¾‹ (ID: {self.test_case_id})...")
            self._prepare_test_case()
            
            self._log(f"ğŸ“‹ æº–å‚™æ¸¬è©¦ç‰ˆæœ¬ (æŒ‡å®š {len(self.version_ids)} å€‹)...")
            self._prepare_versions()
            
            if not self.test_case:
                return {
                    'success': False,
                    'error': f'æ‰¾ä¸åˆ°æ¸¬è©¦æ¡ˆä¾‹ ID: {self.test_case_id}'
                }
            
            if not self.versions:
                return {
                    'success': False,
                    'error': 'æ²’æœ‰æ‰¾åˆ°æŒ‡å®šçš„ç‰ˆæœ¬ï¼Œè«‹ç¢ºèªç‰ˆæœ¬ ID æ˜¯å¦æ­£ç¢º'
                }
            
            self._log(f"âœ… æ¸¬è©¦æ¡ˆä¾‹: {self.test_case.question[:80]}...")
            self._log(f"âœ… å…± {len(self.versions)} å€‹ç‰ˆæœ¬: {[v.version_name for v in self.versions]}")
            self._log(f"âœ… æœ€å¤§ä¸¦è¡Œæ•¸: {self.max_workers}")
            
            # 2. ä½¿ç”¨ ThreadPoolExecutor ä¸¦è¡ŒåŸ·è¡Œæ¸¬è©¦
            start_time = datetime.now()
            
            self._log(f"\n{'â”€'*80}")
            self._log(f"âš¡ é–‹å§‹ä¸¦è¡Œæ¸¬è©¦ï¼ˆ{self.max_workers} å€‹åŸ·è¡Œç·’ï¼‰")
            self._log(f"{'â”€'*80}")
            
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                # æäº¤æ‰€æœ‰ç‰ˆæœ¬çš„æ¸¬è©¦ä»»å‹™
                future_to_version = {
                    executor.submit(self._test_single_version_safe, version): version
                    for version in self.versions
                }
                
                # æ”¶é›†çµæœ
                completed_count = 0
                for future in as_completed(future_to_version):
                    version = future_to_version[future]
                    completed_count += 1
                    
                    try:
                        result = future.result(timeout=120)  # 2 åˆ†é˜è¶…æ™‚
                        with self._results_lock:
                            self._results.append(result)
                            if result.get('test_run_id'):
                                self._test_run_ids.append(result['test_run_id'])
                        
                        status = "âœ…" if result.get('status') == 'success' else "âŒ"
                        f1 = result.get('metrics', {}).get('f1_score', 0)
                        self._log(f"  [{completed_count}/{len(self.versions)}] {status} {version.version_name}: F1={f1:.2%}")
                        
                    except Exception as e:
                        error_result = {
                            'version_id': version.id,
                            'version_name': version.version_name,
                            'strategy_type': version.parameters.get('strategy', version.algorithm_type),
                            'status': 'error',
                            'error_message': str(e),
                            'metrics': {
                                'precision': 0.0,
                                'recall': 0.0,
                                'f1_score': 0.0
                            },
                            'response_time': 0.0,
                            'matched_keywords': [],
                            'total_keywords': 0
                        }
                        with self._results_lock:
                            self._results.append(error_result)
                        
                        self._log(f"  [{completed_count}/{len(self.versions)}] âŒ {version.version_name}: {str(e)}", level='error')
            
            total_time = (datetime.now() - start_time).total_seconds()
            
            # 3. ç”Ÿæˆæ‘˜è¦
            summary = self._generate_summary(total_time)
            
            self._log(f"\n{'='*80}")
            self._log(f"âœ… å¤šåŸ·è¡Œç·’æ¸¬è©¦å®Œæˆï¼")
            self._log(f"   ç¸½æ™‚é–“: {total_time:.2f} ç§’")
            self._log(f"   æˆåŠŸ: {summary['successful_tests']}/{summary['total_versions']}")
            if summary.get('best_version'):
                self._log(f"   æœ€ä½³ç‰ˆæœ¬: {summary['best_version']['version_name']} (F1: {summary['best_version']['metrics']['f1_score']:.2%})")
            self._log(f"{'='*80}")
            
            return {
                'success': True,
                'test_case': {
                    'id': self.test_case.id,
                    'question': self.test_case.question,
                    'difficulty_level': self.test_case.difficulty_level,
                    'expected_keywords': self.test_case.expected_keywords
                },
                'results': self._results,
                'summary': summary
            }
            
        except Exception as e:
            error_msg = f"å¤šåŸ·è¡Œç·’ç‰ˆæœ¬æ¸¬è©¦å¤±æ•—: {str(e)}"
            self._log(error_msg, level='error')
            import traceback
            traceback.print_exc()
            return {
                'success': False,
                'error': error_msg
            }
    
    def _prepare_test_case(self):
        """æº–å‚™æ¸¬è©¦æ¡ˆä¾‹"""
        from api.models import BenchmarkTestCase
        
        try:
            self.test_case = BenchmarkTestCase.objects.get(
                id=self.test_case_id,
                is_active=True
            )
        except BenchmarkTestCase.DoesNotExist:
            self._log(f"æ¸¬è©¦æ¡ˆä¾‹ä¸å­˜åœ¨æˆ–å·²åœç”¨: {self.test_case_id}", level='warning')
            self.test_case = None
    
    def _prepare_versions(self):
        """æº–å‚™è¦æ¸¬è©¦çš„ç‰ˆæœ¬"""
        from api.models import SearchAlgorithmVersion
        
        # åªç²å–æŒ‡å®šä¸”å•Ÿç”¨çš„ç‰ˆæœ¬
        self.versions = list(
            SearchAlgorithmVersion.objects.filter(
                id__in=self.version_ids,
                is_active=True
            ).order_by('id')
        )
        
        # æª¢æŸ¥æ˜¯å¦æœ‰æ‰¾ä¸åˆ°çš„ç‰ˆæœ¬
        found_ids = {v.id for v in self.versions}
        missing_ids = set(self.version_ids) - found_ids
        if missing_ids:
            self._log(f"âš ï¸ ä»¥ä¸‹ç‰ˆæœ¬ ID æœªæ‰¾åˆ°æˆ–å·²åœç”¨: {missing_ids}", level='warning')
    
    def _test_single_version_safe(self, version) -> Dict[str, Any]:
        """
        æ¸¬è©¦å–®ä¸€ç‰ˆæœ¬ï¼ˆåŸ·è¡Œç·’å®‰å…¨ç‰ˆæœ¬ï¼‰
        
        ç¢ºä¿æ¯å€‹åŸ·è¡Œç·’éƒ½æœ‰ç¨ç«‹çš„è³‡æ–™åº«é€£æ¥
        
        Args:
            version: SearchAlgorithmVersion å¯¦ä¾‹
            
        Returns:
            Dict: æ¸¬è©¦çµæœ
        """
        # ç¢ºä¿ Django è³‡æ–™åº«é€£æ¥åœ¨æ–°åŸ·è¡Œç·’ä¸­æ­£ç¢ºåˆå§‹åŒ–
        from django.db import connection
        connection.ensure_connection()
        
        try:
            return self._test_single_version(version)
        finally:
            # æ¸…ç†åŸ·è¡Œç·’çš„è³‡æ–™åº«é€£æ¥
            connection.close()
    
    def _test_single_version(self, version) -> Dict[str, Any]:
        """
        æ¸¬è©¦å–®ä¸€ç‰ˆæœ¬
        
        Args:
            version: SearchAlgorithmVersion å¯¦ä¾‹
            
        Returns:
            Dict: æ¸¬è©¦çµæœ
        """
        from library.protocol_guide.search_service import ProtocolGuideSearchService
        from library.benchmark.search_strategies import get_strategy
        
        start_time = datetime.now()
        thread_name = threading.current_thread().name
        
        self._log(f"[{thread_name}] ğŸ§ª é–‹å§‹æ¸¬è©¦ç‰ˆæœ¬ï¼š{version.version_name}")
        
        # 1. ç²å–ç­–ç•¥é¡å‹
        strategy_type = version.parameters.get('strategy') or version.algorithm_type
        if not strategy_type:
            raise ValueError(f"ç‰ˆæœ¬ {version.version_name} ç¼ºå°‘ç­–ç•¥é¡å‹é…ç½®")
        
        # 2. æº–å‚™ç­–ç•¥åƒæ•¸
        strategy_params = {k: v for k, v in version.parameters.items() if k != 'strategy'}
        
        # 3. ç²å–æœå°‹ç­–ç•¥
        strategy = get_strategy(
            strategy_type,
            ProtocolGuideSearchService()
        )
        
        # 4. åŸ·è¡Œæœå°‹
        search_results = strategy.execute(
            query=self.test_case.question,
            limit=10,
            **strategy_params
        )
        
        # 5. è©•ä¼°çµæœ
        metrics = self._evaluate_results(search_results)
        
        response_time = (datetime.now() - start_time).total_seconds()
        
        # 6. å„²å­˜çµæœåˆ°è³‡æ–™åº«
        test_run = self._save_test_result(
            version=version,
            metrics=metrics,
            response_time=response_time,
            search_results=search_results
        )
        
        self._log(f"[{thread_name}] âœ… {version.version_name} å®Œæˆ - F1: {metrics['f1_score']:.2%}, è€—æ™‚: {response_time:.2f}s")
        
        return {
            'version_id': version.id,
            'version_name': version.version_name,
            'strategy_type': strategy_type,
            'metrics': {
                'precision': float(metrics['precision']),
                'recall': float(metrics['recall']),
                'f1_score': float(metrics['f1_score'])
            },
            'response_time': response_time,
            'matched_keywords': metrics.get('matched_keywords', []),
            'total_keywords': metrics.get('total_keywords', 0),
            'status': 'success',
            'test_run_id': test_run.id if test_run else None
        }
    
    def _evaluate_results(self, search_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        è©•ä¼°æœå°‹çµæœ
        
        Args:
            search_results: æœå°‹çµæœåˆ—è¡¨
            
        Returns:
            Dict: è©•ä¼°æŒ‡æ¨™
        """
        if not search_results:
            return {
                'precision': Decimal('0'),
                'recall': Decimal('0'),
                'f1_score': Decimal('0'),
                'matched_keywords': [],
                'total_keywords': len(self.test_case.expected_keywords) if self.test_case.expected_keywords else 0
            }
        
        # ç²å–ç­”æ¡ˆé—œéµå­—
        answer_keywords = self.test_case.expected_keywords or []
        if not answer_keywords:
            return {
                'precision': Decimal('0'),
                'recall': Decimal('0'),
                'f1_score': Decimal('0'),
                'matched_keywords': [],
                'total_keywords': 0
            }
        
        # æå–æœå°‹çµæœçš„å…§å®¹
        result_texts = []
        for result in search_results:
            text = ""
            if 'title' in result:
                text += result['title'] + " "
            if 'content' in result:
                text += result['content']
            result_texts.append(text)
        
        combined_text = " ".join(result_texts)
        
        # è¨ˆç®—åŒ¹é…çš„é—œéµå­—
        matched_keywords = []
        for keyword in answer_keywords:
            if keyword.lower() in combined_text.lower():
                matched_keywords.append(keyword)
        
        # è¨ˆç®—æŒ‡æ¨™
        total_keywords = len(answer_keywords)
        matched_count = len(matched_keywords)
        
        recall = Decimal(matched_count) / Decimal(total_keywords) if total_keywords > 0 else Decimal('0')
        precision = recall
        
        if precision + recall > 0:
            f1_score = (2 * precision * recall) / (precision + recall)
        else:
            f1_score = Decimal('0')
        
        return {
            'precision': precision,
            'recall': recall,
            'f1_score': f1_score,
            'matched_keywords': matched_keywords,
            'total_keywords': total_keywords
        }
    
    def _save_test_result(
        self,
        version,
        metrics: Dict[str, Any],
        response_time: float,
        search_results: List[Dict[str, Any]]
    ):
        """
        å„²å­˜æ¸¬è©¦çµæœåˆ°è³‡æ–™åº«
        
        Args:
            version: SearchAlgorithmVersion å¯¦ä¾‹
            metrics: è©•ä¼°æŒ‡æ¨™
            response_time: å›æ‡‰æ™‚é–“ï¼ˆç§’ï¼‰
            search_results: æœå°‹çµæœ
            
        Returns:
            BenchmarkTestRun: æ¸¬è©¦é‹è¡Œè¨˜éŒ„
        """
        from api.models import BenchmarkTestRun, BenchmarkTestResult
        
        try:
            # å‰µå»ºæ¸¬è©¦é‹è¡Œè¨˜éŒ„
            test_run = BenchmarkTestRun.objects.create(
                version=version,
                run_name=f"é¸æ“‡ç‰ˆæœ¬æ¸¬è©¦ - {self.test_case.question[:30]}...",
                run_type='selected_version_test',
                total_test_cases=1,
                completed_test_cases=1,
                status='completed',
                avg_precision=metrics['precision'],
                avg_recall=metrics['recall'],
                avg_f1_score=metrics['f1_score'],
                overall_score=metrics['f1_score'],
                avg_response_time=Decimal(str(response_time))
            )
            
            # å‰µå»ºæ¸¬è©¦çµæœè¨˜éŒ„
            matched_keywords = metrics.get('matched_keywords', [])
            total_keywords = metrics.get('total_keywords', 0)
            
            BenchmarkTestResult.objects.create(
                test_run=test_run,
                test_case=self.test_case,
                search_query=self.test_case.question,
                returned_document_ids=[r.get('id', 0) for r in search_results[:10]],
                returned_document_scores=[float(r.get('score', 0)) for r in search_results[:10]],
                precision_score=metrics['precision'],
                recall_score=metrics['recall'],
                f1_score=metrics['f1_score'],
                response_time=Decimal(str(response_time * 1000)),  # æ¯«ç§’
                true_positives=len(matched_keywords),
                false_negatives=total_keywords - len(matched_keywords),
                is_passed=metrics['f1_score'] > Decimal('0.5'),
                pass_reason=f"åŒ¹é… {len(matched_keywords)}/{total_keywords} å€‹é—œéµå­—",
                detailed_results={
                    'search_results': search_results[:5],
                    'matched_keywords': matched_keywords,
                    'total_keywords': total_keywords,
                    'version_name': version.version_name,
                    'strategy_type': version.parameters.get('strategy', version.algorithm_type),
                    'test_type': 'selected_version_multithread'
                }
            )
            
            return test_run
            
        except Exception as e:
            self._log(f"å„²å­˜æ¸¬è©¦çµæœå¤±æ•—: {str(e)}", level='error')
            return None
    
    def _generate_summary(self, total_time: float) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ¸¬è©¦æ‘˜è¦
        
        Args:
            total_time: ç¸½åŸ·è¡Œæ™‚é–“ï¼ˆç§’ï¼‰
            
        Returns:
            Dict: æ¸¬è©¦æ‘˜è¦
        """
        successful_results = [r for r in self._results if r.get('status') == 'success']
        failed_results = [r for r in self._results if r.get('status') == 'error']
        
        # æ‰¾å‡ºæœ€ä½³ç‰ˆæœ¬
        best_version = None
        if successful_results:
            best_version = max(
                successful_results,
                key=lambda x: x['metrics']['f1_score']
            )
        
        # è¨ˆç®—å¹³å‡å›æ‡‰æ™‚é–“
        avg_response_time = 0.0
        if successful_results:
            avg_response_time = sum(
                r['response_time'] for r in successful_results
            ) / len(successful_results)
        
        return {
            'total_versions': len(self._results),
            'successful_tests': len(successful_results),
            'failed_tests': len(failed_results),
            'best_version': best_version,
            'avg_response_time': round(avg_response_time, 2),
            'total_execution_time': round(total_time, 2),
            'test_run_ids': self._test_run_ids,
            'max_workers_used': self.max_workers
        }
    
    def _log(self, message: str, level: str = 'info'):
        """è¼¸å‡ºæ—¥èªŒ"""
        if self.verbose:
            print(f"[MultithreadTester] {message}")
        
        log_func = getattr(logger, level, logger.info)
        log_func(f"[SingleCaseMultithreadTester] {message}")


# ===================== ä¾¿åˆ©å‡½æ•¸ =====================

def test_single_case_selected_versions_multithread(
    test_case_id: int,
    version_ids: List[int],
    max_workers: int = 3,
    verbose: bool = False
) -> Dict[str, Any]:
    """
    ä½¿ç”¨å¤šåŸ·è¡Œç·’æ¸¬è©¦å–®ä¸€æ¡ˆä¾‹çš„æŒ‡å®šç‰ˆæœ¬
    
    Args:
        test_case_id: æ¸¬è©¦æ¡ˆä¾‹ ID
        version_ids: ç‰ˆæœ¬ ID åˆ—è¡¨
        max_workers: æœ€å¤§ä¸¦è¡Œæ•¸ï¼ˆé è¨­ 3ï¼‰
        verbose: æ˜¯å¦è¼¸å‡ºè©³ç´°æ—¥èªŒ
        
    Returns:
        Dict: æ¸¬è©¦çµæœ
    """
    tester = SingleCaseMultithreadTester(
        test_case_id=test_case_id,
        version_ids=version_ids,
        max_workers=max_workers,
        verbose=verbose
    )
    return tester.run_test()
