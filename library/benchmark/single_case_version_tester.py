"""
å–®ä¸€æ¸¬è©¦æ¡ˆä¾‹ç‰ˆæœ¬æ¯”è¼ƒæ¸¬è©¦å™¨ (Single Case Version Tester)
=======================================================

ç”¨æ–¼å°å–®ä¸€æ¸¬è©¦æ¡ˆä¾‹åŸ·è¡Œå¤šå€‹æœå°‹ç‰ˆæœ¬çš„æ¯”è¼ƒæ¸¬è©¦ã€‚

ä½¿ç”¨å ´æ™¯ï¼š
- å¿«é€Ÿæ¸¬è©¦å–®ä¸€å•é¡Œåœ¨ä¸åŒç‰ˆæœ¬çš„è¡¨ç¾
- è¨ºæ–·ç‰¹å®šå•é¡Œçš„æœ€ä½³æœå°‹ç­–ç•¥
- é©—è­‰é—œéµå­—èª¿æ•´æ•ˆæœ

æ™‚é–“å„ªå‹¢ï¼š
- å–®å•é¡Œ Ã— 5 ç‰ˆæœ¬ = 20-30 ç§’
- å®Œæ•´æ‰¹é‡æ¸¬è©¦ = 40-50 åˆ†é˜
- ç¯€çœ 99.2% æ™‚é–“
"""

import logging
from datetime import datetime
from typing import List, Dict, Any, Optional
from decimal import Decimal

logger = logging.getLogger(__name__)


class SingleCaseVersionTester:
    """å–®ä¸€æ¸¬è©¦æ¡ˆä¾‹ç‰ˆæœ¬æ¯”è¼ƒæ¸¬è©¦å™¨"""
    
    def __init__(self, test_case_id: int, version_ids: Optional[List[int]] = None, verbose: bool = False):
        """
        åˆå§‹åŒ–æ¸¬è©¦å™¨
        
        Args:
            test_case_id: æ¸¬è©¦æ¡ˆä¾‹ ID
            version_ids: è¦æ¸¬è©¦çš„ç‰ˆæœ¬ ID åˆ—è¡¨ï¼ˆNone = æ¸¬è©¦æ‰€æœ‰å•Ÿç”¨ç‰ˆæœ¬ï¼‰
            verbose: æ˜¯å¦è¼¸å‡ºè©³ç´°æ—¥èªŒ
        """
        self.test_case_id = test_case_id
        self.version_ids = version_ids
        self.verbose = verbose
        self.test_case = None
        self.versions = []
    
    def run_comparison(self) -> Dict[str, Any]:
        """
        åŸ·è¡Œç‰ˆæœ¬æ¯”è¼ƒæ¸¬è©¦
        
        Returns:
            Dict: æ¸¬è©¦çµæœ
                {
                    'success': bool,
                    'test_case': {...},
                    'results': [
                        {
                            'version_id': int,
                            'version_name': str,
                            'metrics': {
                                'precision': float,
                                'recall': float,
                                'f1_score': float
                            },
                            'response_time': float,
                            'status': 'success' | 'error',
                            'error_message': str (if error)
                        },
                        ...
                    ],
                    'summary': {
                        'total_versions': int,
                        'successful_tests': int,
                        'failed_tests': int,
                        'best_version': {...},
                        'avg_response_time': float,
                        'test_run_ids': [int, ...]
                    }
                }
        """
        try:
            print(f"\n{'='*80}")
            print(f"ğŸš€ é–‹å§‹ç‰ˆæœ¬æ¯”è¼ƒæ¸¬è©¦")
            print(f"{'='*80}")
            
            # 1. æº–å‚™æ¸¬è©¦æ¡ˆä¾‹å’Œç‰ˆæœ¬
            print(f"ğŸ“‹ æº–å‚™æ¸¬è©¦æ¡ˆä¾‹ (ID: {self.test_case_id})...")
            self._prepare_test_case()
            
            print(f"ğŸ“‹ æº–å‚™æ¸¬è©¦ç‰ˆæœ¬...")
            self._prepare_versions()
            
            if not self.test_case:
                print(f"âŒ éŒ¯èª¤: æ‰¾ä¸åˆ°æ¸¬è©¦æ¡ˆä¾‹ ID: {self.test_case_id}")
                return {
                    'success': False,
                    'error': f'æ‰¾ä¸åˆ°æ¸¬è©¦æ¡ˆä¾‹ ID: {self.test_case_id}'
                }
            
            if not self.versions:
                print(f"âŒ éŒ¯èª¤: æ²’æœ‰å¯ç”¨çš„æ¸¬è©¦ç‰ˆæœ¬")
                return {
                    'success': False,
                    'error': 'æ²’æœ‰å¯ç”¨çš„æ¸¬è©¦ç‰ˆæœ¬'
                }
            
            print(f"âœ… æ¸¬è©¦æ¡ˆä¾‹: {self.test_case.question[:80]}...")
            print(f"âœ… å…± {len(self.versions)} å€‹ç‰ˆæœ¬: {[v.version_name for v in self.versions]}")
            print(f"âœ… é æœŸé—œéµå­—: {self.test_case.expected_keywords}")
            print(f"âœ… é›£åº¦ç­‰ç´š: {self.test_case.difficulty_level}")
            
            self._log(f"é–‹å§‹æ¸¬è©¦å•é¡Œ: {self.test_case.question[:50]}...")
            self._log(f"æ¸¬è©¦ {len(self.versions)} å€‹ç‰ˆæœ¬")
            
            # 2. åŸ·è¡Œæ¯å€‹ç‰ˆæœ¬çš„æ¸¬è©¦
            results = []
            test_run_ids = []
            start_time = datetime.now()
            
            for idx, version in enumerate(self.versions, 1):
                print(f"\n{'â”€'*80}")
                print(f"ğŸ“Š é€²åº¦: [{idx}/{len(self.versions)}]")
                print(f"{'â”€'*80}")
                self._log(f"[{idx}/{len(self.versions)}] æ¸¬è©¦ç‰ˆæœ¬: {version.version_name}")
                
                try:
                    result = self._test_single_version(version)
                    results.append(result)
                    
                    if result.get('test_run_id'):
                        test_run_ids.append(result['test_run_id'])
                    
                    self._log(f"  âœ… å®Œæˆ - F1: {result['metrics']['f1_score']:.2%}")
                    
                except Exception as e:
                    error_msg = str(e)
                    print(f"âŒ ç‰ˆæœ¬ {version.version_name} æ¸¬è©¦å¤±æ•—!")
                    print(f"   éŒ¯èª¤è¨Šæ¯: {error_msg}")
                    import traceback
                    print(f"   å®Œæ•´å †ç–Š:")
                    traceback.print_exc()
                    
                    self._log(f"  âŒ å¤±æ•—: {error_msg}", level='error')
                    results.append({
                        'version_id': version.id,
                        'version_name': version.version_name,
                        'status': 'error',
                        'error_message': error_msg,
                        'metrics': {
                            'precision': 0.0,
                            'recall': 0.0,
                            'f1_score': 0.0
                        },
                        'response_time': 0.0
                    })
            
            total_time = (datetime.now() - start_time).total_seconds()
            
            # 3. ç”Ÿæˆæ‘˜è¦
            summary = self._generate_summary(results, total_time, test_run_ids)
            
            self._log(f"æ¸¬è©¦å®Œæˆï¼ç¸½æ™‚é–“: {total_time:.2f} ç§’")
            
            return {
                'success': True,
                'test_case': {
                    'id': self.test_case.id,
                    'question': self.test_case.question,
                    'difficulty_level': self.test_case.difficulty_level,
                    'expected_keywords': self.test_case.expected_keywords
                },
                'results': results,
                'summary': summary
            }
            
        except Exception as e:
            error_msg = f"ç‰ˆæœ¬æ¯”è¼ƒæ¸¬è©¦å¤±æ•—: {str(e)}"
            self._log(error_msg, level='error')
            return {
                'success': False,
                'error': error_msg
            }
    
    def _prepare_test_case(self):
        """æº–å‚™æ¸¬è©¦æ¡ˆä¾‹"""
        from api.models import BenchmarkTestCase
        
        try:
            self.test_case = BenchmarkTestCase.objects.get(
                id=self.test_case_id,
                is_active=True
            )
        except BenchmarkTestCase.DoesNotExist:
            self._log(f"æ¸¬è©¦æ¡ˆä¾‹ä¸å­˜åœ¨æˆ–å·²åœç”¨: {self.test_case_id}", level='warning')
            self.test_case = None
    
    def _prepare_versions(self):
        """æº–å‚™è¦æ¸¬è©¦çš„ç‰ˆæœ¬"""
        from api.models import SearchAlgorithmVersion
        
        if self.version_ids:
            # æ¸¬è©¦æŒ‡å®šçš„ç‰ˆæœ¬
            self.versions = list(
                SearchAlgorithmVersion.objects.filter(
                    id__in=self.version_ids,
                    is_active=True
                ).order_by('id')
            )
        else:
            # æ¸¬è©¦æ‰€æœ‰å•Ÿç”¨çš„ç‰ˆæœ¬
            self.versions = list(
                SearchAlgorithmVersion.objects.filter(
                    is_active=True
                ).order_by('id')
            )
    
    def _test_single_version(self, version) -> Dict[str, Any]:
        """
        æ¸¬è©¦å–®ä¸€ç‰ˆæœ¬
        
        Args:
            version: SearchAlgorithmVersion å¯¦ä¾‹
            
        Returns:
            Dict: æ¸¬è©¦çµæœ
        """
        from api.models import BenchmarkTestRun, BenchmarkTestResult
        from library.protocol_guide.search_service import ProtocolGuideSearchService
        from library.benchmark.search_strategies import get_strategy
        
        start_time = datetime.now()
        
        print(f"\n{'='*80}")
        print(f"ğŸ§ª é–‹å§‹æ¸¬è©¦ç‰ˆæœ¬ï¼š{version.version_name} (ID: {version.id})")
        print(f"{'='*80}")
        
        # 1. ç²å–ç­–ç•¥é¡å‹ï¼ˆå„ªå…ˆä½¿ç”¨ parameters ä¸­çš„ strategyï¼Œå…¶æ¬¡ä½¿ç”¨ algorithm_typeï¼‰
        strategy_type = version.parameters.get('strategy') or version.algorithm_type
        print(f"ğŸ“‹ ç­–ç•¥é¡å‹: {strategy_type}")
        print(f"ğŸ“‹ algorithm_type: {version.algorithm_type}")
        print(f"ğŸ“‹ åŸå§‹ parameters: {version.parameters}")
        
        if not strategy_type:
            raise ValueError(f"ç‰ˆæœ¬ {version.version_name} ç¼ºå°‘ç­–ç•¥é¡å‹é…ç½®")
        
        # 2. æº–å‚™ç­–ç•¥åƒæ•¸ï¼ˆç§»é™¤ 'strategy' éµï¼Œé¿å…å‚³å…¥ç­–ç•¥é¡çš„ __init__ï¼‰
        strategy_params = {k: v for k, v in version.parameters.items() if k != 'strategy'}
        print(f"ğŸ“‹ éæ¿¾å¾Œçš„ç­–ç•¥åƒæ•¸: {strategy_params}")
        
        # 3. ç²å–æœå°‹ç­–ç•¥ï¼ˆâš ï¸ ç­–ç•¥é¡çš„ __init__ åªæ¥å— search_serviceï¼Œå…¶ä»–åƒæ•¸å‚³çµ¦ execute()ï¼‰
        print(f"ğŸ”§ æ­£åœ¨åˆå§‹åŒ–ç­–ç•¥ {strategy_type}...")
        try:
            strategy = get_strategy(
                strategy_type,
                ProtocolGuideSearchService()
                # âš ï¸ ä¸è¦åœ¨é€™è£¡å‚³å…¥å…¶ä»–åƒæ•¸ï¼
            )
            print(f"âœ… ç­–ç•¥åˆå§‹åŒ–æˆåŠŸ: {type(strategy).__name__}")
        except Exception as e:
            print(f"âŒ ç­–ç•¥åˆå§‹åŒ–å¤±æ•—: {str(e)}")
            raise
        
        # 4. åŸ·è¡Œæœå°‹ï¼ˆâš ï¸ åƒæ•¸å‚³çµ¦ execute() è€Œé __init__ï¼‰
        print(f"ğŸ” åŸ·è¡Œæœå°‹æŸ¥è©¢: {self.test_case.question[:50]}...")
        print(f"ğŸ” æœå°‹åƒæ•¸: {strategy_params}")
        try:
            search_results = strategy.execute(
                query=self.test_case.question,
                limit=10,
                **strategy_params  # âš ï¸ åƒæ•¸åœ¨é€™è£¡å‚³å…¥ï¼
            )
            print(f"âœ… æœå°‹å®Œæˆï¼Œæ‰¾åˆ° {len(search_results)} å€‹çµæœ")
        except Exception as e:
            print(f"âŒ æœå°‹åŸ·è¡Œå¤±æ•—: {str(e)}")
            raise
        
        # 5. è©•ä¼°çµæœ
        print(f"ğŸ“Š é–‹å§‹è©•ä¼°æœå°‹çµæœ...")
        try:
            metrics = self._evaluate_results(search_results)
            print(f"âœ… è©•ä¼°å®Œæˆ:")
            print(f"   - Precision: {metrics['precision']:.2%}")
            print(f"   - Recall: {metrics['recall']:.2%}")
            print(f"   - F1 Score: {metrics['f1_score']:.2%}")
        except Exception as e:
            print(f"âŒ è©•ä¼°å¤±æ•—: {str(e)}")
            raise
        
        response_time = (datetime.now() - start_time).total_seconds()
        print(f"â±ï¸ å›æ‡‰æ™‚é–“: {response_time:.2f} ç§’")
        
        # 6. å„²å­˜çµæœåˆ°è³‡æ–™åº«
        print(f"ğŸ’¾ æ­£åœ¨å„²å­˜æ¸¬è©¦çµæœåˆ°è³‡æ–™åº«...")
        try:
            test_run = self._save_test_result(
                version=version,
                metrics=metrics,
                response_time=response_time,
                search_results=search_results
            )
            print(f"âœ… çµæœå·²å„²å­˜ (TestRun ID: {test_run.id})")
        except Exception as e:
            print(f"âŒ å„²å­˜å¤±æ•—: {str(e)}")
            raise
        
        print(f"{'='*80}")
        print(f"âœ… ç‰ˆæœ¬ {version.version_name} æ¸¬è©¦å®Œæˆ!")
        print(f"{'='*80}\n")
        
        return {
            'version_id': version.id,
            'version_name': version.version_name,
            'strategy_type': strategy_type,
            'metrics': {
                'precision': float(metrics['precision']),
                'recall': float(metrics['recall']),
                'f1_score': float(metrics['f1_score'])
            },
            'response_time': response_time,
            'matched_keywords': metrics.get('matched_keywords', []),
            'total_keywords': metrics.get('total_keywords', 0),
            'status': 'success',
            'test_run_id': test_run.id if test_run else None
        }
    
    def _evaluate_results(self, search_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        è©•ä¼°æœå°‹çµæœ
        
        ä½¿ç”¨ç­”æ¡ˆé—œéµå­—é€²è¡Œè©•ä¼°ï¼Œè¨ˆç®— P/R/F1
        
        Args:
            search_results: æœå°‹çµæœåˆ—è¡¨
            
        Returns:
            Dict: è©•ä¼°æŒ‡æ¨™
                {
                    'precision': Decimal,
                    'recall': Decimal,
                    'f1_score': Decimal,
                    'matched_keywords': List[str],
                    'total_keywords': int
                }
        """
        if not search_results:
            return {
                'precision': Decimal('0'),
                'recall': Decimal('0'),
                'f1_score': Decimal('0'),
                'matched_keywords': [],
                'total_keywords': len(self.test_case.expected_keywords) if self.test_case.expected_keywords else 0
            }
        
        # ç²å–ç­”æ¡ˆé—œéµå­—
        answer_keywords = self.test_case.expected_keywords or []
        if not answer_keywords:
            # å¦‚æœæ²’æœ‰é—œéµå­—ï¼Œç„¡æ³•è©•ä¼°
            return {
                'precision': Decimal('0'),
                'recall': Decimal('0'),
                'f1_score': Decimal('0'),
                'matched_keywords': [],
                'total_keywords': 0
            }
        
        # æå–æœå°‹çµæœçš„å…§å®¹
        result_texts = []
        for result in search_results:
            text = ""
            if 'title' in result:
                text += result['title'] + " "
            if 'content' in result:
                text += result['content']
            result_texts.append(text)
        
        combined_text = " ".join(result_texts)
        
        # è¨ˆç®—åŒ¹é…çš„é—œéµå­—
        matched_keywords = []
        for keyword in answer_keywords:
            if keyword.lower() in combined_text.lower():
                matched_keywords.append(keyword)
        
        # è¨ˆç®—æŒ‡æ¨™
        total_keywords = len(answer_keywords)
        matched_count = len(matched_keywords)
        
        # Recall = åŒ¹é…çš„é—œéµå­—æ•¸ / ç¸½é—œéµå­—æ•¸
        recall = Decimal(matched_count) / Decimal(total_keywords) if total_keywords > 0 else Decimal('0')
        
        # Precision = åŒ¹é…çš„é—œéµå­—æ•¸ / (æœå°‹çµæœæ•¸ Ã— ç¸½é—œéµå­—æ•¸)
        # é€™è£¡ä½¿ç”¨ç°¡åŒ–ç‰ˆæœ¬ï¼šå¦‚æœåŒ¹é…åˆ°é—œéµå­—ï¼Œprecision å°±æ˜¯ recall
        # å› ç‚ºæˆ‘å€‘å‡è¨­æ¯å€‹æœå°‹çµæœéƒ½æ˜¯ç›¸é—œçš„
        precision = recall
        
        # F1 Score
        if precision + recall > 0:
            f1_score = (2 * precision * recall) / (precision + recall)
        else:
            f1_score = Decimal('0')
        
        return {
            'precision': precision,
            'recall': recall,
            'f1_score': f1_score,
            'matched_keywords': matched_keywords,
            'total_keywords': total_keywords
        }
    
    def _save_test_result(
        self,
        version,
        metrics: Dict[str, Any],
        response_time: float,
        search_results: List[Dict[str, Any]]
    ):
        """
        å„²å­˜æ¸¬è©¦çµæœåˆ°è³‡æ–™åº«
        
        Args:
            version: SearchAlgorithmVersion å¯¦ä¾‹
            metrics: è©•ä¼°æŒ‡æ¨™
            response_time: å›æ‡‰æ™‚é–“ï¼ˆç§’ï¼‰
            search_results: æœå°‹çµæœ
            
        Returns:
            BenchmarkTestRun: æ¸¬è©¦é‹è¡Œè¨˜éŒ„
        """
        from api.models import BenchmarkTestRun, BenchmarkTestResult
        
        try:
            # âœ… å‰µå»ºæ¸¬è©¦é‹è¡Œè¨˜éŒ„ï¼ˆä½¿ç”¨æ­£ç¢ºçš„æ¬„ä½ï¼‰
            test_run = BenchmarkTestRun.objects.create(
                version=version,  # âš ï¸ å¿…å¡«ï¼šé—œè¯çš„ç‰ˆæœ¬
                run_name=f"å–®æ¡ˆä¾‹æ¸¬è©¦ - {self.test_case.question[:30]}...",
                run_type='single_case_comparison',  # âš ï¸ æ­£ç¢ºçš„æ¬„ä½åç¨±
                total_test_cases=1,
                completed_test_cases=1,
                status='completed',
                # æŒ‡æ¨™æ•¸æ“š
                avg_precision=metrics['precision'],
                avg_recall=metrics['recall'],
                avg_f1_score=metrics['f1_score'],
                overall_score=metrics['f1_score'],  # å–®æ¡ˆä¾‹çš„ overall_score = f1_score
                avg_response_time=Decimal(str(response_time))
            )
            
            # âœ… å‰µå»ºæ¸¬è©¦çµæœè¨˜éŒ„ï¼ˆä½¿ç”¨æ­£ç¢ºçš„æ¬„ä½åç¨±ï¼‰
            matched_keywords = metrics.get('matched_keywords', [])
            total_keywords = metrics.get('total_keywords', 0)
            
            BenchmarkTestResult.objects.create(
                test_run=test_run,
                test_case=self.test_case,
                search_query=self.test_case.question,  # âš ï¸ å¿…å¡«æ¬„ä½
                returned_document_ids=[r.get('id', 0) for r in search_results[:10]],
                returned_document_scores=[float(r.get('score', 0)) for r in search_results[:10]],
                # è©•åˆ†æŒ‡æ¨™ï¼ˆä½¿ç”¨æ­£ç¢ºçš„æ¬„ä½åç¨±ï¼‰
                precision_score=metrics['precision'],  # âš ï¸ precision_score ä¸æ˜¯ precision
                recall_score=metrics['recall'],        # âš ï¸ recall_score ä¸æ˜¯ recall
                f1_score=metrics['f1_score'],
                response_time=Decimal(str(response_time * 1000)),  # âš ï¸ å–®ä½æ˜¯æ¯«ç§’
                # æ··æ·†çŸ©é™£ï¼ˆå¯é¸ï¼‰
                true_positives=len(matched_keywords),
                false_negatives=total_keywords - len(matched_keywords),
                # çµæœåˆ¤å®š
                is_passed=metrics['f1_score'] > Decimal('0.5'),
                pass_reason=f"åŒ¹é… {len(matched_keywords)}/{total_keywords} å€‹é—œéµå­—",
                # è©³ç´°çµæœ
                detailed_results={
                    'search_results': search_results[:5],  # åªå„²å­˜å‰ 5 å€‹çµæœ
                    'matched_keywords': matched_keywords,
                    'total_keywords': total_keywords,
                    'version_name': version.version_name,
                    'strategy_type': version.parameters.get('strategy', version.algorithm_type)
                }
            )
            
            return test_run
            
        except Exception as e:
            self._log(f"å„²å­˜æ¸¬è©¦çµæœå¤±æ•—: {str(e)}", level='error')
            return None
    
    def _generate_summary(
        self,
        results: List[Dict[str, Any]],
        total_time: float,
        test_run_ids: List[int]
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ¸¬è©¦æ‘˜è¦
        
        Args:
            results: æ‰€æœ‰ç‰ˆæœ¬çš„æ¸¬è©¦çµæœ
            total_time: ç¸½åŸ·è¡Œæ™‚é–“ï¼ˆç§’ï¼‰
            test_run_ids: æ¸¬è©¦é‹è¡Œ ID åˆ—è¡¨
            
        Returns:
            Dict: æ¸¬è©¦æ‘˜è¦
        """
        successful_results = [r for r in results if r.get('status') == 'success']
        failed_results = [r for r in results if r.get('status') == 'error']
        
        # æ‰¾å‡ºæœ€ä½³ç‰ˆæœ¬ï¼ˆæŒ‰ F1 Score æ’åºï¼‰
        best_version = None
        if successful_results:
            best_version = max(
                successful_results,
                key=lambda x: x['metrics']['f1_score']
            )
        
        # è¨ˆç®—å¹³å‡å›æ‡‰æ™‚é–“
        avg_response_time = 0.0
        if successful_results:
            avg_response_time = sum(
                r['response_time'] for r in successful_results
            ) / len(successful_results)
        
        return {
            'total_versions': len(results),
            'successful_tests': len(successful_results),
            'failed_tests': len(failed_results),
            'best_version': best_version,
            'avg_response_time': round(avg_response_time, 2),
            'total_execution_time': round(total_time, 2),
            'test_run_ids': test_run_ids
        }
    
    def _log(self, message: str, level: str = 'info'):
        """è¼¸å‡ºæ—¥èªŒ"""
        if self.verbose:
            print(f"[SingleCaseVersionTester] {message}")
        
        log_func = getattr(logger, level, logger.info)
        log_func(f"[SingleCaseVersionTester] {message}")


# ä¾¿åˆ©å‡½æ•¸

def test_single_case_all_versions(test_case_id: int, verbose: bool = False) -> Dict[str, Any]:
    """
    æ¸¬è©¦å–®ä¸€æ¡ˆä¾‹çš„æ‰€æœ‰ç‰ˆæœ¬
    
    Args:
        test_case_id: æ¸¬è©¦æ¡ˆä¾‹ ID
        verbose: æ˜¯å¦è¼¸å‡ºè©³ç´°æ—¥èªŒ
        
    Returns:
        Dict: æ¸¬è©¦çµæœ
    """
    tester = SingleCaseVersionTester(test_case_id, version_ids=None, verbose=verbose)
    return tester.run_comparison()


def test_single_case_selected_versions(
    test_case_id: int,
    version_ids: List[int],
    verbose: bool = False
) -> Dict[str, Any]:
    """
    æ¸¬è©¦å–®ä¸€æ¡ˆä¾‹çš„æŒ‡å®šç‰ˆæœ¬
    
    Args:
        test_case_id: æ¸¬è©¦æ¡ˆä¾‹ ID
        version_ids: ç‰ˆæœ¬ ID åˆ—è¡¨
        verbose: æ˜¯å¦è¼¸å‡ºè©³ç´°æ—¥èªŒ
        
    Returns:
        Dict: æ¸¬è©¦çµæœ
    """
    tester = SingleCaseVersionTester(test_case_id, version_ids=version_ids, verbose=verbose)
    return tester.run_comparison()
