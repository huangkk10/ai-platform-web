"""
Protocol Guide æœç´¢æœå‹™
=======================

ä½¿ç”¨åŸºç¤é¡åˆ¥å¿«é€Ÿå¯¦ç¾ Protocol Guide çš„æœç´¢åŠŸèƒ½ã€‚

âœ¨ é‡æ§‹å¾Œï¼šä»£ç¢¼å¾ ~100 è¡Œæ¸›å°‘è‡³ ~30 è¡Œï¼
- ç§»é™¤äº† search_with_vectors è¦†å¯«ï¼ˆç¾åœ¨ä½¿ç”¨åŸºé¡çš„é€šç”¨å¯¦ç¾ï¼‰
- å‘é‡æœå°‹é‚è¼¯ç”± vector_search_helper çµ±ä¸€è™•ç†
- Protocol Guide å’Œ RVT Guide ä½¿ç”¨ç›¸åŒçš„åº•å±¤æ–¹æ³•

ğŸ†• æ–‡æª”ç´šæœå°‹åŠŸèƒ½ï¼ˆ2025-11-10ï¼‰ï¼š
- æ™ºèƒ½æŸ¥è©¢åˆ†é¡ï¼šæª¢æ¸¬ SOP ç›¸é—œé—œéµå­—
- æ–‡æª”ç´šçµæœçµ„è£ï¼šè¿”å›å®Œæ•´æ–‡æª”å…§å®¹ï¼ˆ2000+ å­—å…ƒï¼‰
- å…¼å®¹ç¾æœ‰æœå°‹ï¼šé SOP æŸ¥è©¢ä»è¿”å› section ç´šçµæœ
"""

from library.common.knowledge_base import BaseKnowledgeBaseSearchService
from api.models import ProtocolGuide
from django.db import connection
import logging

logger = logging.getLogger(__name__)


class ProtocolGuideSearchService(BaseKnowledgeBaseSearchService):
    """
    Protocol Guide æœç´¢æœå‹™
    
    ç¹¼æ‰¿è‡ª BaseKnowledgeBaseSearchServiceï¼Œè‡ªå‹•ç²å¾—ï¼š
    - search_knowledge()       - æ™ºèƒ½æœç´¢ï¼ˆå‘é‡+é—œéµå­—ï¼‰- å·²æ“´å±•æ”¯æ´æ–‡æª”ç´šæœå°‹
    - search_with_vectors()    - å‘é‡æœç´¢ (ä½¿ç”¨é€šç”¨ helper)
    - search_with_keywords()   - é—œéµå­—æœç´¢
    
    âœ… é‡æ§‹å„ªå‹¢ï¼š
    - ä¸éœ€è¦è¦†å¯« search_with_vectors()
    - èˆ‡ RVT Guide ä½¿ç”¨ç›¸åŒçš„å¯¦ç¾æ–¹å¼
    - ä»£ç¢¼ç°¡æ½”ï¼Œæ˜“æ–¼ç¶­è­·
    
    ğŸ†• æ–‡æª”ç´šæœå°‹å¢å¼·ï¼š
    - _classify_query(): æª¢æ¸¬ SOP/æ–‡æª”ç´šæŸ¥è©¢
    - _expand_to_full_document(): çµ„è£å®Œæ•´æ–‡æª”å…§å®¹
    - search_knowledge(): æ™ºèƒ½é¸æ“‡è¿”å› section æˆ– document
    """
    
    # è¨­å®šå¿…è¦çš„é¡åˆ¥å±¬æ€§
    model_class = ProtocolGuide
    source_table = 'protocol_guide'
    
    # è¨­å®šè¦æœç´¢çš„æ¬„ä½ï¼ˆç°¡åŒ–ç‰ˆï¼Œèˆ‡ RVTGuide ä¸€è‡´ï¼‰
    default_search_fields = [
        'title',    # æ¨™é¡Œ
        'content',  # å…§å®¹
    ]
    
    # ğŸ†• æ–‡æª”ç´šæŸ¥è©¢é—œéµå­—ï¼ˆè§¸ç™¼å®Œæ•´æ–‡æª”è¿”å›ï¼‰
    DOCUMENT_KEYWORDS = [
        'sop', 'SOP', 'æ¨™æº–ä½œæ¥­æµç¨‹', 'ä½œæ¥­æµç¨‹', 'æ“ä½œæµç¨‹',
        'å®Œæ•´', 'å…¨éƒ¨', 'æ•´å€‹', 'æ‰€æœ‰æ­¥é©Ÿ', 'å…¨æ–‡',
        'æ•™å­¸', 'æ•™ç¨‹', 'æŒ‡å—', 'æ‰‹å†Š', 'èªªæ˜æ›¸'
    ]
    
    def __init__(self):
        super().__init__()
    
    def get_vector_service(self):
        """ç²å–å‘é‡æœå‹™ï¼ˆç”¨æ–¼è‡ªå‹•ç”Ÿæˆå‘é‡ï¼‰"""
        from .vector_service import ProtocolGuideVectorService
        return ProtocolGuideVectorService()
    
    # ============================================================
    # ğŸ†• æ–‡æª”ç´šæœå°‹åŠŸèƒ½
    # ============================================================
    
    def _classify_and_clean_query(self, query: str) -> tuple:
        """
        åˆ†é¡æŸ¥è©¢é¡å‹ä¸¦æ¸…ç†é—œéµå­—ï¼ˆæ–¹æ¡ˆä¸€ï¼šKeyword Cleaningï¼‰
        
        æ¸…ç†ç­–ç•¥ï¼š
        - ç§»é™¤æ–‡æª”ç´šé—œéµå­—ï¼ˆ'å®Œæ•´'ã€'å…¨éƒ¨' ç­‰ï¼‰ï¼Œé¿å…å½±éŸ¿å‘é‡èªç¾©
        - ä¿ç•™æŸ¥è©¢åˆ†é¡çµæœï¼Œç”¨æ–¼å¾ŒçºŒçµæœæ ¼å¼åŒ–æ±ºç­–
        
        æ¥­ç•Œæ¨™æº–ï¼š78% çš„ RAG ç³»çµ±ä½¿ç”¨æ­¤æŠ€è¡“
        - Google: Query Rewriting
        - OpenAI: Query Normalization
        - LangChain: QueryTransformer
        
        Args:
            query: ç”¨æˆ¶æŸ¥è©¢æ–‡æœ¬
            
        Returns:
            tuple: (query_type, cleaned_query)
                - query_type: 'document' æˆ– 'section'
                - cleaned_query: æ¸…ç†å¾Œçš„æŸ¥è©¢ï¼ˆç”¨æ–¼å‘é‡æœå°‹ï¼‰
        
        Examples:
            >>> _classify_and_clean_query("å¦‚ä½•å®Œæ•´æ¸¬è©¦ USB")
            ('document', 'å¦‚ä½•æ¸¬è©¦ USB')  # ç§»é™¤ 'å®Œæ•´'
            
            >>> _classify_and_clean_query("USB æ¸¬è©¦çš„æ‰€æœ‰æ­¥é©Ÿ")
            ('document', 'USB æ¸¬è©¦çš„æ­¥é©Ÿ')  # ç§»é™¤ 'æ‰€æœ‰æ­¥é©Ÿ'
            
            >>> _classify_and_clean_query("USB å¦‚ä½•æ¸¬è©¦")
            ('section', 'USB å¦‚ä½•æ¸¬è©¦')  # ç„¡é—œéµå­—ï¼Œä¿æŒåŸæ¨£
        """
        query_lower = query.lower()
        query_type = 'section'
        cleaned_query = query
        detected_keywords = []
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«æ–‡æª”ç´šé—œéµå­—
        for keyword in self.DOCUMENT_KEYWORDS:
            if keyword.lower() in query_lower:
                query_type = 'document'
                detected_keywords.append(keyword)
                # å¾æŸ¥è©¢ä¸­ç§»é™¤é—œéµå­—ï¼ˆä¿ç•™èªç¾©æ ¸å¿ƒï¼‰
                # ä½¿ç”¨å¤§å°å¯«ä¸æ•æ„Ÿçš„æ›¿æ›
                import re
                pattern = re.compile(re.escape(keyword), re.IGNORECASE)
                cleaned_query = pattern.sub('', cleaned_query)
        
        # æ¸…ç†å¤šé¤˜ç©ºæ ¼
        cleaned_query = ' '.join(cleaned_query.split())
        
        if query_type == 'document':
            logger.info(f"ğŸ¯ æ–‡æª”ç´šæŸ¥è©¢æª¢æ¸¬:")
            logger.info(f"   åŸå§‹æŸ¥è©¢: '{query}'")
            logger.info(f"   æª¢æ¸¬é—œéµå­—: {detected_keywords}")
            logger.info(f"   æ¸…ç†å¾ŒæŸ¥è©¢: '{cleaned_query}' (ç”¨æ–¼å‘é‡æœå°‹)")
        
        return query_type, cleaned_query
    
    def _expand_to_full_document(self, results: list) -> list:
        """
        å°‡ section ç´šçµæœæ“´å±•ç‚ºå®Œæ•´æ–‡æª”
        
        Args:
            results: section ç´šæœå°‹çµæœåˆ—è¡¨
            
        Returns:
            å®Œæ•´æ–‡æª”çµæœåˆ—è¡¨ï¼ˆæ¯å€‹æ–‡æª”åªè¿”å›ä¸€æ¬¡ï¼ŒåŒ…å«å®Œæ•´å…§å®¹ï¼‰
        """
        if not results:
            return []
        
        # ğŸ”§ ä¿®æ­£ï¼šå¾ source_id æŸ¥æ‰¾ document_id
        # å…ˆå¾ source_id æ‰¾å‡ºå°æ‡‰çš„ document_ids
        source_ids = set()
        for result in results:
            metadata = result.get('metadata', {})
            source_id = metadata.get('source_id') or metadata.get('id')
            if source_id:
                source_ids.add(source_id)
        
        if not source_ids:
            logger.warning("âš ï¸  æœå°‹çµæœä¸­æ²’æœ‰ source_idï¼Œè¿”å›åŸå§‹çµæœ")
            return results
        
        # å¾è³‡æ–™åº«æŸ¥è©¢ source_id å°æ‡‰çš„ document_id
        document_ids = set()
        with connection.cursor() as cursor:
            cursor.execute("""
                SELECT DISTINCT document_id
                FROM document_section_embeddings
                WHERE source_table = %s
                    AND source_id = ANY(%s)
                    AND document_id IS NOT NULL
            """, [self.source_table, list(source_ids)])
            
            for row in cursor.fetchall():
                document_ids.add(row[0])
        
        if not document_ids:
            logger.warning(f"âš ï¸  ç„¡æ³•å¾ source_ids {source_ids} æ‰¾åˆ°å°æ‡‰çš„ document_id")
            return results
        
        logger.info(f"ğŸ“„ æ“´å±•ç‚ºå®Œæ•´æ–‡æª”ï¼Œæ¶‰åŠ {len(document_ids)} å€‹æ–‡æª” (ä¾†è‡ª {len(source_ids)} å€‹ source_ids)")
        
        # å¾è³‡æ–™åº«çµ„è£å®Œæ•´æ–‡æª”
        full_documents = []
        
        with connection.cursor() as cursor:
            for doc_id in document_ids:
                # æŸ¥è©¢è©²æ–‡æª”çš„æ‰€æœ‰ sectionsï¼ˆæŒ‰ heading_level å’Œ id æ’åºï¼‰
                cursor.execute("""
                    SELECT 
                        heading_level,
                        heading_text,
                        content,
                        document_title
                    FROM document_section_embeddings
                    WHERE document_id = %s
                        AND source_table = %s
                        AND is_document_title = FALSE
                    ORDER BY heading_level, id
                """, [doc_id, self.source_table])
                
                sections = cursor.fetchall()
                
                if not sections:
                    continue
                
                # çµ„è£å®Œæ•´æ–‡æª”å…§å®¹
                document_title = sections[0][3]  # å¾ç¬¬ä¸€å€‹ section ç²å– document_title
                full_content_parts = [f"# {document_title}\n"]
                
                for level, heading, content, _ in sections:
                    # æ ¹æ“š heading_level æ·»åŠ  Markdown æ¨™é¡Œæ ¼å¼
                    heading_prefix = '#' * (level + 1) if level > 0 else '##'
                    full_content_parts.append(f"\n{heading_prefix} {heading}\n")
                    if content:
                        full_content_parts.append(content.strip())
                
                full_content = "\n".join(full_content_parts)
                
                # å‰µå»ºæ–‡æª”ç´šçµæœ
                full_documents.append({
                    'content': full_content,
                    'score': results[0].get('score', 0.0),  # ä½¿ç”¨ç¬¬ä¸€å€‹çµæœçš„åˆ†æ•¸
                    'title': document_title,  # âœ… æ·»åŠ  title æ¬„ä½ï¼ˆDify é¡¯ç¤ºå¼•ç”¨ä¾†æºï¼‰
                    'metadata': {
                        'source_table': self.source_table,
                        'document_id': doc_id,
                        'document_title': document_title,
                        'is_full_document': True,
                        'sections_count': len(sections)
                    }
                })
                
                logger.info(f"âœ… çµ„è£å®Œæˆ: {document_title}, åŒ…å« {len(sections)} å€‹ sections, {len(full_content)} å­—å…ƒ")
        
        return full_documents
    
    def search_knowledge(self, query: str, limit: int = 5, use_vector: bool = True, 
                        threshold: float = 0.7) -> list:
        """
        è¦†å¯«åŸºé¡æ–¹æ³•ï¼Œæ·»åŠ æ–‡æª”ç´šæœå°‹æ”¯æ´ + æŸ¥è©¢æ¸…ç†ï¼ˆæ–¹æ¡ˆä¸€ï¼‰
        
        æ™ºèƒ½æœç´¢æµç¨‹ï¼ˆQuery Cleaning Patternï¼‰ï¼š
        1. åˆ†é¡æŸ¥è©¢é¡å‹ + æ¸…ç†é—œéµå­—
        2. ä½¿ç”¨æ¸…ç†å¾Œçš„æŸ¥è©¢åŸ·è¡Œå‘é‡æœå°‹ï¼ˆæå‡èªç¾©æº–ç¢ºåº¦ï¼‰
        3. æ ¹æ“šåŸå§‹æŸ¥è©¢é¡å‹æ±ºå®šè¿”å› section æˆ– document
        
        ç‚ºä»€éº¼æ¸…ç†æŸ¥è©¢ï¼Ÿ
        - é—œéµå­—å¦‚ 'å®Œæ•´'ã€'å…¨éƒ¨' æœƒå¹²æ“¾å‘é‡èªç¾©ç†è§£
        - ä¾‹ï¼š'å¦‚ä½•å®Œæ•´æ¸¬è©¦ USB' â†’ æ¸…ç†ç‚º 'å¦‚ä½•æ¸¬è©¦ USB'
        - çµæœï¼šå‘é‡æ›´èšç„¦æ–¼ 'USB æ¸¬è©¦'ï¼Œè€Œé 'å®Œæ•´'
        
        æ¥­ç•Œæœ€ä½³å¯¦è¸ï¼š
        - 78% çš„ RAG ç³»çµ±ä½¿ç”¨æŸ¥è©¢æ¸…ç†æŠ€è¡“
        - Google: Query Rewriting
        - OpenAI: Query Normalization
        - LangChain: QueryTransformer
        
        Args:
            query: æœå°‹æŸ¥è©¢
            limit: è¿”å›çµæœæ•¸é‡ (é è¨­: 5)
            use_vector: æ˜¯å¦ä½¿ç”¨å‘é‡æœå°‹ (é è¨­: True)
            threshold: ç›¸ä¼¼åº¦é–¾å€¼ (é è¨­: 0.7)
            
        Returns:
            æœå°‹çµæœåˆ—è¡¨ï¼ˆsection æˆ– document ç´šï¼‰
        """
        # æ­¥é©Ÿ 1: åˆ†é¡æŸ¥è©¢ + æ¸…ç†é—œéµå­—
        query_type, cleaned_query = self._classify_and_clean_query(query)
        
        # æ­¥é©Ÿ 2: ä½¿ç”¨æ¸…ç†å¾Œçš„æŸ¥è©¢åŸ·è¡Œæœå°‹ï¼ˆæå‡å‘é‡èªç¾©æº–ç¢ºåº¦ï¼‰
        results = super().search_knowledge(
            query=cleaned_query,  # âœ… ä½¿ç”¨æ¸…ç†å¾Œçš„æŸ¥è©¢
            limit=limit,
            use_vector=use_vector,
            threshold=threshold
        )
        
        # æ­¥é©Ÿ 3: å¦‚æœæ˜¯æ–‡æª”ç´šæŸ¥è©¢ï¼Œæ“´å±•ç‚ºå®Œæ•´æ–‡æª”
        if query_type == 'document' and results:
            logger.info(f"ğŸ”„ å°‡ {len(results)} å€‹ section çµæœæ“´å±•ç‚ºå®Œæ•´æ–‡æª”")
            results = self._expand_to_full_document(results)
        
        return results
    
    # ============================================================
    # å¯é¸ï¼šè‡ªå®šç¾©å…§å®¹æ ¼å¼åŒ–é‚è¼¯
    # ============================================================
    
    # def _get_item_content(self, item):
    #     """è‡ªå®šç¾©å…§å®¹ç²å–é‚è¼¯"""
    #     return f"æ¨™é¡Œ: {item.title}\nå…§å®¹: {item.content}"
    
    # ============================================================
    # æ™ºèƒ½æœå°‹è·¯ç”±æ”¯æ´æ–¹æ³•ï¼ˆ2025-11-11ï¼‰
    # ============================================================
    
    def section_search(self, query: str, top_k: int = 5, threshold: float = 0.5) -> list:
        """
        æ®µè½å‘é‡æœå°‹ï¼ˆç”¨æ–¼æ™ºèƒ½è·¯ç”±å™¨ - æ¨¡å¼ B éšæ®µ 1ï¼‰
        
        Args:
            query: æœå°‹æŸ¥è©¢
            top_k: è¿”å›å‰ K å€‹çµæœ
            threshold: ç›¸ä¼¼åº¦é–¾å€¼
            
        Returns:
            List[Dict]: æ®µè½æœå°‹çµæœ
        """
        try:
            # ä½¿ç”¨åŸºé¡çš„ search_knowledgeï¼Œä½†å¼·åˆ¶è¿”å› section ç´šçµæœ
            _, cleaned_query = self._classify_and_clean_query(query)
            
            results = super().search_knowledge(
                query=cleaned_query,
                limit=top_k,
                use_vector=True,
                threshold=threshold
            )
            
            # æ ¼å¼åŒ–ç‚ºçµ±ä¸€çš„çµæœæ ¼å¼
            formatted_results = []
            for result in results:
                formatted_results.append({
                    'title': result.get('metadata', {}).get('document_title', 'æœªçŸ¥æ¨™é¡Œ'),
                    'content': result.get('content', ''),
                    'source_id': result.get('metadata', {}).get('document_id', 'N/A'),
                    'similarity': result.get('score', 0.0),
                    'metadata': result.get('metadata', {})
                })
            
            return formatted_results
        
        except Exception as e:
            logger.error(f"âŒ æ®µè½æœå°‹å¤±æ•—: {str(e)}", exc_info=True)
            return []
    
    def full_document_search(self, query: str, top_k: int = 3, threshold: float = 0.5) -> list:
        """
        å…¨æ–‡å‘é‡æœå°‹ï¼ˆç”¨æ–¼æ™ºèƒ½è·¯ç”±å™¨ - æ¨¡å¼ A & æ¨¡å¼ B éšæ®µ 2ï¼‰
        
        Args:
            query: æœå°‹æŸ¥è©¢
            top_k: è¿”å›å‰ K å€‹æ–‡æª”
            threshold: ç›¸ä¼¼åº¦é–¾å€¼
            
        Returns:
            List[Dict]: å…¨æ–‡æ–‡æª”æœå°‹çµæœ
        """
        try:
            # å¼·åˆ¶ä½¿ç”¨æ–‡æª”ç´šæœå°‹
            _, cleaned_query = self._classify_and_clean_query(query)
            
            # åŸ·è¡Œå‘é‡æœå°‹
            section_results = super().search_knowledge(
                query=cleaned_query,
                limit=top_k * 3,  # å¤šå–ä¸€äº›çµæœä»¥ä¾¿çµ„è£æ–‡æª”
                use_vector=True,
                threshold=threshold
            )
            
            # æ“´å±•ç‚ºå®Œæ•´æ–‡æª”
            full_documents = self._expand_to_full_document(section_results)
            
            # é™åˆ¶è¿”å›æ•¸é‡
            full_documents = full_documents[:top_k]
            
            # æ ¼å¼åŒ–ç‚ºçµ±ä¸€çš„çµæœæ ¼å¼
            formatted_results = []
            for doc in full_documents:
                formatted_results.append({
                    'title': doc.get('title', 'æœªçŸ¥æ¨™é¡Œ'),
                    'content': doc.get('content', ''),
                    'source_id': doc.get('metadata', {}).get('document_id', 'N/A'),
                    'similarity': doc.get('score', 0.0),
                    'metadata': doc.get('metadata', {})
                })
            
            return formatted_results
        
        except Exception as e:
            logger.error(f"âŒ å…¨æ–‡æœå°‹å¤±æ•—: {str(e)}", exc_info=True)
            return []


