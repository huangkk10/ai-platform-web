"""
Protocol Guide æœç´¢æœå‹™
=======================

ä½¿ç”¨åŸºç¤é¡åˆ¥å¿«é€Ÿå¯¦ç¾ Protocol Guide çš„æœç´¢åŠŸèƒ½ã€‚

âœ¨ é‡æ§‹å¾Œï¼šä»£ç¢¼å¾ ~100 è¡Œæ¸›å°‘è‡³ ~30 è¡Œï¼
- ç§»é™¤äº† search_with_vectors è¦†å¯«ï¼ˆç¾åœ¨ä½¿ç”¨åŸºé¡çš„é€šç”¨å¯¦ç¾ï¼‰
- å‘é‡æœå°‹é‚è¼¯ç”± vector_search_helper çµ±ä¸€è™•ç†
- Protocol Guide å’Œ RVT Guide ä½¿ç”¨ç›¸åŒçš„åº•å±¤æ–¹æ³•

ğŸ†• æ–‡æª”ç´šæœå°‹åŠŸèƒ½ï¼ˆ2025-11-10ï¼‰ï¼š
- æ™ºèƒ½æŸ¥è©¢åˆ†é¡ï¼šæª¢æ¸¬ SOP ç›¸é—œé—œéµå­—
- æ–‡æª”ç´šçµæœçµ„è£ï¼šè¿”å›å®Œæ•´æ–‡æª”å…§å®¹ï¼ˆ2000+ å­—å…ƒï¼‰
- å…¼å®¹ç¾æœ‰æœå°‹ï¼šé SOP æŸ¥è©¢ä»è¿”å› section ç´šçµæœ
"""

from library.common.knowledge_base import BaseKnowledgeBaseSearchService
from api.models import ProtocolGuide
from django.db import connection
import logging
import re

# å˜—è©¦å°å…¥ jiebaï¼Œå¦‚æœå¤±æ•—å‰‡ä½¿ç”¨ fallback
try:
    import jieba
    JIEBA_AVAILABLE = True
except ImportError:
    JIEBA_AVAILABLE = False

logger = logging.getLogger(__name__)


def smart_tokenize(query: str) -> list:
    """
    æ™ºèƒ½åˆ†è©ï¼šæ”¯æ´ä¸­è‹±æ–‡æ··åˆæŸ¥è©¢
    
    ä½¿ç”¨ jieba åˆ†è©è™•ç†ä¸­æ–‡ï¼Œè‡ªå‹•è­˜åˆ¥ä¸­è‹±æ–‡é‚Šç•Œã€‚
    å¦‚æœ jieba ä¸å¯ç”¨ï¼Œå‰‡ä½¿ç”¨ regex fallbackã€‚
    
    Args:
        query: åŸå§‹æŸ¥è©¢å­—ä¸²
        
    Returns:
        List[str]: åˆ†è©å¾Œçš„é—œéµå­—åˆ—è¡¨ï¼ˆå·²éæ¿¾ç©ºç™½å’Œæ¨™é»ï¼‰
        
    Examples:
        >>> smart_tokenize("iolå¯†ç¢¼")
        ['iol', 'å¯†ç¢¼']
        >>> smart_tokenize("iol root å¯†ç¢¼")
        ['iol', 'root', 'å¯†ç¢¼']
        >>> smart_tokenize("crystaldiskmarkæ¸¬è©¦")
        ['crystaldiskmark', 'æ¸¬è©¦']
    """
    if not query or not query.strip():
        return []
    
    query = query.strip()
    
    if JIEBA_AVAILABLE:
        # ä½¿ç”¨ jieba åˆ†è©
        tokens = jieba.cut(query)
        # éæ¿¾ç©ºç™½ã€æ¨™é»å’Œå–®å­—å…ƒæ¨™é»ç¬¦è™Ÿ
        keywords = [
            t.strip() 
            for t in tokens 
            if t.strip() and len(t.strip()) > 0 and not re.match(r'^[\s\-_,ï¼Œã€‚ï¼ï¼Ÿï¼šï¼›ã€]+$', t)
        ]
    else:
        # Fallback: ä½¿ç”¨ regex åœ¨ä¸­è‹±æ–‡é‚Šç•Œæ’å…¥ç©ºæ ¼
        # è‹±æ–‡å¾Œæ¥ä¸­æ–‡
        query = re.sub(r'([a-zA-Z0-9])([^\x00-\x7F])', r'\1 \2', query)
        # ä¸­æ–‡å¾Œæ¥è‹±æ–‡
        query = re.sub(r'([^\x00-\x7F])([a-zA-Z0-9])', r'\1 \2', query)
        keywords = [k.strip() for k in query.split() if k.strip()]
    
    logger.debug(f"ğŸ”¤ æ™ºèƒ½åˆ†è©: '{query}' â†’ {keywords}")
    return keywords


class ProtocolGuideSearchService(BaseKnowledgeBaseSearchService):
    """
    Protocol Guide æœç´¢æœå‹™
    
    ç¹¼æ‰¿è‡ª BaseKnowledgeBaseSearchServiceï¼Œè‡ªå‹•ç²å¾—ï¼š
    - search_knowledge()       - æ™ºèƒ½æœç´¢ï¼ˆå‘é‡+é—œéµå­—ï¼‰- å·²æ“´å±•æ”¯æ´æ–‡æª”ç´šæœå°‹
    - search_with_vectors()    - å‘é‡æœç´¢ (ä½¿ç”¨é€šç”¨ helper)
    - search_with_keywords()   - é—œéµå­—æœç´¢
    
    âœ… é‡æ§‹å„ªå‹¢ï¼š
    - ä¸éœ€è¦è¦†å¯« search_with_vectors()
    - èˆ‡ RVT Guide ä½¿ç”¨ç›¸åŒçš„å¯¦ç¾æ–¹å¼
    - ä»£ç¢¼ç°¡æ½”ï¼Œæ˜“æ–¼ç¶­è­·
    
    ğŸ†• æ–‡æª”ç´šæœå°‹å¢å¼·ï¼š
    - _classify_query(): æª¢æ¸¬ SOP/æ–‡æª”ç´šæŸ¥è©¢
    - _expand_to_full_document(): çµ„è£å®Œæ•´æ–‡æª”å…§å®¹
    - search_knowledge(): æ™ºèƒ½é¸æ“‡è¿”å› section æˆ– document
    """
    
    # è¨­å®šå¿…è¦çš„é¡åˆ¥å±¬æ€§
    model_class = ProtocolGuide
    source_table = 'protocol_guide'
    
    # è¨­å®šè¦æœç´¢çš„æ¬„ä½ï¼ˆç°¡åŒ–ç‰ˆï¼Œèˆ‡ RVTGuide ä¸€è‡´ï¼‰
    default_search_fields = [
        'title',    # æ¨™é¡Œ
        'content',  # å…§å®¹
    ]
    
    # ğŸ†• æ–‡æª”ç´šæŸ¥è©¢é—œéµå­—ï¼ˆè§¸ç™¼å®Œæ•´æ–‡æª”è¿”å›ï¼‰
    DOCUMENT_KEYWORDS = [
        'sop', 'SOP', 'æ¨™æº–ä½œæ¥­æµç¨‹', 'ä½œæ¥­æµç¨‹', 'æ“ä½œæµç¨‹',
        'å®Œæ•´', 'å…¨éƒ¨', 'æ•´å€‹', 'æ‰€æœ‰æ­¥é©Ÿ', 'å…¨æ–‡',
        'æ•™å­¸', 'æ•™ç¨‹', 'æŒ‡å—', 'æ‰‹å†Š', 'èªªæ˜æ›¸'
    ]
    
    def __init__(self):
        super().__init__()
    
    def get_vector_service(self):
        """ç²å–å‘é‡æœå‹™ï¼ˆç”¨æ–¼è‡ªå‹•ç”Ÿæˆå‘é‡ï¼‰"""
        from .vector_service import ProtocolGuideVectorService
        return ProtocolGuideVectorService()
    
    # ============================================================
    # ğŸ†• æ–‡æª”ç´šæœå°‹åŠŸèƒ½
    # ============================================================
    
    def _classify_and_clean_query(self, query: str) -> tuple:
        """
        åˆ†é¡æŸ¥è©¢é¡å‹ä¸¦æ¸…ç†é—œéµå­—ï¼ˆæ–¹æ¡ˆä¸€ï¼šKeyword Cleaningï¼‰
        
        æ¸…ç†ç­–ç•¥ï¼š
        - ç§»é™¤æ–‡æª”ç´šé—œéµå­—ï¼ˆ'å®Œæ•´'ã€'å…¨éƒ¨' ç­‰ï¼‰ï¼Œé¿å…å½±éŸ¿å‘é‡èªç¾©
        - ç§»é™¤è«‹æ±‚æ€§è©èªï¼ˆ'è«‹èªªæ˜'ã€'è«‹è§£é‡‹' ç­‰ï¼‰
        - âœ… æ–°å¢ï¼šå¤§å°å¯«æ­£è¦åŒ–ï¼ˆçµ±ä¸€è½‰ç‚ºå¤§å¯«ï¼Œæå‡åŒ¹é…ç‡ï¼‰
        - ä¿ç•™æŸ¥è©¢åˆ†é¡çµæœï¼Œç”¨æ–¼å¾ŒçºŒçµæœæ ¼å¼åŒ–æ±ºç­–
        
        æ¥­ç•Œæ¨™æº–ï¼š78% çš„ RAG ç³»çµ±ä½¿ç”¨æ­¤æŠ€è¡“
        - Google: Query Rewriting
        - OpenAI: Query Normalization
        - LangChain: QueryTransformer
        
        Args:
            query: ç”¨æˆ¶æŸ¥è©¢æ–‡æœ¬
            
        Returns:
            tuple: (query_type, cleaned_query)
                - query_type: 'document' æˆ– 'section'
                - cleaned_query: æ¸…ç†å¾Œçš„æŸ¥è©¢ï¼ˆç”¨æ–¼å‘é‡æœå°‹ï¼‰
        
        Examples:
            >>> _classify_and_clean_query("å¦‚ä½•å®Œæ•´æ¸¬è©¦ USB")
            ('document', 'USB')  # ç§»é™¤ 'å®Œæ•´'ã€'å¦‚ä½•æ¸¬è©¦'
            
            >>> _classify_and_clean_query("iol sop è«‹èªªæ˜")
            ('document', 'IOL')  # ç§»é™¤ 'sop'ã€'è«‹èªªæ˜'ï¼Œå¤§å¯«åŒ–
            
            >>> _classify_and_clean_query("USB å¦‚ä½•æ¸¬è©¦")
            ('section', 'USB')  # ç„¡é—œéµå­—ï¼Œåªä¿ç•™æ ¸å¿ƒè©
        """
        import re
        
        query_lower = query.lower()
        query_type = 'section'
        cleaned_query = query
        detected_keywords = []
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«æ–‡æª”ç´šé—œéµå­—
        for keyword in self.DOCUMENT_KEYWORDS:
            if keyword.lower() in query_lower:
                query_type = 'document'
                detected_keywords.append(keyword)
                # å¾æŸ¥è©¢ä¸­ç§»é™¤é—œéµå­—ï¼ˆä¿ç•™èªç¾©æ ¸å¿ƒï¼‰
                # ä½¿ç”¨å¤§å°å¯«ä¸æ•æ„Ÿçš„æ›¿æ›
                pattern = re.compile(re.escape(keyword), re.IGNORECASE)
                cleaned_query = pattern.sub('', cleaned_query)
        
        # âœ… æ–°å¢ï¼šç§»é™¤è«‹æ±‚æ€§è©èªï¼ˆæå‡å‘é‡èªç¾©ç²¾æº–åº¦ï¼‰
        REQUEST_WORDS = [
            'è«‹èªªæ˜', 'è«‹è§£é‡‹', 'è«‹å‘Šè¨´', 'è«‹å•', 'è«‹æ•™', 'è«‹å¹«å¿™',
            'å¦‚ä½•', 'æ€éº¼', 'æ€æ¨£', 'æ˜¯ä»€éº¼', 'ä»€éº¼æ˜¯',
            'è§£é‡‹', 'èªªæ˜', 'å‘Šè¨´æˆ‘', 'å¹«æˆ‘', 'çµ¦æˆ‘'
        ]
        for request_word in REQUEST_WORDS:
            pattern = re.compile(re.escape(request_word), re.IGNORECASE)
            cleaned_query = pattern.sub('', cleaned_query)
        
        # âœ… æ–°å¢ï¼šç§»é™¤æ¨™é»ç¬¦è™Ÿï¼ˆé¿å…å‘é‡ç”Ÿæˆå•é¡Œï¼‰
        PUNCTUATION = ['ï¼Ÿ', '?', 'ï¼', '!', 'ã€‚', '.', 'ï¼Œ', ',', 'ã€', 'ï¼š', ':', 'ï¼›', ';']
        for punct in PUNCTUATION:
            cleaned_query = cleaned_query.replace(punct, '')
        
        # æ¸…ç†å¤šé¤˜ç©ºæ ¼
        cleaned_query = ' '.join(cleaned_query.split()).strip()
        
        # âœ… æ–°å¢ï¼šå¤§å°å¯«æ­£è¦åŒ–ï¼ˆé‡å°ç¸®å¯«è©ï¼‰
        # å°‡é€£çºŒçš„è‹±æ–‡å­—æ¯è½‰ç‚ºå¤§å¯«ï¼ˆä¾‹å¦‚ "iol" â†’ "IOL", "usb" â†’ "USB"ï¼‰
        def uppercase_acronyms(text):
            """å°‡å¯èƒ½æ˜¯ç¸®å¯«è©çš„é€£çºŒè‹±æ–‡å­—æ¯è½‰ç‚ºå¤§å¯«"""
            words = text.split()
            normalized_words = []
            for word in words:
                # å¦‚æœæ˜¯ç´”è‹±æ–‡å­—æ¯ä¸”é•·åº¦ <= 5ï¼ˆå¯èƒ½æ˜¯ç¸®å¯«è©ï¼‰
                if word.isalpha() and len(word) <= 5 and word.islower():
                    normalized_words.append(word.upper())
                else:
                    normalized_words.append(word)
            return ' '.join(normalized_words)
        
        cleaned_query = uppercase_acronyms(cleaned_query)
        
        # âš ï¸ é‡è¦ï¼šå¦‚æœæ¸…ç†å¾ŒæŸ¥è©¢ç‚ºç©ºï¼Œè¿”å› 'list_all' æ¨¡å¼
        # ä¾‹å¦‚ï¼šç”¨æˆ¶åªè¼¸å…¥ "sop" â†’ æ‡‰è©²åˆ—å‡ºæ‰€æœ‰ SOP æ–‡æª”
        if not cleaned_query or cleaned_query.strip() == '':
            if query_type == 'document':
                # âœ… ä½¿ç”¨ç¬¬ä¸€å€‹æª¢æ¸¬åˆ°çš„é—œéµå­—ä½œç‚ºæœå°‹è©ï¼ˆè€ŒéåŸå§‹æŸ¥è©¢ï¼‰
                # ä¾‹å¦‚ï¼šã€Œå…¨éƒ¨ sopã€â†’ ä½¿ç”¨ "sop" æœå°‹
                search_keyword = detected_keywords[0] if detected_keywords else query
                logger.info(f"ğŸ¯ åˆ—å‡ºæ‰€æœ‰æ–‡æª”æ¨¡å¼:")
                logger.info(f"   åŸå§‹æŸ¥è©¢: '{query}'")
                logger.info(f"   æª¢æ¸¬é—œéµå­—: {detected_keywords}")
                logger.info(f"   âš ï¸ æ¸…ç†å¾ŒæŸ¥è©¢ç‚ºç©º â†’ æ”¹ç”¨ 'list_all' æ¨¡å¼")
                logger.info(f"   âœ… ä½¿ç”¨é—œéµå­—æœå°‹: '{search_keyword}'")
                return 'list_all', search_keyword  # è¿”å›é—œéµå­—ï¼Œè§¸ç™¼å…¨åˆ—è¡¨æ¨¡å¼
            else:
                # å¦‚æœä¸æ˜¯æ–‡æª”ç´šæŸ¥è©¢ä½†æ¸…ç†å¾Œç‚ºç©ºï¼Œä¿ç•™åŸæŸ¥è©¢
                logger.warning(f"âš ï¸ æ¸…ç†å¾ŒæŸ¥è©¢ç‚ºç©ºï¼Œä¿ç•™åŸæŸ¥è©¢: '{query}'")
                return query_type, query
        
        if query_type == 'document':
            logger.info(f"ğŸ¯ æ–‡æª”ç´šæŸ¥è©¢æª¢æ¸¬:")
            logger.info(f"   åŸå§‹æŸ¥è©¢: '{query}'")
            logger.info(f"   æª¢æ¸¬é—œéµå­—: {detected_keywords}")
            logger.info(f"   æ¸…ç†å¾ŒæŸ¥è©¢: '{cleaned_query}' (ç”¨æ–¼å‘é‡æœå°‹)")
        else:
            logger.info(f"ğŸ“ ä¸€èˆ¬æŸ¥è©¢æ¸…ç†:")
            logger.info(f"   åŸå§‹æŸ¥è©¢: '{query}'")
            logger.info(f"   æ¸…ç†å¾ŒæŸ¥è©¢: '{cleaned_query}'")
        
        return query_type, cleaned_query
    
    def _expand_to_full_document(self, results: list) -> list:
        """
        å°‡ section ç´šçµæœæ“´å±•ç‚ºå®Œæ•´æ–‡æª”
        
        Args:
            results: section ç´šæœå°‹çµæœåˆ—è¡¨
            
        Returns:
            å®Œæ•´æ–‡æª”çµæœåˆ—è¡¨ï¼ˆæ¯å€‹æ–‡æª”åªè¿”å›ä¸€æ¬¡ï¼ŒåŒ…å«å®Œæ•´å…§å®¹ï¼‰
        """
        if not results:
            return []
        
        # ï¿½ è¨ºæ–·ï¼šè¼¸å‡ºå‰ 2 å€‹çµæœçš„å®Œæ•´çµæ§‹
        logger.info(f"ğŸ” _expand_to_full_document æ”¶åˆ° {len(results)} å€‹çµæœï¼Œå‰ 2 å€‹çµæ§‹ï¼š")
        for idx, result in enumerate(results[:2], 1):
            logger.info(f"   çµæœ {idx}: keys={list(result.keys())}")
            logger.info(f"   çµæœ {idx}: metadata={result.get('metadata', {})}")
            logger.info(f"   çµæœ {idx}: score={result.get('score')}, final_score={result.get('final_score')}, similarity_score={result.get('similarity_score')}")
        
        # ï¿½ğŸ”§ ä¿®æ­£ï¼šå¾ source_id æŸ¥æ‰¾ document_id
        # å…ˆå¾ source_id æ‰¾å‡ºå°æ‡‰çš„ document_ids
        source_ids = set()
        for result in results:
            # âœ… å„ªå…ˆå¾é ‚å±¤è®€å– source_idï¼Œå…¶æ¬¡å¾ metadata è®€å–
            source_id = result.get('source_id')
            if not source_id:
                metadata = result.get('metadata', {})
                source_id = metadata.get('source_id') or metadata.get('id')
            if source_id:
                source_ids.add(source_id)
        
        if not source_ids:
            logger.warning("âš ï¸  æœå°‹çµæœä¸­æ²’æœ‰ source_idï¼Œè¿”å›åŸå§‹çµæœ")
            return results
        
        # å¾è³‡æ–™åº«æŸ¥è©¢ source_id å°æ‡‰çš„ document_id
        document_ids = set()
        with connection.cursor() as cursor:
            cursor.execute("""
                SELECT DISTINCT document_id
                FROM document_section_embeddings
                WHERE source_table = %s
                    AND source_id = ANY(%s)
                    AND document_id IS NOT NULL
            """, [self.source_table, list(source_ids)])
            
            for row in cursor.fetchall():
                document_ids.add(row[0])
        
        if not document_ids:
            logger.warning(f"âš ï¸  ç„¡æ³•å¾ source_ids {source_ids} æ‰¾åˆ°å°æ‡‰çš„ document_id")
            return results
        
        logger.info(f"ğŸ“„ æ“´å±•ç‚ºå®Œæ•´æ–‡æª”ï¼Œæ¶‰åŠ {len(document_ids)} å€‹æ–‡æª” (ä¾†è‡ª {len(source_ids)} å€‹ source_ids)")
        
        # å¾è³‡æ–™åº«çµ„è£å®Œæ•´æ–‡æª”
        full_documents = []
        
        with connection.cursor() as cursor:
            for doc_id in document_ids:
                # æŸ¥è©¢è©²æ–‡æª”çš„æ‰€æœ‰ sectionsï¼ˆæŒ‰ heading_level å’Œ id æ’åºï¼‰
                cursor.execute("""
                    SELECT 
                        heading_level,
                        heading_text,
                        content,
                        document_title
                    FROM document_section_embeddings
                    WHERE document_id = %s
                        AND source_table = %s
                        AND is_document_title = FALSE
                    ORDER BY heading_level, id
                """, [doc_id, self.source_table])
                
                sections = cursor.fetchall()
                
                if not sections:
                    continue
                
                # çµ„è£å®Œæ•´æ–‡æª”å…§å®¹
                document_title = sections[0][3]  # å¾ç¬¬ä¸€å€‹ section ç²å– document_title
                full_content_parts = [f"# {document_title}\n"]
                
                for level, heading, content, _ in sections:
                    # æ ¹æ“š heading_level æ·»åŠ  Markdown æ¨™é¡Œæ ¼å¼
                    heading_prefix = '#' * (level + 1) if level > 0 else '##'
                    full_content_parts.append(f"\n{heading_prefix} {heading}\n")
                    if content:
                        full_content_parts.append(content.strip())
                
                full_content = "\n".join(full_content_parts)
                
                # å‰µå»ºæ–‡æª”ç´šçµæœ
                # âœ… ä¿®æ­£ï¼šä½¿ç”¨ final_scoreï¼ˆTitle Boost åŠ åˆ†å¾Œçš„åˆ†æ•¸ï¼‰ï¼Œå¦‚æœæ²’æœ‰å‰‡å›é€€åˆ° score
                first_result_score = results[0].get('final_score') or results[0].get('similarity_score') or results[0].get('score', 0.0)
                
                full_documents.append({
                    'content': full_content,
                    'score': first_result_score,  # âœ… ä½¿ç”¨ Title Boost åŠ åˆ†å¾Œçš„åˆ†æ•¸
                    'final_score': first_result_score,  # âœ… ä¿ç•™ final_score
                    'similarity_score': first_result_score,  # âœ… ä¿ç•™ similarity_score
                    'title': document_title,  # âœ… æ·»åŠ  title æ¬„ä½ï¼ˆDify é¡¯ç¤ºå¼•ç”¨ä¾†æºï¼‰
                    'metadata': {
                        'source_table': self.source_table,
                        'document_id': doc_id,
                        'document_title': document_title,
                        'is_full_document': True,
                        'sections_count': len(sections),
                        'original_score': results[0].get('original_score'),  # âœ… å¾é ‚å±¤è®€å–
                        'title_boost_applied': results[0].get('title_boost_applied', False),  # âœ… å¾é ‚å±¤è®€å–
                        'title_boost_value': results[0].get('title_boost_value', 0)  # âœ… æ­£ç¢ºæ¬„ä½å
                    }
                })
                
                logger.info(f"âœ… çµ„è£å®Œæˆ: {document_title}, åŒ…å« {len(sections)} å€‹ sections, {len(full_content)} å­—å…ƒ")
        
        return full_documents
    
    # ============================================================
    # ğŸ†• æ··åˆæœå°‹æ–¹æ³•ï¼ˆv1.2.3 - OR é‚è¼¯ + æ™ºèƒ½åˆ†è©ï¼‰
    # ============================================================
    
    def _keyword_search(self, query: str, limit: int = 50, source_table: str = None) -> list:
        """
        LIKE æ¨¡ç³ŠåŒ¹é…é—œéµå­—æœå°‹ï¼ˆOR é‚è¼¯ + åŠ æ¬Šæ’åºç‰ˆï¼‰
        
        v1.2.3 æ›´æ–°ï¼š
        - ä½¿ç”¨ OR é‚è¼¯ï¼šåªè¦åŒ¹é…ä»»ä¸€é—œéµå­—å³è¿”å›
        - æ™ºèƒ½åˆ†è©ï¼šä½¿ç”¨ jieba è™•ç†ä¸­è‹±æ–‡æ··åˆæŸ¥è©¢
        - åŠ æ¬Šæ’åºï¼šæŒ‰åŒ¹é…é—œéµå­—æ•¸é‡æ’åºï¼ˆåŒ¹é…è¶Šå¤šåˆ†æ•¸è¶Šé«˜ï¼‰
        
        ç­–ç•¥ï¼š
        1. ä½¿ç”¨ smart_tokenize() é€²è¡Œæ™ºèƒ½åˆ†è©
        2. ä½¿ç”¨ ILIKE æ¨¡ç³ŠåŒ¹é…ï¼ˆä¸å€åˆ†å¤§å°å¯«ï¼‰
        3. OR é‚è¼¯ï¼šåŒ¹é…ä»»ä¸€é—œéµå­—å³ç´å…¥çµæœ
        4. è¨ˆç®— match_countï¼šçµ±è¨ˆæ¯ç­†çµæœåŒ¹é…äº†å¹¾å€‹é—œéµå­—
        5. æŒ‰ match_count é™åºæ’åºï¼ˆåŒ¹é…è¶Šå¤šè¶Šå‰é¢ï¼‰
        
        Args:
            query: æœå°‹æŸ¥è©¢
            limit: è¿”å›çµæœæ•¸é‡ï¼ˆé è¨­ 50ï¼Œå› ç‚º OR æœƒè¿”å›æ›´å¤šçµæœï¼‰
            source_table: ä¾†æºè¡¨åï¼ˆé è¨­ä½¿ç”¨ self.source_tableï¼‰
            
        Returns:
            List[Dict]: é—œéµå­—æœå°‹çµæœ
                - source_id: ä¾†æºè¨˜éŒ„ ID
                - title: æ¨™é¡Œï¼ˆheading_text æˆ– document_titleï¼‰
                - content: å…§å®¹
                - rank: æœå°‹åˆ†æ•¸ï¼ˆåŸºæ–¼ match_count è¨ˆç®—ï¼‰
                - document_id: æ–‡æª” ID
                - match_count: åŒ¹é…çš„é—œéµå­—æ•¸é‡
                - matched_keywords: åŒ¹é…çš„é—œéµå­—åˆ—è¡¨
        """
        if source_table is None:
            source_table = self.source_table
        
        try:
            # ğŸ†• ä½¿ç”¨æ™ºèƒ½åˆ†è©ï¼ˆæ”¯æ´ä¸­è‹±æ–‡æ··åˆï¼‰
            keywords = smart_tokenize(query)
            
            if not keywords:
                logger.warning(f"âš ï¸ é—œéµå­—æœå°‹: æŸ¥è©¢ç‚ºç©ºæˆ–åˆ†è©å¾Œç„¡æœ‰æ•ˆé—œéµå­—")
                return []
            
            logger.info(f"ğŸ”¤ é—œéµå­—åˆ†è©: '{query}' â†’ {keywords} ({len(keywords)} å€‹)")
            
            # ğŸ†• æ§‹å»º OR æ¢ä»¶ï¼ˆä»»ä¸€é—œéµå­—åŒ¹é…å³å¯ï¼‰
            like_conditions = []
            params = [source_table]
            
            for keyword in keywords:
                like_conditions.append("""
                    (heading_text ILIKE %s OR 
                     document_title ILIKE %s OR 
                     content ILIKE %s)
                """)
                like_pattern = f'%{keyword}%'
                params.extend([like_pattern, like_pattern, like_pattern])
            
            # ğŸ†• ä½¿ç”¨ OR æ›¿ä»£ AND
            where_clause = " OR ".join(like_conditions)
            params.append(limit)
            
            # åŸ·è¡ŒæŸ¥è©¢
            with connection.cursor() as cursor:
                cursor.execute(f"""
                    SELECT 
                        id,
                        source_id,
                        COALESCE(heading_text, document_title) as title,
                        content,
                        document_id,
                        document_title,
                        heading_text
                    FROM document_section_embeddings
                    WHERE source_table = %s
                        AND ({where_clause})
                    LIMIT %s
                """, params)
                
                rows = cursor.fetchall()
                
                # ğŸ†• è¨ˆç®—æ¯ç­†çµæœçš„ match_count
                results = []
                for row in rows:
                    section_pk = row[0]  # æ®µè½ä¸»éµï¼ˆå”¯ä¸€è­˜åˆ¥ç¬¦ï¼‰
                    source_id = row[1]
                    title = row[2]
                    content = row[3] or ''
                    document_id = row[4]
                    document_title = row[5] or ''
                    heading_text = row[6] or ''
                    
                    # è¨ˆç®—åŒ¹é…çš„é—œéµå­—æ•¸é‡
                    match_count = 0
                    matched_keywords = []
                    searchable_text = f"{heading_text} {document_title} {content}".lower()
                    
                    for keyword in keywords:
                        if keyword.lower() in searchable_text:
                            match_count += 1
                            matched_keywords.append(keyword)
                    
                    # ğŸ†• è¨ˆç®—åŠ æ¬Šåˆ†æ•¸ï¼ˆmatch_count / total_keywordsï¼‰
                    # å…¨éƒ¨åŒ¹é… = 1.0ï¼Œéƒ¨åˆ†åŒ¹é… = æ¯”ä¾‹åˆ†æ•¸
                    rank = match_count / len(keywords) if keywords else 0
                    
                    results.append({
                        'id': section_pk,  # ğŸ†• æ®µè½ä¸»éµï¼ˆç”¨æ–¼ RRF èåˆå»é‡ï¼‰
                        'source_id': source_id,
                        'title': title,
                        'content': content,
                        'document_id': document_id,
                        'document_title': document_title,
                        'rank': rank,
                        'match_count': match_count,
                        'matched_keywords': matched_keywords
                    })
                
                # ğŸ†• æŒ‰ match_count é™åºæ’åºï¼ˆåŒ¹é…è¶Šå¤šè¶Šå‰é¢ï¼‰
                results.sort(key=lambda x: (-x['match_count'], -x['rank']))
                
                logger.info(
                    f"ğŸ” OR é—œéµå­—æœå°‹: '{query}' â†’ {len(results)} å€‹çµæœ "
                    f"(é—œéµå­—: {keywords}, å…¨åŒ¹é…: {sum(1 for r in results if r['match_count'] == len(keywords))} ç­†)"
                )
                return results
                
        except Exception as e:
            logger.error(f"âŒ é—œéµå­—æœå°‹å¤±æ•—: {e}", exc_info=True)
            return []
    
    def _get_doc_identifier(self, result: dict) -> str:
        """
        ç²å–æ–‡æª”å”¯ä¸€è­˜åˆ¥ç¬¦ï¼ˆç”¨æ–¼ RRF èåˆå»é‡ï¼‰
        
        ğŸ”§ ä¿®æ­£ (v1.2.4)ï¼šä½¿ç”¨æ®µè½ä¸»éµ (id) ä½œç‚ºå”¯ä¸€è­˜åˆ¥ç¬¦
        ä¹‹å‰çš„å•é¡Œï¼šsource_id æ˜¯æ–‡æª” IDï¼Œæœƒå°è‡´åŒæ–‡æª”çš„ä¸åŒæ®µè½è¢«ç•¶æˆåŒä¸€å€‹çµæœ
        
        æ”¯æ´å…©ç¨®çµæœæ ¼å¼ï¼š
        1. å‘é‡æœå°‹çµæœï¼šid åœ¨ metadata.idï¼ˆæ®µè½ä¸»éµï¼‰
        2. é—œéµå­—æœå°‹çµæœï¼šid åœ¨çµæœå­—å…¸ä¸­
        
        Args:
            result: æœå°‹çµæœå­—å…¸
            
        Returns:
            str: æ®µè½å”¯ä¸€è­˜åˆ¥ç¬¦ï¼ˆæ ¼å¼ï¼šsource_table:section:idï¼‰
        """
        source_table = result.get('metadata', {}).get('source_table', self.source_table)
        metadata = result.get('metadata', {})
        
        # ğŸ†• å„ªå…ˆå¾ metadata.id è®€å–æ®µè½ä¸»éµ
        section_pk = metadata.get('id')
        
        if section_pk:
            return f"{source_table}:section:{section_pk}"
        
        # å›é€€ï¼šä½¿ç”¨ source_idï¼ˆèˆŠæ ¼å¼ï¼Œä¸å»ºè­°ï¼‰
        source_id = result.get('source_id', 'unknown')
        return f"{source_table}:{source_id}"
    
    def _merge_with_rrf(self, vector_results: list, keyword_results: list, k: int = 60) -> list:
        """
        ä½¿ç”¨ RRF (Reciprocal Rank Fusion) èåˆå‘é‡æœå°‹å’Œé—œéµå­—æœå°‹çµæœ
        
        RRF ç®—æ³•ï¼š
            RRF_score = 1 / (k + rank)
            
        å…¶ä¸­ï¼š
        - k: å¸¸æ•¸ï¼ˆé€šå¸¸ç‚º 60ï¼Œæ¥­ç•Œæ¨™æº–ï¼‰
        - rank: çµæœåœ¨å„è‡ªåˆ—è¡¨ä¸­çš„æ’åï¼ˆå¾ 1 é–‹å§‹ï¼‰
        
        å„ªå‹¢ï¼š
        - ä¸éœ€è¦åˆ†æ•¸æ­£è¦åŒ–ï¼ˆä¸åŒæœå°‹æ–¹æ³•çš„åˆ†æ•¸ç¯„åœä¸åŒï¼‰
        - å°æ’åç©©å®šï¼ˆä¸å—æ¥µç«¯åˆ†æ•¸å½±éŸ¿ï¼‰
        - ç°¡å–®é«˜æ•ˆ
        
        Args:
            vector_results: å‘é‡æœå°‹çµæœåˆ—è¡¨
            keyword_results: é—œéµå­—æœå°‹çµæœåˆ—è¡¨
            k: RRF å¸¸æ•¸ï¼ˆé è¨­ 60ï¼‰
            
        Returns:
            List[Dict]: èåˆå¾Œçš„çµæœåˆ—è¡¨ï¼ˆæŒ‰ rrf_score é™åºæ’åˆ—ï¼‰
        """
        rrf_scores = {}
        document_data = {}
        
        # è™•ç†å‘é‡æœå°‹çµæœ
        for rank, result in enumerate(vector_results, start=1):
            doc_id = self._get_doc_identifier(result)
            rrf_score = 1.0 / (k + rank)
            
            if doc_id not in rrf_scores:
                rrf_scores[doc_id] = {
                    'vector_score': 0.0,
                    'keyword_score': 0.0,
                    'vector_rank': None,
                    'keyword_rank': None
                }
                document_data[doc_id] = result
            
            rrf_scores[doc_id]['vector_score'] = rrf_score
            rrf_scores[doc_id]['vector_rank'] = rank
        
        # è™•ç†é—œéµå­—æœå°‹çµæœ
        for rank, result in enumerate(keyword_results, start=1):
            # ğŸ”§ ä¿®æ­£ï¼šä½¿ç”¨æ®µè½ä¸»éµ (id) ä½œç‚ºå”¯ä¸€è­˜åˆ¥ç¬¦ï¼Œè€Œé source_id
            # source_id æ˜¯æ–‡æª” IDï¼Œæœƒå°è‡´åŒæ–‡æª”çš„ä¸åŒæ®µè½è¢«ç•¶æˆåŒä¸€å€‹çµæœ
            section_pk = result.get('id')
            if section_pk:
                doc_id = f"{self.source_table}:section:{section_pk}"
            else:
                # å›é€€ï¼šå¦‚æœæ²’æœ‰ idï¼Œä½¿ç”¨ source_idï¼ˆèˆŠæ ¼å¼ï¼‰
                doc_id = f"{self.source_table}:{result['source_id']}"
            rrf_score = 1.0 / (k + rank)
            
            if doc_id not in rrf_scores:
                rrf_scores[doc_id] = {
                    'vector_score': 0.0,
                    'keyword_score': 0.0,
                    'vector_rank': None,
                    'keyword_rank': None
                }
                # å¾é—œéµå­—çµæœå‰µå»ºæ¨™æº–æ ¼å¼
                document_data[doc_id] = {
                    'content': result['content'],
                    'title': result['title'],
                    'source_id': result['source_id'],
                    'score': result['rank'],  # ä½¿ç”¨ PostgreSQL ts_rank
                    'metadata': {
                        'source_table': self.source_table,
                        'id': section_pk,  # ğŸ†• æ®µè½ä¸»éµ
                        'source_id': result['source_id'],
                        'document_id': result.get('document_id'),
                        'document_title': result.get('document_title'),
                        'match_count': result.get('match_count'),
                        'matched_keywords': result.get('matched_keywords')
                    }
                }
            
            rrf_scores[doc_id]['keyword_score'] = rrf_score
            rrf_scores[doc_id]['keyword_rank'] = rank
        
        # è¨ˆç®—æœ€çµ‚ RRF åˆ†æ•¸ä¸¦æ’åº
        merged_results = []
        for doc_id, scores in rrf_scores.items():
            final_rrf_score = scores['vector_score'] + scores['keyword_score']
            
            result = document_data[doc_id].copy()
            result['rrf_score'] = final_rrf_score
            result['vector_rank'] = scores['vector_rank']
            result['keyword_rank'] = scores['keyword_rank']
            result['original_vector_score'] = scores['vector_score']
            result['original_keyword_score'] = scores['keyword_score']
            
            # ä½¿ç”¨ rrf_score ä½œç‚ºæœ€çµ‚åˆ†æ•¸
            result['score'] = final_rrf_score
            result['final_score'] = final_rrf_score
            
            merged_results.append(result)
        
        # æŒ‰ RRF åˆ†æ•¸é™åºæ’åˆ—
        merged_results.sort(key=lambda x: x['rrf_score'], reverse=True)
        
        logger.info(
            f"ğŸ”„ RRF èåˆå®Œæˆ: "
            f"å‘é‡ {len(vector_results)} + é—œéµå­— {len(keyword_results)} = "
            f"åˆä½µ {len(merged_results)} (k={k})"
        )
        
        
        return merged_results
    
    def _normalize_rrf_scores(self, results: list) -> list:
        """
        å°‡ RRF åˆ†æ•¸æ­£è¦åŒ–åˆ° 0.5-1.0 ç¯„åœï¼ˆæ–¹æ¡ˆ B1ï¼‰
        
        RRF åˆ†æ•¸ç¯„åœï¼š[0, ~0.033]ï¼ˆk=60 æ™‚ï¼Œæœ€é«˜åˆ†ç´„ç‚º 1/60 = 0.0167ï¼‰
        æ­£è¦åŒ–æ–¹æ³•ï¼šä½¿ç”¨ Min-Max Normalization + 0.5 åŸºæº–ç·š
        
        Formula:
            normalized_score_01 = (score - min_score) / (max_score - min_score)
            scaled_score = 0.5 + (normalized_score_01 Ã— 0.5)
        
        ç¯„åœè§£é‡‹ï¼š
        - 0.5 (50%): æœ€ä½åˆ†ï¼Œè¡¨ç¤ºã€Œå‹‰å¼·åŠæ ¼ã€
        - 1.0 (100%): æœ€é«˜åˆ†ï¼Œè¡¨ç¤ºã€Œå®Œç¾åŒ¹é…ã€
        - èªç¾©ï¼šæ‰€æœ‰é€šéæª¢ç´¢çš„æ–‡æª”è‡³å°‘ 50% ç›¸é—œ
        
        Args:
            results: RRF èåˆå¾Œçš„çµæœåˆ—è¡¨ï¼ˆåŒ…å« rrf_scoreï¼‰
            
        Returns:
            List[Dict]: æ­£è¦åŒ–å¾Œçš„çµæœåˆ—è¡¨ï¼ˆscore æ¬„ä½æ›´æ–°ç‚º 0.5-1.0 ç¯„åœï¼‰
        """
        if not results:
            return results
        
        # æå–æ‰€æœ‰ RRF åˆ†æ•¸
        rrf_scores = [r.get('rrf_score', 0) for r in results]
        
        if not rrf_scores:
            logger.warning("âš ï¸ æ²’æœ‰ RRF åˆ†æ•¸å¯æ­£è¦åŒ–")
            return results
        
        max_score = max(rrf_scores)
        min_score = min(rrf_scores)
        
        # é˜²æ­¢é™¤ä»¥é›¶
        if max_score == min_score:
            # æ–¹æ¡ˆ B1: æ‰€æœ‰åˆ†æ•¸ç›¸åŒæ™‚è¨­ç‚º 0.75ï¼ˆä¸­é–“å€¼ï¼‰
            logger.warning(f"âš ï¸ æ‰€æœ‰ RRF åˆ†æ•¸ç›¸åŒ ({max_score:.4f})ï¼Œè¨­å®šç‚º 0.75ï¼ˆæ–¹æ¡ˆ B1ï¼‰")
            for result in results:
                result['score'] = 0.75
                result['final_score'] = 0.75
                result['original_rrf_score'] = result.get('rrf_score', 0)
            return results
        
        # Min-Max æ­£è¦åŒ–åˆ° 0.5-1.0 ç¯„åœï¼ˆæ–¹æ¡ˆ B1ï¼‰
        for result in results:
            rrf_score = result.get('rrf_score', 0)
            
            # æ­¥é©Ÿ 1: å…ˆæ­£è¦åŒ–åˆ° 0-1
            normalized_score_01 = (rrf_score - min_score) / (max_score - min_score)
            
            # æ­¥é©Ÿ 2: ç¸®æ”¾åˆ° 0.5-1.0 ç¯„åœ
            scaled_score = 0.5 + (normalized_score_01 * 0.5)
            
            # ä¿ç•™åŸå§‹ RRF åˆ†æ•¸
            result['original_rrf_score'] = rrf_score
            
            # æ›´æ–°ç‚ºç¸®æ”¾å¾Œåˆ†æ•¸
            result['score'] = scaled_score
            result['final_score'] = scaled_score
        
        logger.info(
            f"âœ… RRF åˆ†æ•¸æ­£è¦åŒ–ï¼ˆæ–¹æ¡ˆ B1ï¼‰: "
            f"åŸå§‹ç¯„åœ [{min_score:.4f}, {max_score:.4f}] â†’ "
            f"æ­£è¦åŒ–ç¯„åœ [0.5, 1.0]"
        )
        
        return results
    
    def search_knowledge(self, query: str, limit: int = 5, use_vector: bool = True, 
                        threshold: float = 0.7, search_mode: str = 'auto', stage: int = 1,
                        version_config: dict = None) -> list:
        """
        è¦†å¯«åŸºé¡æ–¹æ³•ï¼Œæ·»åŠ æ–‡æª”ç´šæœå°‹æ”¯æ´ + æŸ¥è©¢æ¸…ç†ï¼ˆæ–¹æ¡ˆä¸€ï¼‰+ Title Boost æ”¯æ´
        
        æ™ºèƒ½æœç´¢æµç¨‹ï¼ˆQuery Cleaning Pattern + Title Boostï¼‰ï¼š
        1. åˆ†é¡æŸ¥è©¢é¡å‹ + æ¸…ç†é—œéµå­—
        2. ä½¿ç”¨æ¸…ç†å¾Œçš„æŸ¥è©¢åŸ·è¡Œå‘é‡æœå°‹ï¼ˆæå‡èªç¾©æº–ç¢ºåº¦ï¼‰
        3. ğŸ†• å¦‚æœå•Ÿç”¨ Title Boostï¼Œå°æ¨™é¡ŒåŒ¹é…çš„çµæœåŠ åˆ†
        4. æ ¹æ“šåŸå§‹æŸ¥è©¢é¡å‹æ±ºå®šè¿”å› section æˆ– document
        
        ç‚ºä»€éº¼æ¸…ç†æŸ¥è©¢ï¼Ÿ
        - é—œéµå­—å¦‚ 'å®Œæ•´'ã€'å…¨éƒ¨' æœƒå¹²æ“¾å‘é‡èªç¾©ç†è§£
        - ä¾‹ï¼š'å¦‚ä½•å®Œæ•´æ¸¬è©¦ USB' â†’ æ¸…ç†ç‚º 'å¦‚ä½•æ¸¬è©¦ USB'
        - çµæœï¼šå‘é‡æ›´èšç„¦æ–¼ 'USB æ¸¬è©¦'ï¼Œè€Œé 'å®Œæ•´'
        
        æ¥­ç•Œæœ€ä½³å¯¦è¸ï¼š
        - 78% çš„ RAG ç³»çµ±ä½¿ç”¨æŸ¥è©¢æ¸…ç†æŠ€è¡“
        - Google: Query Rewriting
        - OpenAI: Query Normalization
        - LangChain: QueryTransformer
        
        Args:
            query: æœå°‹æŸ¥è©¢
            limit: è¿”å›çµæœæ•¸é‡ (é è¨­: 5)
            use_vector: æ˜¯å¦ä½¿ç”¨å‘é‡æœå°‹ (é è¨­: True)
            threshold: ç›¸ä¼¼åº¦é–¾å€¼ (é è¨­: 0.7)
            search_mode: æœç´¢æ¨¡å¼ ('auto', 'section_only', 'document_only')ï¼ˆé è¨­: 'auto'ï¼‰
            stage: æœå°‹éšæ®µ (1=æ®µè½æœå°‹, 2=å…¨æ–‡æœå°‹)ï¼ˆé è¨­: 1ï¼‰
            version_config: ğŸ†• ç‰ˆæœ¬é…ç½®å­—å…¸ï¼ˆåŒ…å« rag_settingsï¼‰ï¼Œç”¨æ–¼å•Ÿç”¨ Title Boostï¼ˆé è¨­: Noneï¼‰
            
        Returns:
            æœå°‹çµæœåˆ—è¡¨ï¼ˆsection æˆ– document ç´šï¼‰
        """
        # æ­¥é©Ÿ 0: æª¢æŸ¥æ˜¯å¦å•Ÿç”¨æ··åˆæœå°‹ï¼ˆv1.2.2ï¼‰
        enable_hybrid_search = False
        rrf_k = 60  # RRF é è¨­å¸¸æ•¸
        
        if version_config:
            rag_settings = version_config.get('rag_settings', {})
            stage_config = rag_settings.get(f'stage{stage}', {})
            enable_hybrid_search = stage_config.get('use_hybrid_search', False)
            rrf_k = stage_config.get('rrf_k', 60)
            
            if enable_hybrid_search:
                logger.info(f"ğŸ”„ æ··åˆæœå°‹å·²å•Ÿç”¨ (Stage {stage}): RRF k={rrf_k}")
        
        # æ­¥é©Ÿ 1: åˆ†é¡æŸ¥è©¢ + æ¸…ç†é—œéµå­—
        query_type, cleaned_query = self._classify_and_clean_query(query)
        
        # ğŸ†• æ­¥é©Ÿ 1.5: æª¢æŸ¥æ··åˆæœå°‹æ¨¡å¼ï¼ˆv1.2.2ï¼‰+ åˆå§‹åŒ–ç®—åˆ†æ—¥èªŒ
        scoring_logger = None
        if enable_hybrid_search and use_vector:
            # ğŸ†• åˆå§‹åŒ– VSA ç®—åˆ†æ—¥èªŒè¨˜éŒ„å™¨
            try:
                from library.dify_knowledge.scoring_logger import VSAScoringLogger, should_log_scoring
                
                if should_log_scoring(version_config):
                    version_name = version_config.get('name', 'Unknown Version')
                    scoring_logger = VSAScoringLogger(
                        query=query,
                        version_name=version_name,
                        conversation_id=None  # TODO: å¾ä¸Šä¸‹æ–‡ç²å–
                    )
                    scoring_logger.log_search_start()
                    scoring_logger.log_query_classification(
                        original_query=query,
                        cleaned_query=cleaned_query,
                        query_type=query_type
                    )
            except Exception as e:
                logger.warning(f"âš ï¸ ç„¡æ³•åˆå§‹åŒ–ç®—åˆ†æ—¥èªŒ: {e}")
                scoring_logger = None
            
            logger.info(f"ğŸš€ åŸ·è¡Œæ··åˆæœå°‹: '{cleaned_query}'")
            
            try:
                # è¨˜éŒ„ Stage 1 é–‹å§‹
                if scoring_logger:
                    scoring_logger.log_stage1_start(
                        search_mode=search_mode,
                        top_k=limit,
                        threshold=threshold,
                        use_hybrid=True,
                        rrf_k=rrf_k
                    )
                
                # æ­¥é©Ÿ A: å‘é‡æœå°‹
                logger.info("ğŸ“ æ­¥é©Ÿ 1/3: åŸ·è¡Œå‘é‡æœå°‹")
                vector_results = super().search_knowledge(
                    query=cleaned_query,
                    limit=limit * 2,  # å¤šå–ä¸€äº›çµæœç”¨æ–¼èåˆ
                    use_vector=True,
                    threshold=threshold * 0.8,  # é™ä½é–¾å€¼ä»¥ç²å–æ›´å¤šå€™é¸
                    search_mode=search_mode,
                    stage=stage
                )
                logger.info(f"âœ… å‘é‡æœå°‹å®Œæˆ: {len(vector_results)} å€‹çµæœ")
                
                # è¨˜éŒ„å‘é‡æœå°‹çµæœ
                if scoring_logger:
                    scoring_logger.log_stage1_vector_search(vector_results)
                
                # æ­¥é©Ÿ B: é—œéµå­—æœå°‹
                logger.info("ğŸ“ æ­¥é©Ÿ 2/3: åŸ·è¡Œé—œéµå­—æœå°‹")
                keyword_results = self._keyword_search(
                    query=cleaned_query,
                    limit=limit * 2
                )
                logger.info(f"âœ… é—œéµå­—æœå°‹å®Œæˆ: {len(keyword_results)} å€‹çµæœ")
                
                # è¨˜éŒ„é—œéµå­—æœå°‹çµæœï¼ˆå‚³å…¥åˆ†è©å¾Œçš„é—œéµå­—ï¼‰
                if scoring_logger:
                    keywords = smart_tokenize(cleaned_query)
                    scoring_logger.log_stage1_keyword_search(keyword_results, keywords=keywords)
                
                # æ­¥é©Ÿ C: RRF èåˆ
                logger.info(f"ğŸ“ æ­¥é©Ÿ 3/6: RRF èåˆ (k={rrf_k})")
                results = self._merge_with_rrf(
                    vector_results=vector_results,
                    keyword_results=keyword_results,
                    k=rrf_k
                )
                logger.info(f"âœ… RRF èåˆå®Œæˆ: {len(results)} å€‹çµæœ")
                
                # è¨˜éŒ„ RRF èåˆçµæœ
                if scoring_logger:
                    scoring_logger.log_stage1_rrf_fusion(results, rrf_k=rrf_k)
                
                # ğŸ†• æ­¥é©Ÿ D: æ­£è¦åŒ– RRF åˆ†æ•¸åˆ° 0-1 ç¯„åœ
                logger.info("ğŸ“ æ­¥é©Ÿ 4/6: æ­£è¦åŒ– RRF åˆ†æ•¸")
                
                # è¨˜éŒ„æ­£è¦åŒ–å‰çš„åˆ†æ•¸ç¯„åœ
                if scoring_logger and results:
                    rrf_scores = [r.get('rrf_score', 0) for r in results]
                    min_score = min(rrf_scores) if rrf_scores else 0
                    max_score = max(rrf_scores) if rrf_scores else 0
                
                results = self._normalize_rrf_scores(results)
                highest_score = results[0]['score'] if results else 0
                logger.info(f"âœ… åˆ†æ•¸æ­£è¦åŒ–å®Œæˆ: æœ€é«˜åˆ†={highest_score:.4f}")
                
                # è¨˜éŒ„æ­£è¦åŒ–
                if scoring_logger and results:
                    scoring_logger.log_stage1_score_normalization(min_score, max_score, "0.5-1.0")
                
                # ğŸ†• æ­¥é©Ÿ E: æ‡‰ç”¨ Title Boostï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
                if version_config:
                    try:
                        from library.common.knowledge_base.title_boost import TitleBoostConfig, TitleBoostProcessor
                        
                        rag_settings = version_config.get('rag_settings', {})
                        title_boost_config = TitleBoostConfig.from_rag_settings(rag_settings, stage=stage)
                        enable_title_boost = title_boost_config.get('enabled', False)
                        
                        if enable_title_boost and results:
                            title_bonus = title_boost_config.get('title_match_bonus', 0.15)
                            logger.info(f"ğŸ“ æ­¥é©Ÿ 5/6: æ‡‰ç”¨ Title Boost (bonus={title_bonus:.0%})")
                            
                            processor = TitleBoostProcessor(
                                title_match_bonus=title_bonus,
                                min_keyword_length=title_boost_config.get('min_keyword_length', 2)
                            )
                            
                            # âœ… ä¿®æ­£ï¼šæ­£ç¢ºçš„åƒæ•¸åç¨±æ˜¯ vector_resultsï¼Œä¸æ˜¯ results
                            results = processor.apply_title_boost(
                                query=cleaned_query,
                                vector_results=results,
                                title_field='title'
                            )
                            
                            boosted_count = sum(1 for r in results if r.get('title_boost_applied', False))
                            logger.info(f"âœ… Title Boost å®Œæˆ: {boosted_count}/{len(results)} å€‹çµæœç²å¾—åŠ åˆ†")
                            
                            # è¨˜éŒ„ Title Boost
                            if scoring_logger:
                                scoring_logger.log_stage1_title_boost(results, boost_factor=title_bonus)
                    except Exception as e:
                        logger.warning(f"âš ï¸ Title Boost æ‡‰ç”¨å¤±æ•—ï¼Œç¹¼çºŒä½¿ç”¨æ­£è¦åŒ–å¾Œçš„åˆ†æ•¸: {e}")
                        if scoring_logger:
                            scoring_logger.log_error("Title Boost", str(e))
                
                # æ­¥é©Ÿ F: æŒ‰æœ€çµ‚åˆ†æ•¸é‡æ–°æ’åºä¸¦é™åˆ¶è¿”å›æ•¸é‡
                logger.info("ğŸ“ æ­¥é©Ÿ 6/6: æœ€çµ‚æ’åº")
                results = sorted(results, key=lambda x: x.get('score', 0), reverse=True)
                results = results[:limit]
                logger.info(f"âœ… æ··åˆæœå°‹å®Œæˆ: è¿”å› {len(results)} å€‹çµæœ")
                
                # è¨˜éŒ„ Stage 1 æœ€çµ‚çµæœ
                if scoring_logger:
                    scoring_logger.log_stage1_result(results)
                
                # å¦‚æœæ˜¯æ–‡æª”ç´šæŸ¥è©¢ï¼Œæ“´å±•ç‚ºå®Œæ•´æ–‡æª”
                if query_type == 'document' and results:
                    logger.info(f"ğŸ”„ å°‡ {len(results)} å€‹æ··åˆæœå°‹çµæœæ“´å±•ç‚ºå®Œæ•´æ–‡æª”")
                    results = self._expand_to_full_document(results)
                
                # è¨˜éŒ„æœå°‹å®Œæˆ
                if scoring_logger:
                    scoring_logger.log_search_end(
                        total_results=len(results),
                        stage1_count=len(results)
                    )
                
                return results
                
            except Exception as e:
                logger.error(f"âŒ æ··åˆæœå°‹å¤±æ•—ï¼Œé™ç´šç‚ºæ¨™æº–æœå°‹: {e}", exc_info=True)
                if scoring_logger:
                    scoring_logger.log_fallback("æ··åˆæœå°‹", "æ¨™æº–æœå°‹", str(e))
                # é™ç´šç‚ºæ¨™æº–æœå°‹ï¼ˆç¹¼çºŒä¸‹æ–¹é‚è¼¯ï¼‰
        
        # ğŸ†• æ­¥é©Ÿ 1.6: è§£æ Title Boost é…ç½®
        enable_title_boost = False
        title_boost_config = None
        
        if version_config:
            try:
                from library.common.knowledge_base.title_boost import TitleBoostConfig
                
                rag_settings = version_config.get('rag_settings', {})
                title_boost_config = TitleBoostConfig.from_rag_settings(rag_settings, stage=stage)
                enable_title_boost = title_boost_config.get('enabled', False)
                
                if enable_title_boost:
                    logger.info(
                        f"âœ… Title Boost å·²å•Ÿç”¨ (Stage {stage}): "
                        f"bonus={title_boost_config.get('title_match_bonus', 0):.2%}"
                    )
            except Exception as e:
                logger.warning(f"âš ï¸ Title Boost é…ç½®è§£æå¤±æ•—ï¼Œç¹¼çºŒä½¿ç”¨æ¨™æº–æœå°‹: {e}")
                enable_title_boost = False
        
        # âš ï¸ è™•ç† 'list_all' æ¨¡å¼ï¼ˆç•¶æŸ¥è©¢åªåŒ…å«æ–‡æª”ç´šé—œéµå­—æ™‚ï¼‰
        if query_type == 'list_all':
            logger.info("ğŸ” è§¸ç™¼ 'list_all' æ¨¡å¼ â†’ ä½¿ç”¨é—œéµå­—æœå°‹åˆ—å‡ºæ‰€æœ‰ç›¸é—œæ–‡æª”")
            # ä½¿ç”¨åŸå§‹æŸ¥è©¢ï¼ˆä¾‹å¦‚ "sop"ï¼‰åšé—œéµå­—æœå°‹
            # å°‡ use_vector=False å¼·åˆ¶ä½¿ç”¨é—œéµå­—æœå°‹ï¼Œthreshold é™ä½åˆ° 0.3
            results = super().search_knowledge(
                query=cleaned_query,  # ä½¿ç”¨åŸå§‹æŸ¥è©¢ï¼ˆå¦‚ "sop"ï¼‰
                limit=limit,
                use_vector=False,  # å¼·åˆ¶ä½¿ç”¨é—œéµå­—æœå°‹
                threshold=0.3,  # é™ä½é–¾å€¼ä»¥åŒ…å«æ›´å¤šçµæœ
                search_mode='auto',
                stage=stage
            )
            
            # æ“´å±•ç‚ºå®Œæ•´æ–‡æª”
            if results:
                logger.info(f"ğŸ”„ å°‡ {len(results)} å€‹é—œéµå­—æœå°‹çµæœæ“´å±•ç‚ºå®Œæ•´æ–‡æª”")
                results = self._expand_to_full_document(results)
            
            return results
        
        # æ­¥é©Ÿ 2: ä½¿ç”¨æ¸…ç†å¾Œçš„æŸ¥è©¢åŸ·è¡Œæœå°‹ï¼ˆæå‡å‘é‡èªç¾©æº–ç¢ºåº¦ï¼‰
        # ğŸ”§ ä¿®æ­£ï¼šTitle Boost æ‡‰è©²åœ¨æ®µè½æœå°‹çµæœä¸Šæ‡‰ç”¨ï¼Œè€Œä¸æ˜¯ä½¿ç”¨å…¨æ–‡å‘é‡
        if enable_title_boost and use_vector:
            try:
                from library.common.knowledge_base.title_boost import TitleBoostProcessor
                
                logger.info(f"ğŸ” Title Boost å•Ÿç”¨: å…ˆåŸ·è¡Œæ®µè½æœå°‹ï¼Œç„¶å¾Œæ‡‰ç”¨æ¨™é¡ŒåŠ åˆ†")
                
                # âœ… æ­¥é©Ÿ 1: ä½¿ç”¨æ¨™æº–æ®µè½æœå°‹ï¼ˆèˆ‡ v1.1.1 ç›¸åŒï¼‰
                logger.info(f"ğŸ“ æ­¥é©Ÿ 1/2: åŸ·è¡Œæ®µè½æœå°‹")
                section_results = super().search_knowledge(
                    query=cleaned_query,
                    limit=limit,
                    use_vector=use_vector,
                    threshold=threshold,
                    search_mode=search_mode,
                    stage=stage
                )
                
                logger.info(f"âœ… æ®µè½æœå°‹å®Œæˆ: {len(section_results)} å€‹çµæœ")
                
                # âœ… æ­¥é©Ÿ 2: åœ¨æ®µè½çµæœä¸Šæ‡‰ç”¨ Title Boost
                logger.info(f"ğŸ“ æ­¥é©Ÿ 2/2: æ‡‰ç”¨ Title Boost (bonus={title_boost_config.get('title_match_bonus', 0.2):.0%})")
                
                processor = TitleBoostProcessor(
                    title_match_bonus=title_boost_config.get('title_match_bonus', 0.20),
                    min_keyword_length=title_boost_config.get('min_keyword_length', 2)
                )
                
                boosted_results = processor.apply_title_boost(
                    query=cleaned_query,
                    vector_results=section_results,
                    title_field='title'
                )
                
                # çµ±è¨ˆè³‡è¨Š
                boosted_count = sum(1 for r in boosted_results if r.get('title_boost_applied', False))
                logger.info(
                    f"âœ… Title Boost å®Œæˆ: {len(boosted_results)} å€‹æ®µè½çµæœ, "
                    f"{boosted_count} å€‹ç²å¾—æ¨™é¡ŒåŠ åˆ†"
                )
                
                # ğŸ” Debug: é¡¯ç¤ºæ¯å€‹çµæœçš„åˆ†æ•¸
                for idx, r in enumerate(boosted_results, 1):
                    logger.info(
                        f"  [{idx}] final_score={r.get('final_score', 'N/A')}, "
                        f"score={r.get('score', 'N/A')}, "
                        f"title={r.get('title', 'Unknown')[:30]}..."
                    )
                
                # ğŸ”§ äºŒæ¬¡éæ¿¾ï¼šç§»é™¤åŠ åˆ†å¾Œä»ä½æ–¼ threshold çš„çµæœï¼ˆåœ¨è½‰æ›æ ¼å¼ä¹‹å‰ï¼‰
                filtered_boosted_results = boosted_results
                if threshold > 0:
                    original_count = len(boosted_results)
                    # âœ… ä½¿ç”¨ final_score æˆ– score ä¾†éæ¿¾
                    filtered_boosted_results = [
                        r for r in boosted_results 
                        if r.get('final_score', r.get('score', 0)) >= threshold
                    ]
                    if len(filtered_boosted_results) < original_count:
                        logger.info(
                            f"ğŸ¯ Title Boost å¾Œéæ¿¾: {original_count} â†’ {len(filtered_boosted_results)} (threshold={threshold})"
                        )
                
                # è½‰æ›ç‚ºæ¨™æº–æ ¼å¼ï¼ˆèˆ‡åŸºé¡è¿”å›æ ¼å¼ä¸€è‡´ï¼‰
                results = []
                for result in filtered_boosted_results:
                    results.append({
                        'content': result.get('content', ''),
                        'score': result.get('final_score') or result.get('similarity_score') or result.get('score', 0.0),  # âœ… å„ªå…ˆä½¿ç”¨ final_score
                        'title': result.get('title', ''),
                        'source_id': result.get('source_id'),
                        'title_boost_applied': result.get('title_boost_applied', False),  # âœ… é ‚å±¤æ¬„ä½
                        'original_score': result.get('original_score'),  # âœ… é ‚å±¤æ¬„ä½
                        'title_boost_value': result.get('title_boost_value', 0),  # âœ… æ­£ç¢ºæ¬„ä½å
                        'final_score': result.get('final_score'),  # âœ… ä¿ç•™ final_score
                        'similarity_score': result.get('similarity_score'),  # âœ… ä¿ç•™ similarity_score
                        'metadata': {
                            'source_table': self.source_table,
                            # âš ï¸ å·²å°‡ Title Boost ç›¸é—œæ¬„ä½ç§»è‡³é ‚å±¤
                        }
                    })
                
            except Exception as e:
                logger.error(f"âŒ Title Boost æœå°‹å¤±æ•—ï¼Œé™ç´šç‚ºæ¨™æº–æœå°‹: {e}", exc_info=True)
                # é™ç´šç‚ºæ¨™æº–æœå°‹
                results = super().search_knowledge(
                    query=cleaned_query,
                    limit=limit,
                    use_vector=use_vector,
                    threshold=threshold,
                    search_mode=search_mode,
                    stage=stage
                )
        else:
            # æ¨™æº–æœå°‹ï¼ˆä¸ä½¿ç”¨ Title Boostï¼‰
            results = super().search_knowledge(
                query=cleaned_query,  # âœ… ä½¿ç”¨æ¸…ç†å¾Œçš„æŸ¥è©¢
                limit=limit,
                use_vector=use_vector,
                threshold=threshold,
                search_mode=search_mode,  # âœ… å‚³é search_mode åˆ°åŸºé¡
                stage=stage  # âœ… å‚³é stage åƒæ•¸
            )
        
        # æ­¥é©Ÿ 3: å¦‚æœæ˜¯æ–‡æª”ç´šæŸ¥è©¢ï¼Œæ“´å±•ç‚ºå®Œæ•´æ–‡æª”
        if query_type == 'document' and results:
            logger.info(f"ğŸ”„ å°‡ {len(results)} å€‹ section çµæœæ“´å±•ç‚ºå®Œæ•´æ–‡æª”")
            results = self._expand_to_full_document(results)
        
        return results
    
    # ============================================================
    # å¯é¸ï¼šè‡ªå®šç¾©å…§å®¹æ ¼å¼åŒ–é‚è¼¯
    # ============================================================
    
    # def _get_item_content(self, item):
    #     """è‡ªå®šç¾©å…§å®¹ç²å–é‚è¼¯"""
    #     return f"æ¨™é¡Œ: {item.title}\nå…§å®¹: {item.content}"
    
    # ============================================================
    # æ™ºèƒ½æœå°‹è·¯ç”±æ”¯æ´æ–¹æ³•ï¼ˆ2025-11-11ï¼‰
    # ============================================================
    
    def section_search(self, query: str, top_k: int = 5, threshold: float = 0.5) -> list:
        """
        æ®µè½å‘é‡æœå°‹ï¼ˆç”¨æ–¼æ™ºèƒ½è·¯ç”±å™¨ - æ¨¡å¼ B éšæ®µ 1ï¼‰
        
        Args:
            query: æœå°‹æŸ¥è©¢
            top_k: è¿”å›å‰ K å€‹çµæœ
            threshold: ç›¸ä¼¼åº¦é–¾å€¼
            
        Returns:
            List[Dict]: æ®µè½æœå°‹çµæœ
        """
        try:
            # ä½¿ç”¨åŸºé¡çš„ search_knowledgeï¼Œä½†å¼·åˆ¶è¿”å› section ç´šçµæœ
            _, cleaned_query = self._classify_and_clean_query(query)
            
            # âœ… å‚³é stage=1 (æ®µè½æœå°‹ä½¿ç”¨ç¬¬ä¸€éšæ®µæ¬Šé‡)
            results = super().search_knowledge(
                query=cleaned_query,
                limit=top_k,
                use_vector=True,
                threshold=threshold,
                stage=1  # ç¬¬ä¸€éšæ®µï¼šæ®µè½æœå°‹
            )
            
            # æ ¼å¼åŒ–ç‚ºçµ±ä¸€çš„çµæœæ ¼å¼
            formatted_results = []
            for result in results:
                formatted_results.append({
                    'title': result.get('metadata', {}).get('document_title', 'æœªçŸ¥æ¨™é¡Œ'),
                    'content': result.get('content', ''),
                    'source_id': result.get('metadata', {}).get('document_id', 'N/A'),
                    'similarity': result.get('score', 0.0),
                    'metadata': result.get('metadata', {})
                })
            
            return formatted_results
        
        except Exception as e:
            logger.error(f"âŒ æ®µè½æœå°‹å¤±æ•—: {str(e)}", exc_info=True)
            return []
    
    def full_document_search(self, query: str, top_k: int = 3, threshold: float = 0.5) -> list:
        """
        å…¨æ–‡å‘é‡æœå°‹ï¼ˆç”¨æ–¼æ™ºèƒ½è·¯ç”±å™¨ - æ¨¡å¼ A & æ¨¡å¼ B éšæ®µ 2ï¼‰
        
        Args:
            query: æœå°‹æŸ¥è©¢
            top_k: è¿”å›å‰ K å€‹æ–‡æª”
            threshold: ç›¸ä¼¼åº¦é–¾å€¼
            
        Returns:
            List[Dict]: å…¨æ–‡æ–‡æª”æœå°‹çµæœ
        """
        try:
            # å¼·åˆ¶ä½¿ç”¨æ–‡æª”ç´šæœå°‹
            _, cleaned_query = self._classify_and_clean_query(query)
            
            # âœ… å‚³é stage=2 (å…¨æ–‡æœå°‹ä½¿ç”¨ç¬¬äºŒéšæ®µæ¬Šé‡)
            # åŸ·è¡Œå‘é‡æœå°‹
            section_results = super().search_knowledge(
                query=cleaned_query,
                limit=top_k * 3,  # å¤šå–ä¸€äº›çµæœä»¥ä¾¿çµ„è£æ–‡æª”
                use_vector=True,
                threshold=threshold,
                stage=2  # ç¬¬äºŒéšæ®µï¼šå…¨æ–‡æœå°‹
            )
            
            # æ“´å±•ç‚ºå®Œæ•´æ–‡æª”
            full_documents = self._expand_to_full_document(section_results)
            
            # é™åˆ¶è¿”å›æ•¸é‡
            full_documents = full_documents[:top_k]
            
            # æ ¼å¼åŒ–ç‚ºçµ±ä¸€çš„çµæœæ ¼å¼
            formatted_results = []
            for doc in full_documents:
                formatted_results.append({
                    'title': doc.get('title', 'æœªçŸ¥æ¨™é¡Œ'),
                    'content': doc.get('content', ''),
                    'source_id': doc.get('metadata', {}).get('document_id', 'N/A'),
                    'similarity': doc.get('score', 0.0),
                    'metadata': doc.get('metadata', {})
                })
            
            return formatted_results
        
        except Exception as e:
            logger.error(f"âŒ å…¨æ–‡æœå°‹å¤±æ•—: {str(e)}", exc_info=True)
            return []


