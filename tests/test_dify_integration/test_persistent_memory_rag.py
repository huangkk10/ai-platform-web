#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ°¸ä¹…è¨˜æ†¶å‘é‡åŒ– RAG æ¸¬è©¦è…³æœ¬
åŸºæ–¼ test_vector_rag.pyï¼Œå¢åŠ æŒçºŒå°è©±å’Œè¨˜æ†¶åŠŸèƒ½
è®“ AI èƒ½å¤ è¨˜ä½ä¹‹å‰çš„å°è©±å…§å®¹å’Œå­¸ç¿’åˆ°çš„çŸ¥è­˜
"""

import requests
import json
import sqlite3
import time
import os
import uuid
from datetime import datetime
from typing import List, Dict, Any, Tuple, Optional
import hashlib

# å˜—è©¦å°å…¥å‘é‡åŒ–ç›¸é—œå¥—ä»¶
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    print("âš ï¸ sentence-transformers æœªå®‰è£ï¼Œå°‡ä½¿ç”¨ç°¡åŒ–ç‰ˆå‘é‡åŒ–")

try:
    import chromadb
    CHROMADB_AVAILABLE = True
except ImportError:
    CHROMADB_AVAILABLE = False
    print("âš ï¸ chromadb æœªå®‰è£ï¼Œå°‡ä½¿ç”¨å…§å»ºå‘é‡æœå°‹")

# Dify API é…ç½®
DIFY_CONFIG = {
    'api_url': 'http://10.10.172.37/v1/chat-messages',
    'api_key': 'app-R5nTTZw6jSQX75sDvyUMxLgo',  # æ‡‰ç”¨ API Token
    'dataset_api_key': 'dataset-JLa32OwILQHkgPqYStTCW4sC',  # çŸ¥è­˜åº« API Token
    'base_url': 'http://10.10.172.37'
}

# å‘é‡åŒ–é…ç½®
VECTOR_CONFIG = {
    'model_name': 'all-MiniLM-L6-v2',
    'vector_dim': 384,
    'similarity_threshold': 0.7,
    'max_results': 5
}

# è¨˜æ†¶é…ç½®
MEMORY_CONFIG = {
    'memory_file': 'tests/test_dify_integration/memory_data.json',
    'conversation_file': 'tests/test_dify_integration/conversation_history.json',
    'max_conversation_length': 20,  # æœ€å¤§å°è©±è¼ªæ•¸
    'max_memory_items': 100,  # æœ€å¤§è¨˜æ†¶é …ç›®æ•¸
    'memory_decay_days': 30  # è¨˜æ†¶è¡°æ¸›å¤©æ•¸
}

class PersistentMemoryStore:
    """æŒä¹…åŒ–è¨˜æ†¶å­˜å„²ç³»çµ±"""
    
    def __init__(self):
        self.memory_file = MEMORY_CONFIG['memory_file']
        self.conversation_file = MEMORY_CONFIG['conversation_file']
        self.memory_data = self.load_memory()
        self.conversation_history = self.load_conversation_history()
    
    def load_memory(self) -> Dict:
        """è¼‰å…¥è¨˜æ†¶è³‡æ–™"""
        try:
            if os.path.exists(self.memory_file):
                with open(self.memory_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    print(f"ğŸ“š è¼‰å…¥ {len(data.get('knowledge_base', []))} ç­†è¨˜æ†¶è³‡æ–™")
                    return data
        except Exception as e:
            print(f"âš ï¸ è¼‰å…¥è¨˜æ†¶è³‡æ–™å¤±æ•—: {e}")
        
        return {
            'knowledge_base': [],  # å­¸ç¿’åˆ°çš„çŸ¥è­˜
            'user_preferences': {},  # ç”¨æˆ¶åå¥½
            'learned_facts': [],  # å­¸ç¿’åˆ°çš„äº‹å¯¦
            'created_at': datetime.now().isoformat(),
            'last_updated': datetime.now().isoformat()
        }
    
    def save_memory(self):
        """ä¿å­˜è¨˜æ†¶è³‡æ–™"""
        try:
            self.memory_data['last_updated'] = datetime.now().isoformat()
            os.makedirs(os.path.dirname(self.memory_file), exist_ok=True)
            
            with open(self.memory_file, 'w', encoding='utf-8') as f:
                json.dump(self.memory_data, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ è¨˜æ†¶è³‡æ–™å·²ä¿å­˜åˆ° {self.memory_file}")
        except Exception as e:
            print(f"âŒ ä¿å­˜è¨˜æ†¶è³‡æ–™å¤±æ•—: {e}")
    
    def load_conversation_history(self) -> List[Dict]:
        """è¼‰å…¥å°è©±æ­·å²"""
        try:
            if os.path.exists(self.conversation_file):
                with open(self.conversation_file, 'r', encoding='utf-8') as f:
                    history = json.load(f)
                    print(f"ğŸ“ è¼‰å…¥ {len(history)} ç­†å°è©±è¨˜éŒ„")
                    return history
        except Exception as e:
            print(f"âš ï¸ è¼‰å…¥å°è©±æ­·å²å¤±æ•—: {e}")
        
        return []
    
    def save_conversation_history(self):
        """ä¿å­˜å°è©±æ­·å²"""
        try:
            # é™åˆ¶å°è©±æ­·å²é•·åº¦
            if len(self.conversation_history) > MEMORY_CONFIG['max_conversation_length']:
                self.conversation_history = self.conversation_history[-MEMORY_CONFIG['max_conversation_length']:]
            
            os.makedirs(os.path.dirname(self.conversation_file), exist_ok=True)
            
            with open(self.conversation_file, 'w', encoding='utf-8') as f:
                json.dump(self.conversation_history, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ å°è©±æ­·å²å·²ä¿å­˜åˆ° {self.conversation_file}")
        except Exception as e:
            print(f"âŒ ä¿å­˜å°è©±æ­·å²å¤±æ•—: {e}")
    
    def add_to_knowledge_base(self, question: str, answer: str, context: str = "", source: str = "vector_search"):
        """æ·»åŠ åˆ°çŸ¥è­˜åº«"""
        knowledge_item = {
            'id': str(uuid.uuid4()),
            'question': question,
            'answer': answer,
            'context': context,
            'source': source,
            'timestamp': datetime.now().isoformat(),
            'access_count': 0
        }
        
        self.memory_data['knowledge_base'].append(knowledge_item)
        
        # é™åˆ¶çŸ¥è­˜åº«å¤§å°
        if len(self.memory_data['knowledge_base']) > MEMORY_CONFIG['max_memory_items']:
            # ç§»é™¤æœ€èˆŠçš„é …ç›®
            self.memory_data['knowledge_base'] = self.memory_data['knowledge_base'][-MEMORY_CONFIG['max_memory_items']:]
        
        print(f"ğŸ§  æ–°çŸ¥è­˜å·²æ·»åŠ åˆ°è¨˜æ†¶åº«: {question[:50]}...")
    
    def add_conversation(self, conversation_id: str, question: str, answer: str, search_results: List = None):
        """æ·»åŠ å°è©±è¨˜éŒ„"""
        conversation_item = {
            'conversation_id': conversation_id,
            'question': question,
            'answer': answer,
            'search_results': search_results or [],
            'timestamp': datetime.now().isoformat()
        }
        
        self.conversation_history.append(conversation_item)
        print(f"ğŸ“ å°è©±è¨˜éŒ„å·²æ·»åŠ : {question[:30]}...")
    
    def search_knowledge_base(self, query: str, limit: int = 3) -> List[Dict]:
        """å¾çŸ¥è­˜åº«æœå°‹ç›¸é—œçŸ¥è­˜"""
        relevant_knowledge = []
        query_lower = query.lower()
        
        for item in self.memory_data['knowledge_base']:
            # ç°¡å–®çš„é—œéµå­—åŒ¹é…
            question_lower = item['question'].lower()
            answer_lower = item['answer'].lower()
            
            if (any(word in question_lower for word in query_lower.split()) or
                any(word in answer_lower for word in query_lower.split())):
                
                item['access_count'] += 1  # å¢åŠ è¨ªå•æ¬¡æ•¸
                relevant_knowledge.append(item)
        
        # æŒ‰è¨ªå•æ¬¡æ•¸å’Œæ™‚é–“æ’åº
        relevant_knowledge.sort(key=lambda x: (x['access_count'], x['timestamp']), reverse=True)
        
        return relevant_knowledge[:limit]
    
    def get_recent_conversations(self, limit: int = 5) -> List[Dict]:
        """ç²å–æœ€è¿‘çš„å°è©±"""
        return self.conversation_history[-limit:] if self.conversation_history else []

class SimpleVectorStore:
    """ç°¡åŒ–ç‰ˆå‘é‡è³‡æ–™åº«"""
    
    def __init__(self):
        self.vectors = []
        self.metadata = []
        self.texts = []
    
    def add_vectors(self, vectors: List[List[float]], texts: List[str], metadata: List[Dict]):
        """æ·»åŠ å‘é‡è³‡æ–™"""
        self.vectors.extend(vectors)
        self.texts.extend(texts)
        self.metadata.extend(metadata)
    
    def search(self, query_vector: List[float], top_k: int = 5) -> List[Tuple[str, Dict, float]]:
        """æœå°‹ç›¸ä¼¼å‘é‡"""
        if not self.vectors:
            return []
        
        similarities = []
        
        for i, vector in enumerate(self.vectors):
            # ç°¡åŒ–ç‰ˆé¤˜å¼¦ç›¸ä¼¼åº¦è¨ˆç®—
            dot_product = sum(a * b for a, b in zip(query_vector, vector))
            norm1 = sum(a * a for a in query_vector) ** 0.5
            norm2 = sum(b * b for b in vector) ** 0.5
            
            if norm1 > 0 and norm2 > 0:
                similarity = dot_product / (norm1 * norm2)
                similarities.append((i, similarity))
        
        similarities.sort(key=lambda x: x[1], reverse=True)
        results = []
        
        for i, (idx, score) in enumerate(similarities[:top_k]):
            if score >= VECTOR_CONFIG['similarity_threshold']:
                results.append((self.texts[idx], self.metadata[idx], score))
        
        return results

class PersistentMemoryRAGTester:
    """å…·æœ‰æ°¸ä¹…è¨˜æ†¶åŠŸèƒ½çš„ RAG æ¸¬è©¦å™¨"""
    
    def __init__(self):
        self.embedding_model = None
        self.vector_store = None
        self.memory_store = PersistentMemoryStore()
        self.current_conversation_id = None
        self.dataset_id = None  # Dify çŸ¥è­˜åº« ID
        self.knowledge_base_enabled = False  # çŸ¥è­˜åº«æ˜¯å¦å•Ÿç”¨
        self.setup_embedding_model()
        self.setup_vector_store()
    
    def setup_embedding_model(self):
        """è¨­ç½®åµŒå…¥æ¨¡å‹"""
        if SENTENCE_TRANSFORMERS_AVAILABLE:
            try:
                print(f"ğŸ”§ è¼‰å…¥åµŒå…¥æ¨¡å‹: {VECTOR_CONFIG['model_name']}")
                self.embedding_model = SentenceTransformer(VECTOR_CONFIG['model_name'])
                print("âœ… åµŒå…¥æ¨¡å‹è¼‰å…¥æˆåŠŸ")
            except Exception as e:
                print(f"âŒ åµŒå…¥æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
                self.embedding_model = None
        else:
            print("âš ï¸ ä½¿ç”¨ç°¡åŒ–ç‰ˆå‘é‡åŒ–")
    
    def setup_vector_store(self):
        """è¨­ç½®å‘é‡è³‡æ–™åº«"""
        if CHROMADB_AVAILABLE:
            try:
                print("ğŸ”§ è¨­ç½® ChromaDB å‘é‡è³‡æ–™åº«")
                self.chroma_client = chromadb.Client()
                self.collection = self.chroma_client.create_collection(
                    name="company_employees_persistent",
                    metadata={"hnsw:space": "cosine"}
                )
                print("âœ… ChromaDB å‘é‡è³‡æ–™åº«è¨­ç½®æˆåŠŸ")
            except Exception as e:
                print(f"âŒ ChromaDB è¨­ç½®å¤±æ•—: {e}")
                self.vector_store = SimpleVectorStore()
        else:
            print("ğŸ”§ ä½¿ç”¨ç°¡åŒ–ç‰ˆå‘é‡è³‡æ–™åº«")
            self.vector_store = SimpleVectorStore()
    
    def simple_embedding(self, text: str) -> List[float]:
        """ç°¡åŒ–ç‰ˆæ–‡å­—åµŒå…¥"""
        vector = [0.0] * 100
        vector[0] = len(text) / 100.0
        
        char_count = {}
        for char in text.lower():
            char_count[char] = char_count.get(char, 0) + 1
        
        common_chars = 'abcdefghijklmnopqrstuvwxyz0123456789ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å'
        for i, char in enumerate(common_chars[:99]):
            if char in char_count:
                vector[i + 1] = char_count[char] / len(text)
        
        return vector
    
    def get_embedding(self, text: str) -> List[float]:
        """ç²å–æ–‡å­—åµŒå…¥å‘é‡"""
        if self.embedding_model:
            try:
                embedding = self.embedding_model.encode(text)
                return embedding.tolist()
            except Exception as e:
                print(f"âš ï¸ åµŒå…¥ç”Ÿæˆå¤±æ•—ï¼Œä½¿ç”¨ç°¡åŒ–ç‰ˆ: {e}")
                return self.simple_embedding(text)
        else:
            return self.simple_embedding(text)
    
    def load_and_vectorize_data(self):
        """è¼‰å…¥ä¸¦å‘é‡åŒ–è³‡æ–™åº«è³‡æ–™"""
        print("\n" + "="*60)
        print("ğŸ“Š è¼‰å…¥ä¸¦å‘é‡åŒ–å“¡å·¥è³‡æ–™")
        print("="*60)
        
        try:
            conn = sqlite3.connect('tests/test_dify_integration/company.db')
            cursor = conn.cursor()
            cursor.execute("SELECT * FROM employees")
            columns = [description[0] for description in cursor.description]
            rows = cursor.fetchall()
            conn.close()
            
            print(f"ğŸ“‹ è¼‰å…¥ {len(rows)} ç­†å“¡å·¥è³‡æ–™")
            
            texts = []
            metadata = []
            vectors = []
            
            for row in rows:
                employee_data = dict(zip(columns, row))
                
                text = f"""
                å“¡å·¥å§“å: {employee_data['name']}
                éƒ¨é–€: {employee_data['department']}
                è·ä½: {employee_data['position']}
                è–ªè³‡: {employee_data['salary']}
                å…¥è·æ—¥æœŸ: {employee_data['hire_date']}
                é›»å­éƒµä»¶: {employee_data['email']}
                """.strip()
                
                vector = self.get_embedding(text)
                
                texts.append(text)
                metadata.append(employee_data)
                vectors.append(vector)
                
                print(f"âœ… å·²å‘é‡åŒ–: {employee_data['name']} ({employee_data['department']})")
            
            # å­˜å„²åˆ°å‘é‡è³‡æ–™åº«
            if CHROMADB_AVAILABLE and hasattr(self, 'collection'):
                try:
                    ids = [f"emp_{i}" for i in range(len(texts))]
                    self.collection.add(
                        embeddings=vectors,
                        documents=texts,
                        metadatas=metadata,
                        ids=ids
                    )
                    print("âœ… è³‡æ–™å·²å­˜å„²åˆ° ChromaDB")
                except Exception as e:
                    print(f"âš ï¸ ChromaDB å­˜å„²å¤±æ•—ï¼Œä½¿ç”¨ç°¡åŒ–ç‰ˆ: {e}")
                    self.vector_store = SimpleVectorStore()
                    self.vector_store.add_vectors(vectors, texts, metadata)
            else:
                self.vector_store.add_vectors(vectors, texts, metadata)
                print("âœ… è³‡æ–™å·²å­˜å„²åˆ°ç°¡åŒ–ç‰ˆå‘é‡è³‡æ–™åº«")
            
            print(f"ğŸ¯ å‘é‡åŒ–å®Œæˆï¼Œç¸½å…±è™•ç† {len(texts)} ç­†è³‡æ–™")
            return True
            
        except Exception as e:
            print(f"âŒ è³‡æ–™è¼‰å…¥å¤±æ•—: {e}")
            return False
    
    def create_dify_dataset(self) -> bool:
        """å»ºç«‹ Dify çŸ¥è­˜åº«è³‡æ–™é›†"""
        print("\nğŸ“š å»ºç«‹ Dify çŸ¥è­˜åº«...")
        
        headers = {
            'Authorization': f'Bearer {DIFY_CONFIG["dataset_api_key"]}',
            'Content-Type': 'application/json'
        }
        
        dataset_data = {
            'name': f'å“¡å·¥è³‡æ–™åº«_æ°¸ä¹…è¨˜æ†¶_{int(time.time())}',
            'description': 'å…·æœ‰æ°¸ä¹…è¨˜æ†¶åŠŸèƒ½çš„å“¡å·¥è³‡æ–™åº«ï¼ŒåŒ…å«å‘é‡æœå°‹å’Œå°è©±æ­·å²'
        }
        
        try:
            response = requests.post(
                f'{DIFY_CONFIG["base_url"]}/v1/datasets',
                headers=headers,
                json=dataset_data,
                timeout=30
            )
            
            print(f"ğŸ“¥ å»ºç«‹è³‡æ–™é›†å›æ‡‰: HTTP {response.status_code}")
            
            if response.status_code == 200:
                result = response.json()
                self.dataset_id = result.get('id')
                print(f"âœ… Dify çŸ¥è­˜åº«å»ºç«‹æˆåŠŸï¼ŒID: {self.dataset_id}")
                return True
            else:
                print(f"âŒ å»ºç«‹ Dify çŸ¥è­˜åº«å¤±æ•—: {response.text}")
                return False
                
        except Exception as e:
            print(f"âŒ å»ºç«‹ Dify çŸ¥è­˜åº«ç•°å¸¸: {e}")
            return False
    
    def upload_to_dify_knowledge_base(self) -> bool:
        """ä¸Šå‚³å“¡å·¥è³‡æ–™åˆ° Dify çŸ¥è­˜åº«"""
        if not self.dataset_id:
            print("âŒ æ²’æœ‰è³‡æ–™é›† IDï¼Œç„¡æ³•ä¸Šå‚³")
            return False
        
        print(f"\nğŸ“„ ä¸Šå‚³å“¡å·¥è³‡æ–™åˆ° Dify çŸ¥è­˜åº«...")
        
        try:
            # ç”Ÿæˆå®Œæ•´çš„å“¡å·¥è³‡æ–™æª”æ¡ˆ
            employees_content = self.generate_employees_content()
            
            # å»ºç«‹è‡¨æ™‚æª”æ¡ˆ
            temp_filename = f'tests/test_dify_integration/temp_employees_{int(time.time())}.txt'
            
            with open(temp_filename, 'w', encoding='utf-8') as f:
                f.write(employees_content)
            
            print(f"ğŸ“ å“¡å·¥è³‡æ–™æª”æ¡ˆå·²ç”Ÿæˆ: {temp_filename}")
            
            # ä½¿ç”¨æ­£ç¢ºçš„æª”æ¡ˆä¸Šå‚³ API
            headers = {
                'Authorization': f'Bearer {DIFY_CONFIG["dataset_api_key"]}'
                # æ³¨æ„ï¼šæª”æ¡ˆä¸Šå‚³ä¸éœ€è¦ Content-Type: application/json
            }
            
            # ä¸Šå‚³æª”æ¡ˆ
            with open(temp_filename, 'rb') as f:
                files = {
                    'file': (f'employees_data_{int(time.time())}.txt', f, 'text/plain')
                }
                
                data = {
                    'indexing_technique': 'high_quality',
                    'process_rule': json.dumps({
                        'mode': 'automatic',
                        'rules': {
                            'pre_processing_rules': [
                                {'id': 'remove_extra_spaces', 'enabled': True},
                                {'id': 'remove_urls_emails', 'enabled': False}
                            ],
                            'segmentation': {
                                'separator': '\n',
                                'max_tokens': 1000
                            }
                        }
                    }),
                    'duplicate_removal': 'true'
                }
                
                print(f"ğŸš€ ä¸Šå‚³æª”æ¡ˆåˆ°è³‡æ–™é›† {self.dataset_id}...")
                
                response = requests.post(
                    f'{DIFY_CONFIG["base_url"]}/v1/datasets/{self.dataset_id}/document/create_by_file',
                    headers=headers,
                    files=files,
                    data=data,
                    timeout=60
                )
                
                print(f"ğŸ“¥ æª”æ¡ˆä¸Šå‚³å›æ‡‰: HTTP {response.status_code}")
                
                if response.status_code == 200:
                    result = response.json()
                    document_id = result.get('document', {}).get('id')
                    print(f"âœ… æª”æ¡ˆä¸Šå‚³æˆåŠŸï¼Œæ–‡ä»¶ ID: {document_id}")
                    
                    # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
                    try:
                        os.remove(temp_filename)
                        print(f"ğŸ—‘ï¸ è‡¨æ™‚æª”æ¡ˆå·²æ¸…ç†: {temp_filename}")
                    except:
                        pass
                    
                    # ç­‰å¾…æ–‡ä»¶è™•ç†å®Œæˆ
                    print("â³ ç­‰å¾…æ–‡ä»¶ç´¢å¼•å»ºç«‹...")
                    self.wait_for_document_processing(document_id)
                    
                    self.knowledge_base_enabled = True
                    print("ğŸ‰ Dify çŸ¥è­˜åº«ä¸Šå‚³å®Œæˆä¸¦å·²å•Ÿç”¨")
                    return True
                else:
                    print(f"âŒ æª”æ¡ˆä¸Šå‚³å¤±æ•—: {response.text}")
                    return False
                employee_data = dict(zip(columns, row))
                
                # æ§‹å»ºå“¡å·¥æ–‡ä»¶å…§å®¹
                document_content = f"""å“¡å·¥è³‡æ–™ - {employee_data['name']}

åŸºæœ¬è³‡è¨Šï¼š
- å§“åï¼š{employee_data['name']}
- å“¡å·¥ç·¨è™Ÿï¼š{employee_data['id']}
- éƒ¨é–€ï¼š{employee_data['department']}
- è·ä½ï¼š{employee_data['position']}
- è–ªè³‡ï¼š{employee_data['salary']:,} å…ƒï¼ˆæœˆè–ªï¼‰
- å…¥è·æ—¥æœŸï¼š{employee_data['hire_date']}
- é›»å­éƒµä»¶ï¼š{employee_data['email']}

éƒ¨é–€æ­¸å±¬ï¼š{employee_data['department']}
è·ç´šå±¤æ¬¡ï¼š{employee_data['position']}
è–ªè³‡æ°´æº–ï¼š{employee_data['salary']:,} å…ƒ

è¯ç¹«è³‡è¨Šï¼š
- Email: {employee_data['email']}
- å…¥è·æ™‚é–“: {employee_data['hire_date']}
"""
                
                # æº–å‚™æ–‡ä»¶è³‡æ–™
                file_data = {
                    'name': f"{employee_data['name']}_profile.txt",
                    'text': document_content,
                    'indexing_technique': 'high_quality',  # æ·»åŠ å¿…éœ€åƒæ•¸
                    'process_rule': {
                        'mode': 'automatic'
                    }
                }
                
                try:
                    # ä¸Šå‚³æ–‡ä»¶åˆ°è³‡æ–™é›†
                    response = requests.post(
                        f'{DIFY_CONFIG["base_url"]}/v1/datasets/{self.dataset_id}/document/create_by_text',
                        headers=headers,
                        json=file_data,
                        timeout=30
                    )
                    
                    if response.status_code == 200:
                        success_count += 1
                        print(f"âœ… ä¸Šå‚³å“¡å·¥ {employee_data['name']} æˆåŠŸ ({success_count}/{len(rows)})")
                    else:
                        print(f"âŒ ä¸Šå‚³å“¡å·¥ {employee_data['name']} å¤±æ•—: HTTP {response.status_code}")
                        print(f"   å›æ‡‰: {response.text}")
                    
                    # é¿å…è«‹æ±‚éæ–¼é »ç¹
                    time.sleep(0.5)
                    
                except Exception as e:
                    print(f"âŒ ä¸Šå‚³å“¡å·¥ {employee_data['name']} ç•°å¸¸: {e}")
            
            print(f"\nğŸ“Š ä¸Šå‚³çµæœ: {success_count}/{len(rows)} ä»½æ–‡ä»¶æˆåŠŸ")
            
            if success_count > 0:
                self.knowledge_base_enabled = True
                print("âœ… Dify çŸ¥è­˜åº«å·²å•Ÿç”¨")
                return True
            else:
                return False
                
        except Exception as e:
            print(f"âŒ ä¸Šå‚³åˆ° Dify çŸ¥è­˜åº«å¤±æ•—: {e}")
            return False
    
    def generate_employees_content(self) -> str:
        """ç”Ÿæˆå“¡å·¥è³‡æ–™å…§å®¹"""
        try:
            conn = sqlite3.connect('tests/test_dify_integration/company.db')
            cursor = conn.cursor()
            cursor.execute("SELECT * FROM employees")
            columns = [description[0] for description in cursor.description]
            rows = cursor.fetchall()
            conn.close()
            
            content = "# å…¬å¸å“¡å·¥è³‡æ–™åº«\n\n"
            content += "æœ¬æ–‡ä»¶åŒ…å«å…¬å¸æ‰€æœ‰å“¡å·¥çš„è©³ç´°è³‡è¨Šï¼ŒåŒ…æ‹¬å§“åã€éƒ¨é–€ã€è·ä½ã€è–ªè³‡ç­‰ã€‚\n\n"
            
            # æŒ‰éƒ¨é–€åˆ†çµ„
            departments = {}
            for row in rows:
                employee_data = dict(zip(columns, row))
                dept = employee_data['department']
                if dept not in departments:
                    departments[dept] = []
                departments[dept].append(employee_data)
            
            for dept_name, employees in departments.items():
                content += f"## {dept_name}\n\n"
                
                for emp in employees:
                    content += f"### {emp['name']} (å“¡å·¥ç·¨è™Ÿ: {emp['id']})\n"
                    content += f"- **å§“å**: {emp['name']}\n"
                    content += f"- **éƒ¨é–€**: {emp['department']}\n"
                    content += f"- **è·ä½**: {emp['position']}\n"
                    content += f"- **è–ªè³‡**: {emp['salary']:,} å…ƒï¼ˆæœˆè–ªï¼‰\n"
                    content += f"- **å…¥è·æ—¥æœŸ**: {emp['hire_date']}\n"
                    content += f"- **é›»å­éƒµä»¶**: {emp['email']}\n\n"
            
            # æ·»åŠ çµ±è¨ˆè³‡è¨Š
            content += "## çµ±è¨ˆè³‡è¨Š\n\n"
            content += f"- **ç¸½å“¡å·¥æ•¸**: {len(rows)} äºº\n"
            
            for dept_name, employees in departments.items():
                avg_salary = sum(emp['salary'] for emp in employees) / len(employees)
                content += f"- **{dept_name}**: {len(employees)} äººï¼Œå¹³å‡è–ªè³‡ {avg_salary:,.0f} å…ƒ\n"
            
            return content
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå“¡å·¥å…§å®¹å¤±æ•—: {e}")
            return "å“¡å·¥è³‡æ–™è¼‰å…¥å¤±æ•—"
    
    def wait_for_document_processing(self, document_id: str, max_wait: int = 60):
        """ç­‰å¾…æ–‡ä»¶è™•ç†å®Œæˆ"""
        if not document_id:
            return
        
        headers = {
            'Authorization': f'Bearer {DIFY_CONFIG["dataset_api_key"]}',
            'Content-Type': 'application/json'
        }
        
        for i in range(max_wait):
            try:
                response = requests.get(
                    f'{DIFY_CONFIG["base_url"]}/v1/datasets/{self.dataset_id}/documents/{document_id}',
                    headers=headers,
                    timeout=10
                )
                
                if response.status_code == 200:
                    result = response.json()
                    status = result.get('indexing_status', 'unknown')
                    
                    if status == 'completed':
                        print(f"âœ… æ–‡ä»¶è™•ç†å®Œæˆ ({i+1}s)")
                        return
                    elif status == 'error':
                        print(f"âŒ æ–‡ä»¶è™•ç†å¤±æ•—: {result.get('error', 'Unknown error')}")
                        return
                    
                    if i % 10 == 0:  # æ¯10ç§’é¡¯ç¤ºä¸€æ¬¡ç‹€æ…‹
                        print(f"â³ æ–‡ä»¶è™•ç†ä¸­... ç‹€æ…‹: {status} ({i}s)")
                
                time.sleep(1)
                
            except Exception as e:
                if i % 10 == 0:
                    print(f"âš ï¸ æª¢æŸ¥è™•ç†ç‹€æ…‹å¤±æ•—: {e}")
                time.sleep(1)
        
        print(f"âš ï¸ ç­‰å¾…æ–‡ä»¶è™•ç†è¶…æ™‚ ({max_wait}s)ï¼Œä½†å¯èƒ½å·²å®Œæˆ")
    
    def query_dify_knowledge_base(self, question: str) -> dict:
        """ç›´æ¥æŸ¥è©¢é…ç½®äº†çŸ¥è­˜åº«çš„ Dify æ‡‰ç”¨"""
        if not self.knowledge_base_enabled:
            print("âš ï¸ Dify çŸ¥è­˜åº«æœªå•Ÿç”¨ï¼Œä½¿ç”¨æœ¬åœ°è¨˜æ†¶")
            return None
        
        print(f"ğŸ” æŸ¥è©¢ Dify çŸ¥è­˜åº«: {question}")
        
        headers = {
            'Authorization': f'Bearer {DIFY_CONFIG["api_key"]}',  # ä½¿ç”¨æ‡‰ç”¨ Token
            'Content-Type': 'application/json'
        }
        
        payload = {
            'inputs': {},
            'query': question,
            'response_mode': 'blocking',
            'user': 'persistent_memory_kb_test'
        }
        
        try:
            response = requests.post(
                DIFY_CONFIG['api_url'],
                headers=headers,
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                answer = result.get('answer', '')
                metadata = result.get('metadata', {})
                
                # æª¢æŸ¥æ˜¯å¦ä½¿ç”¨äº†çŸ¥è­˜åº«
                retrieval_sources = metadata.get('retrieval_sources', [])
                
                return {
                    'success': True,
                    'answer': answer,
                    'uses_knowledge_base': len(retrieval_sources) > 0,
                    'sources_count': len(retrieval_sources),
                    'sources': retrieval_sources
                }
            else:
                print(f"âŒ Dify çŸ¥è­˜åº«æŸ¥è©¢å¤±æ•—: HTTP {response.status_code}")
                return None
                
        except Exception as e:
            print(f"âŒ Dify çŸ¥è­˜åº«æŸ¥è©¢ç•°å¸¸: {e}")
            return None
    
    def vector_search(self, query: str, top_k: int = 3) -> List[Tuple[str, Dict, float]]:
        """å‘é‡æœå°‹"""
        try:
            query_vector = self.get_embedding(query)
            
            if CHROMADB_AVAILABLE and hasattr(self, 'collection'):
                results = self.collection.query(
                    query_embeddings=[query_vector],
                    n_results=top_k
                )
                
                search_results = []
                for i in range(len(results['documents'][0])):
                    text = results['documents'][0][i]
                    metadata = results['metadatas'][0][i]
                    distance = results['distances'][0][i]
                    similarity = 1 - distance
                    search_results.append((text, metadata, similarity))
                
                return search_results
            else:
                return self.vector_store.search(query_vector, top_k)
        
        except Exception as e:
            print(f"âŒ å‘é‡æœå°‹å¤±æ•—: {e}")
            return []
    
    def build_context_with_memory(self, question: str, search_results: List[Tuple[str, Dict, float]]) -> str:
        """æ§‹å»ºåŒ…å«è¨˜æ†¶çš„ä¸Šä¸‹æ–‡"""
        context_parts = []
        
        # 1. åŠ å…¥ç›¸é—œçš„å‘é‡æœå°‹çµæœ
        if search_results:
            context_parts.append("=== å“¡å·¥è³‡æ–™åº«æœå°‹çµæœ ===")
            for text, metadata, score in search_results:
                context_parts.append(f"ç›¸é—œå“¡å·¥: {metadata['name']} ({metadata['department']})")
                context_parts.append(text)
                context_parts.append("")
        
        # 2. åŠ å…¥ç›¸é—œçš„æ­·å²çŸ¥è­˜
        relevant_knowledge = self.memory_store.search_knowledge_base(question, limit=2)
        if relevant_knowledge:
            context_parts.append("=== ç›¸é—œæ­·å²çŸ¥è­˜ ===")
            for knowledge in relevant_knowledge:
                context_parts.append(f"å•é¡Œ: {knowledge['question']}")
                context_parts.append(f"å›ç­”: {knowledge['answer']}")
                context_parts.append("")
        
        # 3. åŠ å…¥æœ€è¿‘çš„å°è©±æ­·å²
        recent_conversations = self.memory_store.get_recent_conversations(limit=3)
        if recent_conversations:
            context_parts.append("=== æœ€è¿‘å°è©±æ­·å² ===")
            for conv in recent_conversations:
                context_parts.append(f"å•: {conv['question']}")
                context_parts.append(f"ç­”: {conv['answer'][:100]}...")
                context_parts.append("")
        
        return "\n".join(context_parts)
    
    def call_dify_with_persistent_memory(self, question: str, context_data: str = "") -> dict:
        """èª¿ç”¨ Dify API ä¸¦ç¶­æŒæŒçºŒå°è©±ï¼Œå„ªå…ˆä½¿ç”¨çŸ¥è­˜åº«"""
        
        # å„ªå…ˆå˜—è©¦ä½¿ç”¨ Dify çŸ¥è­˜åº«
        if self.knowledge_base_enabled:
            kb_result = self.query_dify_knowledge_base(question)
            if kb_result and kb_result['success'] and kb_result['uses_knowledge_base']:
                print(f"âœ… ä½¿ç”¨ Dify çŸ¥è­˜åº«å›ç­” (æ‰¾åˆ° {kb_result['sources_count']} å€‹ç›¸é—œè³‡æº)")
                return {
                    'success': True,
                    'answer': kb_result['answer'],
                    'response_time': 1.0,  # ä¼°è¨ˆå€¼
                    'source': 'dify_knowledge_base',
                    'uses_knowledge_base': True,
                    'sources_count': kb_result['sources_count']
                }
        
        # å¦‚æœçŸ¥è­˜åº«æ²’æœ‰æ‰¾åˆ°ç­”æ¡ˆï¼Œä½¿ç”¨æœ¬åœ°å‘é‡æœå°‹ + ä¸Šä¸‹æ–‡
        print("ğŸ”„ ä½¿ç”¨æœ¬åœ°å‘é‡æœå°‹ + ä¸Šä¸‹æ–‡æ¨¡å¼")
        
        headers = {
            'Authorization': f'Bearer {DIFY_CONFIG["api_key"]}',
            'Content-Type': 'application/json'
        }
        
        # å¦‚æœæ²’æœ‰ç•¶å‰å°è©±IDï¼Œå‰µå»ºä¸€å€‹æ–°çš„
        if not self.current_conversation_id:
            self.current_conversation_id = str(uuid.uuid4())
            print(f"ğŸ†• é–‹å§‹æ–°å°è©±: {self.current_conversation_id[:8]}...")
        
        # ä½¿ç”¨ä¿®å¾©éçš„ä¸Šä¸‹æ–‡åµŒå…¥æ–¹æ³•
        if context_data:
            enhanced_question = f"""åŸºæ–¼ä»¥ä¸‹è³‡æ–™å›ç­”å•é¡Œï¼š

{context_data}

å•é¡Œï¼š{question}

è«‹æ ¹æ“šä¸Šè¿°è³‡æ–™è©³ç´°å›ç­”ï¼Œå¦‚æœè³‡æ–™ä¸­æœ‰ç›¸é—œè³‡è¨Šï¼Œè«‹å…·é«”èªªæ˜ã€‚"""
        else:
            enhanced_question = question
        
        payload = {
            'inputs': {},
            'query': enhanced_question,
            'response_mode': 'blocking',
            'user': 'persistent_memory_test'
        }
        
        try:
            print(f"ğŸ¤– èª¿ç”¨ Dify AI (å°è©±: {self.current_conversation_id[:8]}): {question[:50]}...")
            start_time = time.time()
            
            response = requests.post(
                DIFY_CONFIG['api_url'],
                headers=headers,
                json=payload,
                timeout=60
            )
            
            elapsed = time.time() - start_time
            
            if response.status_code == 200:
                result = response.json()
                return {
                    'success': True,
                    'answer': result.get('answer', ''),
                    'response_time': elapsed,
                    'message_id': result.get('message_id', ''),
                    'conversation_id': result.get('conversation_id', self.current_conversation_id),
                    'source': 'local_context',
                    'uses_knowledge_base': False
                }
            else:
                return {
                    'success': False,
                    'error': f"HTTP {response.status_code}: {response.text}",
                    'response_time': elapsed,
                    'source': 'error'
                }
                
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'response_time': 0,
                'source': 'error'
            }
    
    def test_persistent_memory_queries(self):
        """æ¸¬è©¦æ°¸ä¹…è¨˜æ†¶æŸ¥è©¢"""
        print("\n" + "="*60)
        print("ğŸ§  æ°¸ä¹…è¨˜æ†¶ RAG æŸ¥è©¢æ¸¬è©¦")
        print("="*60)
        
        test_queries = [
            {
                'question': 'æŠ€è¡“éƒ¨æœ‰å“ªäº›å“¡å·¥ï¼Ÿè«‹è¨˜ä½é€™å€‹è³‡è¨Šã€‚',
                'description': 'å»ºç«‹åŸºç¤çŸ¥è­˜'
            },
            {
                'question': 'å‰›æ‰æˆ‘å•çš„æŠ€è¡“éƒ¨å“¡å·¥ï¼Œä»–å€‘çš„å¹³å‡è–ªè³‡æ˜¯å¤šå°‘ï¼Ÿ',
                'description': 'æ¸¬è©¦çŸ­æœŸè¨˜æ†¶'
            },
            {
                'question': 'æç¾è¯æ˜¯ä»€éº¼è·ä½ï¼Ÿè«‹è¨˜ä½å¥¹çš„è³‡è¨Šã€‚',
                'description': 'å­¸ç¿’ç‰¹å®šå“¡å·¥è³‡è¨Š'
            },
            {
                'question': 'ä½ é‚„è¨˜å¾—æˆ‘ä¹‹å‰å•éæŠ€è¡“éƒ¨çš„äº‹æƒ…å—ï¼Ÿ',
                'description': 'æ¸¬è©¦å°è©±è¨˜æ†¶'
            },
            {
                'question': 'æ ¹æ“šä½ å­¸åˆ°çš„è³‡è¨Šï¼Œèª°çš„è–ªè³‡æ¯”è¼ƒé«˜ï¼Ÿ',
                'description': 'æ¸¬è©¦çŸ¥è­˜æ•´åˆ'
            }
        ]
        
        results = []
        
        for i, query_info in enumerate(test_queries, 1):
            question = query_info['question']
            description = query_info['description']
            
            print(f"\nğŸ”¸ æ¸¬è©¦ {i}: {description}")
            print(f"â“ å•é¡Œ: {question}")
            print("-" * 50)
            
            # 1. å‘é‡æœå°‹
            search_start = time.time()
            search_results = self.vector_search(question, top_k=3)
            search_time = time.time() - search_start
            
            # 2. æ§‹å»ºåŒ…å«è¨˜æ†¶çš„ä¸Šä¸‹æ–‡
            context_data = self.build_context_with_memory(question, search_results)
            
            if search_results or context_data.strip():
                print(f"ğŸ¯ æœå°‹çµæœ: {len(search_results)} å€‹å‘é‡åŒ¹é… ({search_time:.2f}s)")
                print(f"ğŸ§  è¨˜æ†¶å…§å®¹: {len(self.memory_store.search_knowledge_base(question))} å€‹ç›¸é—œè¨˜æ†¶")
                
                # 3. èª¿ç”¨ AI
                ai_result = self.call_dify_with_persistent_memory(question, context_data)
                
                if ai_result['success']:
                    print(f"âœ… AI å›ç­” ({ai_result['response_time']:.1f}s):")
                    print(f"ğŸ“ {ai_result['answer']}")
                    
                    # 4. ä¿å­˜åˆ°è¨˜æ†¶ä¸­
                    source_info = ai_result.get('source', 'unknown')
                    if ai_result.get('uses_knowledge_base', False):
                        source_info += f"_kb_sources_{ai_result.get('sources_count', 0)}"
                    
                    self.memory_store.add_to_knowledge_base(
                        question=question,
                        answer=ai_result['answer'],
                        context=context_data[:500],  # é™åˆ¶ä¸Šä¸‹æ–‡é•·åº¦
                        source=source_info
                    )
                    
                    # 5. ä¿å­˜å°è©±è¨˜éŒ„
                    search_summary = [{"name": meta['name'], "dept": meta['department']} 
                                    for _, meta, _ in search_results]
                    self.memory_store.add_conversation(
                        conversation_id=self.current_conversation_id,
                        question=question,
                        answer=ai_result['answer'],
                        search_results=search_summary
                    )
                    
                    results.append({
                        'question': question,
                        'description': description,
                        'search_time': search_time,
                        'search_results_count': len(search_results),
                        'memory_results_count': len(self.memory_store.search_knowledge_base(question)),
                        'ai_success': True,
                        'ai_response_time': ai_result['response_time'],
                        'total_time': search_time + ai_result['response_time']
                    })
                else:
                    print(f"âŒ AI èª¿ç”¨å¤±æ•—: {ai_result['error']}")
                    results.append({
                        'question': question,
                        'description': description,
                        'search_time': search_time,
                        'ai_success': False,
                        'error': ai_result['error']
                    })
            
            time.sleep(1)  # é¿å…è«‹æ±‚éæ–¼é »ç¹
        
        # ä¿å­˜è¨˜æ†¶å’Œå°è©±æ­·å²
        self.memory_store.save_memory()
        self.memory_store.save_conversation_history()
        
        return results
    
    def test_memory_persistence(self):
        """æ¸¬è©¦è¨˜æ†¶æŒä¹…æ€§"""
        print("\n" + "="*60)
        print("ğŸ’¾ è¨˜æ†¶æŒä¹…æ€§æ¸¬è©¦")
        print("="*60)
        
        # é‡æ–°è¼‰å…¥è¨˜æ†¶
        old_memory_count = len(self.memory_store.memory_data['knowledge_base'])
        old_conversation_count = len(self.memory_store.conversation_history)
        
        # å‰µå»ºæ–°çš„è¨˜æ†¶å­˜å„²å¯¦ä¾‹ï¼ˆæ¨¡æ“¬é‡å•Ÿï¼‰
        print("ğŸ”„ æ¨¡æ“¬ç³»çµ±é‡å•Ÿï¼Œé‡æ–°è¼‰å…¥è¨˜æ†¶...")
        new_memory_store = PersistentMemoryStore()
        
        new_memory_count = len(new_memory_store.memory_data['knowledge_base'])
        new_conversation_count = len(new_memory_store.conversation_history)
        
        print(f"ğŸ“Š è¨˜æ†¶ä¿ç•™æƒ…æ³:")
        print(f"   çŸ¥è­˜åº«é …ç›®: {old_memory_count} â†’ {new_memory_count}")
        print(f"   å°è©±è¨˜éŒ„: {old_conversation_count} â†’ {new_conversation_count}")
        
        if new_memory_count >= old_memory_count and new_conversation_count >= old_conversation_count:
            print("âœ… è¨˜æ†¶æŒä¹…æ€§æ¸¬è©¦é€šé")
            
            # æ¸¬è©¦è¨˜æ†¶æœå°‹
            test_query = "æŠ€è¡“éƒ¨"
            relevant_memories = new_memory_store.search_knowledge_base(test_query)
            print(f"ğŸ” æœå°‹ã€Œ{test_query}ã€æ‰¾åˆ° {len(relevant_memories)} å€‹ç›¸é—œè¨˜æ†¶")
            
            for memory in relevant_memories[:2]:
                print(f"   - {memory['question'][:40]}... (è¨ªå•æ¬¡æ•¸: {memory['access_count']})")
        else:
            print("âŒ è¨˜æ†¶æŒä¹…æ€§æ¸¬è©¦å¤±æ•—")

def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    print("ğŸš€ æ°¸ä¹…è¨˜æ†¶å‘é‡åŒ– RAG æ¸¬è©¦ç³»çµ±")
    print("=" * 60)
    print(f"ğŸ”— API ç«¯é»: {DIFY_CONFIG['api_url']}")
    print(f"ğŸ§  åµŒå…¥æ¨¡å‹: {VECTOR_CONFIG['model_name']}")
    print(f"ğŸ’¾ è¨˜æ†¶å­˜å„²: {MEMORY_CONFIG['memory_file']}")
    print("=" * 60)
    
    # æª¢æŸ¥ä¾è³´
    print("\nğŸ” æª¢æŸ¥ç³»çµ±ä¾è³´:")
    print(f"ğŸ“¦ sentence-transformers: {'âœ… å¯ç”¨' if SENTENCE_TRANSFORMERS_AVAILABLE else 'âŒ ä¸å¯ç”¨'}")
    print(f"ğŸ“¦ chromadb: {'âœ… å¯ç”¨' if CHROMADB_AVAILABLE else 'âŒ ä¸å¯ç”¨'}")
    
    # åˆå§‹åŒ–æ¸¬è©¦å™¨
    tester = PersistentMemoryRAGTester()
    
    # 1. è¼‰å…¥ä¸¦å‘é‡åŒ–è³‡æ–™
    if not tester.load_and_vectorize_data():
        print("\nâŒ è³‡æ–™è¼‰å…¥å¤±æ•—ï¼Œæ¸¬è©¦çµ‚æ­¢")
        return
    
    # 2. åˆå§‹åŒ– Dify çŸ¥è­˜åº«ï¼ˆå¯é¸ï¼‰
    print("\nğŸ”§ Dify çŸ¥è­˜åº«åˆå§‹åŒ–...")
    kb_setup_choice = input("æ˜¯å¦å»ºç«‹ Dify çŸ¥è­˜åº«ï¼Ÿ(y/N): ").strip().lower()
    
    if kb_setup_choice in ['y', 'yes']:
        print("ğŸ“š é–‹å§‹å»ºç«‹ Dify çŸ¥è­˜åº«...")
        if tester.create_dify_dataset():
            print("â³ ç­‰å¾… 5 ç§’å¾Œé–‹å§‹ä¸Šå‚³è³‡æ–™...")
            time.sleep(5)
            if tester.upload_to_dify_knowledge_base():
                print("âœ… Dify çŸ¥è­˜åº«å»ºç«‹å®Œæˆ")
                print("â³ ç­‰å¾… 30 ç§’è®“ Dify å»ºç«‹ç´¢å¼•...")
                time.sleep(30)
            else:
                print("âš ï¸ Dify çŸ¥è­˜åº«ä¸Šå‚³å¤±æ•—ï¼Œå°‡ä½¿ç”¨æœ¬åœ°å‘é‡æœå°‹")
        else:
            print("âš ï¸ Dify çŸ¥è­˜åº«å»ºç«‹å¤±æ•—ï¼Œå°‡ä½¿ç”¨æœ¬åœ°å‘é‡æœå°‹")
    else:
        print("ğŸ“ è·³é Dify çŸ¥è­˜åº«å»ºç«‹ï¼Œåƒ…ä½¿ç”¨æœ¬åœ°å‘é‡æœå°‹")
    
    # 3. æ¸¬è©¦æ°¸ä¹…è¨˜æ†¶æŸ¥è©¢
    print(f"\nğŸ§  ç•¶å‰è¨˜æ†¶ç‹€æ…‹:")
    print(f"   çŸ¥è­˜åº«é …ç›®: {len(tester.memory_store.memory_data['knowledge_base'])}")
    print(f"   å°è©±è¨˜éŒ„: {len(tester.memory_store.conversation_history)}")
    
    query_results = tester.test_persistent_memory_queries()
    
    # 3. æ¸¬è©¦è¨˜æ†¶æŒä¹…æ€§
    tester.test_memory_persistence()
    
    # 4. ç¸½çµå ±å‘Š
    print("\n" + "="*60)
    print("ğŸ“Š æ°¸ä¹…è¨˜æ†¶æ¸¬è©¦ç¸½çµå ±å‘Š")
    print("="*60)
    
    if query_results:
        successful_queries = [r for r in query_results if r.get('ai_success', False)]
        print(f"âœ… æˆåŠŸæŸ¥è©¢: {len(successful_queries)}/{len(query_results)}")
        
        if successful_queries:
            avg_search_time = sum(r['search_time'] for r in successful_queries) / len(successful_queries)
            avg_ai_time = sum(r['ai_response_time'] for r in successful_queries) / len(successful_queries)
            avg_total_time = sum(r['total_time'] for r in successful_queries) / len(successful_queries)
            
            total_memory_used = sum(r.get('memory_results_count', 0) for r in successful_queries)
            
            print(f"â±ï¸ å¹³å‡å‘é‡æœå°‹æ™‚é–“: {avg_search_time:.2f}s")
            print(f"â±ï¸ å¹³å‡ AI å›æ‡‰æ™‚é–“: {avg_ai_time:.1f}s")
            print(f"â±ï¸ å¹³å‡ç¸½æ™‚é–“: {avg_total_time:.1f}s")
            print(f"ğŸ§  ç¸½è¨˜æ†¶ä½¿ç”¨æ¬¡æ•¸: {total_memory_used}")
            
            # çŸ¥è­˜åº«ä½¿ç”¨çµ±è¨ˆ
            if tester.knowledge_base_enabled:
                print(f"ğŸ“š Dify çŸ¥è­˜åº«ç‹€æ…‹: âœ… å·²å•Ÿç”¨ (è³‡æ–™é›† ID: {tester.dataset_id})")
            else:
                print(f"ğŸ“š Dify çŸ¥è­˜åº«ç‹€æ…‹: âŒ æœªå•Ÿç”¨")
    
    final_memory_count = len(tester.memory_store.memory_data['knowledge_base'])
    final_conversation_count = len(tester.memory_store.conversation_history)
    
    print(f"\nğŸ’¾ æœ€çµ‚è¨˜æ†¶ç‹€æ…‹:")
    print(f"   çŸ¥è­˜åº«é …ç›®: {final_memory_count}")
    print(f"   å°è©±è¨˜éŒ„: {final_conversation_count}")
    print(f"   è¨˜æ†¶æª”æ¡ˆ: {MEMORY_CONFIG['memory_file']}")
    print(f"   å°è©±æª”æ¡ˆ: {MEMORY_CONFIG['conversation_file']}")
    
    print("\nğŸ’¡ å¢å¼·ç‰ˆæ°¸ä¹…è¨˜æ†¶ç‰¹é»:")
    print("ğŸ§  AI æœƒè¨˜ä½å­¸ç¿’åˆ°çš„çŸ¥è­˜å’Œå°è©±æ­·å²")
    print("ğŸ’¾ è¨˜æ†¶è³‡æ–™æœƒä¿å­˜åœ¨ JSON æª”æ¡ˆä¸­")
    print("ğŸ”„ é‡å•Ÿå¾Œæœƒè‡ªå‹•è¼‰å…¥ä¹‹å‰çš„è¨˜æ†¶")
    print("ğŸ” æŸ¥è©¢æ™‚æœƒçµåˆæ­·å²çŸ¥è­˜å’Œæ–°æœå°‹çµæœ")
    print("ğŸ“ˆ è¨˜æ†¶æœƒæ ¹æ“šä½¿ç”¨é »ç‡é€²è¡Œæ’åº")
    print("ğŸ“š æ”¯æ´ Dify çŸ¥è­˜åº«æ•´åˆï¼Œå¯¦ç¾çœŸæ­£çš„æ°¸ä¹…è¨˜æ†¶")
    print("ğŸ¯ æ™ºèƒ½é¸æ“‡æœ€ä½³å›ç­”ä¾†æºï¼ˆçŸ¥è­˜åº«å„ªå…ˆï¼Œæœ¬åœ°å‘é‡å‚™ç”¨ï¼‰")
    print("ğŸ”— é›™é‡ä¿éšœï¼šDify é›²ç«¯çŸ¥è­˜ + æœ¬åœ°å‘é‡æœå°‹")
    
    print("\nâœ… æ°¸ä¹…è¨˜æ†¶å‘é‡åŒ– RAG æ¸¬è©¦å®Œæˆï¼")

if __name__ == "__main__":
    main()