#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å‘é‡åŒ– RAG æ¸¬è©¦è…³æœ¬
å°‡ company.db è³‡æ–™è½‰æ›ç‚ºå‘é‡è³‡æ–™ï¼Œä¸¦æ¸¬è©¦åŸºæ–¼å‘é‡æœå°‹çš„ AI å•ç­”åŠŸèƒ½
"""

import requests
import json
import sqlite3
import time
import os
import numpy as np
from typing import List, Dict, Any, Tuple
import hashlib

# å˜—è©¦å°å…¥å‘é‡åŒ–ç›¸é—œå¥—ä»¶
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    print("âš ï¸ sentence-transformers æœªå®‰è£ï¼Œå°‡ä½¿ç”¨ç°¡åŒ–ç‰ˆå‘é‡åŒ–")

try:
    import chromadb
    CHROMADB_AVAILABLE = True
except ImportError:
    CHROMADB_AVAILABLE = False
    print("âš ï¸ chromadb æœªå®‰è£ï¼Œå°‡ä½¿ç”¨å…§å»ºå‘é‡æœå°‹")

# Dify API é…ç½®
DIFY_CONFIG = {
    'api_url': 'http://10.10.172.37/v1/chat-messages',
    'api_key': 'app-R5nTTZw6jSQX75sDvyUMxLgo',
    'base_url': 'http://10.10.172.37'
}

# å‘é‡åŒ–é…ç½®
VECTOR_CONFIG = {
    'model_name': 'all-MiniLM-L6-v2',  # è¼•é‡åŒ–çš„å‘é‡æ¨¡å‹
    'vector_dim': 384,  # å‘é‡ç¶­åº¦
    'similarity_threshold': 0.7,  # ç›¸ä¼¼åº¦é–¾å€¼
    'max_results': 5  # æœ€å¤§æœå°‹çµæœæ•¸
}

class SimpleVectorStore:
    """ç°¡åŒ–ç‰ˆå‘é‡è³‡æ–™åº«ï¼ˆç•¶ ChromaDB ä¸å¯ç”¨æ™‚ä½¿ç”¨ï¼‰"""
    
    def __init__(self):
        self.vectors = []
        self.metadata = []
        self.texts = []
    
    def add_vectors(self, vectors: List[List[float]], texts: List[str], metadata: List[Dict]):
        """æ·»åŠ å‘é‡è³‡æ–™"""
        self.vectors.extend(vectors)
        self.texts.extend(texts)
        self.metadata.extend(metadata)
    
    def search(self, query_vector: List[float], top_k: int = 5) -> List[Tuple[str, Dict, float]]:
        """æœå°‹ç›¸ä¼¼å‘é‡"""
        if not self.vectors:
            return []
        
        # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
        query_vector = np.array(query_vector)
        similarities = []
        
        for i, vector in enumerate(self.vectors):
            vector = np.array(vector)
            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))
            similarities.append((i, similarity))
        
        # æ’åºä¸¦è¿”å›å‰ k å€‹çµæœ
        similarities.sort(key=lambda x: x[1], reverse=True)
        results = []
        
        for i, (idx, score) in enumerate(similarities[:top_k]):
            if score >= VECTOR_CONFIG['similarity_threshold']:
                results.append((self.texts[idx], self.metadata[idx], score))
        
        return results

class VectorRAGTester:
    """å‘é‡åŒ– RAG æ¸¬è©¦å™¨"""
    
    def __init__(self):
        self.embedding_model = None
        self.vector_store = None
        self.setup_embedding_model()
        self.setup_vector_store()
    
    def setup_embedding_model(self):
        """è¨­ç½®åµŒå…¥æ¨¡å‹"""
        if SENTENCE_TRANSFORMERS_AVAILABLE:
            try:
                print(f"ğŸ”§ è¼‰å…¥åµŒå…¥æ¨¡å‹: {VECTOR_CONFIG['model_name']}")
                self.embedding_model = SentenceTransformer(VECTOR_CONFIG['model_name'])
                print("âœ… åµŒå…¥æ¨¡å‹è¼‰å…¥æˆåŠŸ")
            except Exception as e:
                print(f"âŒ åµŒå…¥æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
                self.embedding_model = None
        else:
            print("âš ï¸ ä½¿ç”¨ç°¡åŒ–ç‰ˆå‘é‡åŒ–ï¼ˆåŸºæ–¼æ–‡å­—é•·åº¦å’Œå­—å…ƒé »ç‡ï¼‰")
    
    def setup_vector_store(self):
        """è¨­ç½®å‘é‡è³‡æ–™åº«"""
        if CHROMADB_AVAILABLE:
            try:
                print("ğŸ”§ è¨­ç½® ChromaDB å‘é‡è³‡æ–™åº«")
                self.chroma_client = chromadb.Client()
                self.collection = self.chroma_client.create_collection(
                    name="company_employees",
                    metadata={"hnsw:space": "cosine"}
                )
                print("âœ… ChromaDB å‘é‡è³‡æ–™åº«è¨­ç½®æˆåŠŸ")
            except Exception as e:
                print(f"âŒ ChromaDB è¨­ç½®å¤±æ•—: {e}")
                self.vector_store = SimpleVectorStore()
        else:
            print("ğŸ”§ ä½¿ç”¨ç°¡åŒ–ç‰ˆå‘é‡è³‡æ–™åº«")
            self.vector_store = SimpleVectorStore()
    
    def simple_embedding(self, text: str) -> List[float]:
        """ç°¡åŒ–ç‰ˆæ–‡å­—åµŒå…¥ï¼ˆç•¶ç„¡æ³•ä½¿ç”¨ SentenceTransformers æ™‚ï¼‰"""
        # åŸºæ–¼å­—å…ƒé »ç‡å’Œä½ç½®çš„ç°¡å–®å‘é‡åŒ–
        vector = [0.0] * 100  # ç°¡åŒ–ç‚º 100 ç¶­
        
        # æ–‡å­—é•·åº¦ç‰¹å¾µ
        vector[0] = len(text) / 100.0
        
        # å­—å…ƒé »ç‡ç‰¹å¾µ
        char_count = {}
        for char in text.lower():
            char_count[char] = char_count.get(char, 0) + 1
        
        # å°‡å¸¸è¦‹å­—å…ƒæ˜ å°„åˆ°å‘é‡ç¶­åº¦
        common_chars = 'abcdefghijklmnopqrstuvwxyz0123456789ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å'
        for i, char in enumerate(common_chars[:99]):
            if char in char_count:
                vector[i + 1] = char_count[char] / len(text)
        
        return vector
    
    def get_embedding(self, text: str) -> List[float]:
        """ç²å–æ–‡å­—åµŒå…¥å‘é‡"""
        if self.embedding_model:
            try:
                embedding = self.embedding_model.encode(text)
                return embedding.tolist()
            except Exception as e:
                print(f"âš ï¸ åµŒå…¥ç”Ÿæˆå¤±æ•—ï¼Œä½¿ç”¨ç°¡åŒ–ç‰ˆ: {e}")
                return self.simple_embedding(text)
        else:
            return self.simple_embedding(text)
    
    def load_and_vectorize_data(self):
        """è¼‰å…¥ä¸¦å‘é‡åŒ–è³‡æ–™åº«è³‡æ–™"""
        print("\n" + "="*60)
        print("ğŸ“Š è¼‰å…¥ä¸¦å‘é‡åŒ–å“¡å·¥è³‡æ–™")
        print("="*60)
        
        try:
            # é€£æ¥è³‡æ–™åº«
            conn = sqlite3.connect('tests/test_dify_integration/company.db')
            cursor = conn.cursor()
            
            # ç²å–æ‰€æœ‰å“¡å·¥è³‡æ–™
            cursor.execute("SELECT * FROM employees")
            columns = [description[0] for description in cursor.description]
            rows = cursor.fetchall()
            conn.close()
            
            print(f"ğŸ“‹ è¼‰å…¥ {len(rows)} ç­†å“¡å·¥è³‡æ–™")
            
            # è½‰æ›ç‚ºæ–‡å­—ä¸¦å‘é‡åŒ–
            texts = []
            metadata = []
            vectors = []
            
            for row in rows:
                # å°‡å“¡å·¥è³‡æ–™è½‰æ›ç‚ºå¯æœå°‹çš„æ–‡å­—
                employee_data = dict(zip(columns, row))
                
                # æ§‹å»ºæè¿°æ€§æ–‡å­—
                text = f"""
                å“¡å·¥å§“å: {employee_data['name']}
                éƒ¨é–€: {employee_data['department']}
                è·ä½: {employee_data['position']}
                è–ªè³‡: {employee_data['salary']}
                å…¥è·æ—¥æœŸ: {employee_data['hire_date']}
                é›»å­éƒµä»¶: {employee_data['email']}
                """.strip()
                
                # ç”Ÿæˆå‘é‡
                vector = self.get_embedding(text)
                
                texts.append(text)
                metadata.append(employee_data)
                vectors.append(vector)
                
                print(f"âœ… å·²å‘é‡åŒ–: {employee_data['name']} ({employee_data['department']})")
            
            # å­˜å„²åˆ°å‘é‡è³‡æ–™åº«
            if CHROMADB_AVAILABLE and hasattr(self, 'collection'):
                try:
                    # ç”Ÿæˆå”¯ä¸€ ID
                    ids = [f"emp_{i}" for i in range(len(texts))]
                    
                    self.collection.add(
                        embeddings=vectors,
                        documents=texts,
                        metadatas=metadata,
                        ids=ids
                    )
                    print("âœ… è³‡æ–™å·²å­˜å„²åˆ° ChromaDB")
                except Exception as e:
                    print(f"âš ï¸ ChromaDB å­˜å„²å¤±æ•—ï¼Œä½¿ç”¨ç°¡åŒ–ç‰ˆ: {e}")
                    self.vector_store = SimpleVectorStore()
                    self.vector_store.add_vectors(vectors, texts, metadata)
            else:
                self.vector_store.add_vectors(vectors, texts, metadata)
                print("âœ… è³‡æ–™å·²å­˜å„²åˆ°ç°¡åŒ–ç‰ˆå‘é‡è³‡æ–™åº«")
            
            print(f"ğŸ¯ å‘é‡åŒ–å®Œæˆï¼Œç¸½å…±è™•ç† {len(texts)} ç­†è³‡æ–™")
            return True
            
        except Exception as e:
            print(f"âŒ è³‡æ–™è¼‰å…¥å¤±æ•—: {e}")
            return False
    
    def vector_search(self, query: str, top_k: int = 3) -> List[Tuple[str, Dict, float]]:
        """å‘é‡æœå°‹"""
        try:
            # å°‡æŸ¥è©¢è½‰æ›ç‚ºå‘é‡
            query_vector = self.get_embedding(query)
            
            if CHROMADB_AVAILABLE and hasattr(self, 'collection'):
                # ä½¿ç”¨ ChromaDB æœå°‹
                results = self.collection.query(
                    query_embeddings=[query_vector],
                    n_results=top_k
                )
                
                search_results = []
                for i in range(len(results['documents'][0])):
                    text = results['documents'][0][i]
                    metadata = results['metadatas'][0][i]
                    distance = results['distances'][0][i]
                    similarity = 1 - distance  # è½‰æ›ç‚ºç›¸ä¼¼åº¦åˆ†æ•¸
                    search_results.append((text, metadata, similarity))
                
                return search_results
            else:
                # ä½¿ç”¨ç°¡åŒ–ç‰ˆæœå°‹
                return self.vector_store.search(query_vector, top_k)
        
        except Exception as e:
            print(f"âŒ å‘é‡æœå°‹å¤±æ•—: {e}")
            return []
    
    def call_dify_with_context(self, question: str, context_data: str = "") -> dict:
        """èª¿ç”¨ Dify API ä¸¦å‚³å…¥ä¸Šä¸‹æ–‡"""
        headers = {
            'Authorization': f'Bearer {DIFY_CONFIG["api_key"]}',
            'Content-Type': 'application/json'
        }
        
        payload = {
            'inputs': {
                'context': context_data
            } if context_data else {},
            'query': question,
            'response_mode': 'blocking',
            'user': 'vector_rag_test'
        }
        
        try:
            print(f"ğŸ¤– èª¿ç”¨ Dify AI: {question[:50]}...")
            start_time = time.time()
            
            response = requests.post(
                DIFY_CONFIG['api_url'],
                headers=headers,
                json=payload,
                timeout=60
            )
            
            elapsed = time.time() - start_time
            
            if response.status_code == 200:
                result = response.json()
                return {
                    'success': True,
                    'answer': result.get('answer', ''),
                    'response_time': elapsed,
                    'message_id': result.get('message_id', ''),
                    'conversation_id': result.get('conversation_id', '')
                }
            else:
                return {
                    'success': False,
                    'error': f"HTTP {response.status_code}: {response.text}",
                    'response_time': elapsed
                }
                
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'response_time': 0
            }
    
    def test_vector_rag_queries(self):
        """æ¸¬è©¦å‘é‡åŒ– RAG æŸ¥è©¢"""
        print("\n" + "="*60)
        print("ğŸ” å‘é‡åŒ– RAG æŸ¥è©¢æ¸¬è©¦")
        print("="*60)
        
        test_queries = [
            {
                'question': 'æŠ€è¡“éƒ¨æœ‰å“ªäº›å“¡å·¥ï¼Ÿä»–å€‘çš„è–ªè³‡å¦‚ä½•ï¼Ÿ',
                'description': 'éƒ¨é–€ç¯©é¸æŸ¥è©¢'
            },
            {
                'question': 'è–ªè³‡æœ€é«˜çš„å“¡å·¥æ˜¯èª°ï¼Ÿ',
                'description': 'è–ªè³‡æ’åºæŸ¥è©¢'
            },
            {
                'question': 'æœ‰å“ªäº›è»Ÿé«”å·¥ç¨‹å¸«ï¼Ÿ',
                'description': 'è·ä½ç¯©é¸æŸ¥è©¢'
            },
            {
                'question': '2022å¹´å…¥è·çš„å“¡å·¥æœ‰å“ªäº›ï¼Ÿ',
                'description': 'æ™‚é–“ç¯©é¸æŸ¥è©¢'
            },
            {
                'question': 'æç¾è¯çš„è©³ç´°è³‡è¨Š',
                'description': 'ç‰¹å®šå“¡å·¥æŸ¥è©¢'
            }
        ]
        
        results = []
        
        for i, query_info in enumerate(test_queries, 1):
            question = query_info['question']
            description = query_info['description']
            
            print(f"\nğŸ”¸ æ¸¬è©¦ {i}: {description}")
            print(f"â“ å•é¡Œ: {question}")
            print("-" * 50)
            
            # 1. å‘é‡æœå°‹ç›¸é—œè³‡æ–™
            search_start = time.time()
            search_results = self.vector_search(question, top_k=3)
            search_time = time.time() - search_start
            
            if search_results:
                print(f"ğŸ¯ æ‰¾åˆ° {len(search_results)} å€‹ç›¸é—œçµæœ ({search_time:.2f}s)")
                
                # æ§‹å»ºä¸Šä¸‹æ–‡
                context_texts = []
                for text, metadata, score in search_results:
                    print(f"   - {metadata['name']} ({metadata['department']}) ç›¸ä¼¼åº¦: {score:.3f}")
                    context_texts.append(text)
                
                context_data = "\n".join(context_texts)
                
                # 2. ä½¿ç”¨æœå°‹çµæœä½œç‚ºä¸Šä¸‹æ–‡èª¿ç”¨ AI
                ai_result = self.call_dify_with_context(question, context_data)
                
                if ai_result['success']:
                    print(f"âœ… AI å›ç­” ({ai_result['response_time']:.1f}s):")
                    print(f"ğŸ“ {ai_result['answer']}")
                    
                    results.append({
                        'question': question,
                        'description': description,
                        'search_time': search_time,
                        'search_results_count': len(search_results),
                        'ai_success': True,
                        'ai_response_time': ai_result['response_time'],
                        'total_time': search_time + ai_result['response_time']
                    })
                else:
                    print(f"âŒ AI èª¿ç”¨å¤±æ•—: {ai_result['error']}")
                    results.append({
                        'question': question,
                        'description': description,
                        'search_time': search_time,
                        'search_results_count': len(search_results),
                        'ai_success': False,
                        'error': ai_result['error']
                    })
            else:
                print("âŒ æœªæ‰¾åˆ°ç›¸é—œè³‡æ–™")
                results.append({
                    'question': question,
                    'description': description,
                    'search_time': search_time,
                    'search_results_count': 0,
                    'ai_success': False,
                    'error': 'æœªæ‰¾åˆ°ç›¸é—œè³‡æ–™'
                })
            
            time.sleep(1)  # é¿å…è«‹æ±‚éæ–¼é »ç¹
        
        return results
    
    def compare_vector_vs_traditional_rag(self):
        """æ¯”è¼ƒå‘é‡åŒ– RAG èˆ‡å‚³çµ± RAG"""
        print("\n" + "="*60)
        print("âš–ï¸ å‘é‡åŒ– RAG vs å‚³çµ± RAG æ¯”è¼ƒ")
        print("="*60)
        
        test_question = "æŠ€è¡“éƒ¨çš„å¹³å‡è–ªè³‡æ˜¯å¤šå°‘ï¼Ÿ"
        print(f"ğŸ”¸ æ¸¬è©¦å•é¡Œ: {test_question}")
        
        # 1. å‘é‡åŒ– RAG æ–¹æ³•
        print("\nğŸ”¹ å‘é‡åŒ– RAG æ–¹æ³•:")
        print("-" * 30)
        
        vector_start = time.time()
        search_results = self.vector_search(test_question, top_k=5)
        
        if search_results:
            context_texts = [text for text, _, _ in search_results]
            context_data = "\n".join(context_texts)
            vector_ai_result = self.call_dify_with_context(test_question, context_data)
            vector_total_time = time.time() - vector_start
            
            print(f"âœ… å‘é‡æœå°‹æ‰¾åˆ° {len(search_results)} å€‹çµæœ")
            if vector_ai_result['success']:
                print(f"ğŸ“ AI å›ç­”: {vector_ai_result['answer'][:150]}...")
                print(f"â±ï¸ ç¸½è€—æ™‚: {vector_total_time:.1f}s")
        
        # 2. å‚³çµ± RAG æ–¹æ³•ï¼ˆç›´æ¥å‚³å…¥æ‰€æœ‰è³‡æ–™ï¼‰
        print("\nğŸ”¹ å‚³çµ± RAG æ–¹æ³•:")
        print("-" * 30)
        
        traditional_start = time.time()
        
        # ç²å–æ‰€æœ‰å“¡å·¥è³‡æ–™
        try:
            conn = sqlite3.connect('tests/test_dify_integration/company.db')
            cursor = conn.cursor()
            cursor.execute("SELECT * FROM employees WHERE department = 'æŠ€è¡“éƒ¨'")
            columns = [description[0] for description in cursor.description]
            rows = cursor.fetchall()
            conn.close()
            
            employees = [dict(zip(columns, row)) for row in rows]
            traditional_context = json.dumps(employees, ensure_ascii=False, indent=2)
            
            traditional_ai_result = self.call_dify_with_context(test_question, traditional_context)
            traditional_total_time = time.time() - traditional_start
            
            print(f"âœ… ç›´æ¥æŸ¥è©¢æ‰¾åˆ° {len(employees)} ç­†è³‡æ–™")
            if traditional_ai_result['success']:
                print(f"ğŸ“ AI å›ç­”: {traditional_ai_result['answer'][:150]}...")
                print(f"â±ï¸ ç¸½è€—æ™‚: {traditional_total_time:.1f}s")
        
        except Exception as e:
            print(f"âŒ å‚³çµ±æ–¹æ³•å¤±æ•—: {e}")
        
        # 3. æ¯”è¼ƒçµæœ
        print("\nğŸ”¸ æ¯”è¼ƒç¸½çµ:")
        print("-" * 30)
        if 'vector_total_time' in locals() and 'traditional_total_time' in locals():
            print(f"ğŸ“Š å‘é‡åŒ– RAG: {vector_total_time:.1f}s")
            print(f"ğŸ“Š å‚³çµ± RAG: {traditional_total_time:.1f}s")
            if vector_total_time < traditional_total_time:
                print("ğŸ† å‘é‡åŒ– RAG æ›´å¿«")
            else:
                print("ğŸ† å‚³çµ± RAG æ›´å¿«")

def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    print("ğŸš€ å‘é‡åŒ– RAG æ¸¬è©¦ç³»çµ±")
    print("=" * 60)
    print(f"ğŸ”— API ç«¯é»: {DIFY_CONFIG['api_url']}")
    print(f"ğŸ§  åµŒå…¥æ¨¡å‹: {VECTOR_CONFIG['model_name']}")
    print(f"ğŸ“ å‘é‡ç¶­åº¦: {VECTOR_CONFIG['vector_dim']}")
    print("=" * 60)
    
    # æª¢æŸ¥ä¾è³´
    print("\nğŸ” æª¢æŸ¥ç³»çµ±ä¾è³´:")
    print(f"ğŸ“¦ sentence-transformers: {'âœ… å¯ç”¨' if SENTENCE_TRANSFORMERS_AVAILABLE else 'âŒ ä¸å¯ç”¨'}")
    print(f"ğŸ“¦ chromadb: {'âœ… å¯ç”¨' if CHROMADB_AVAILABLE else 'âŒ ä¸å¯ç”¨'}")
    
    # åˆå§‹åŒ–æ¸¬è©¦å™¨
    tester = VectorRAGTester()
    
    # 1. è¼‰å…¥ä¸¦å‘é‡åŒ–è³‡æ–™
    if not tester.load_and_vectorize_data():
        print("\nâŒ è³‡æ–™è¼‰å…¥å¤±æ•—ï¼Œæ¸¬è©¦çµ‚æ­¢")
        return
    
    # 2. æ¸¬è©¦å‘é‡åŒ– RAG æŸ¥è©¢
    query_results = tester.test_vector_rag_queries()
    
    # 3. æ¯”è¼ƒä¸åŒæ–¹æ³•
    tester.compare_vector_vs_traditional_rag()
    
    # 4. ç¸½çµå ±å‘Š
    print("\n" + "="*60)
    print("ğŸ“Š æ¸¬è©¦ç¸½çµå ±å‘Š")
    print("="*60)
    
    if query_results:
        successful_queries = [r for r in query_results if r.get('ai_success', False)]
        print(f"âœ… æˆåŠŸæŸ¥è©¢: {len(successful_queries)}/{len(query_results)}")
        
        if successful_queries:
            avg_search_time = sum(r['search_time'] for r in successful_queries) / len(successful_queries)
            avg_ai_time = sum(r['ai_response_time'] for r in successful_queries) / len(successful_queries)
            avg_total_time = sum(r['total_time'] for r in successful_queries) / len(successful_queries)
            
            print(f"â±ï¸ å¹³å‡å‘é‡æœå°‹æ™‚é–“: {avg_search_time:.2f}s")
            print(f"â±ï¸ å¹³å‡ AI å›æ‡‰æ™‚é–“: {avg_ai_time:.1f}s")
            print(f"â±ï¸ å¹³å‡ç¸½æ™‚é–“: {avg_total_time:.1f}s")
    
    print("\nğŸ’¡ å»ºè­°:")
    if not SENTENCE_TRANSFORMERS_AVAILABLE:
        print("ğŸ”§ å®‰è£ sentence-transformers ä»¥ç²å¾—æ›´å¥½çš„å‘é‡åŒ–æ•ˆæœ:")
        print("   pip install sentence-transformers")
    
    if not CHROMADB_AVAILABLE:
        print("ğŸ”§ å®‰è£ chromadb ä»¥ç²å¾—æ›´å¥½çš„å‘é‡è³‡æ–™åº«æ•ˆèƒ½:")
        print("   pip install chromadb")
    
    print("\nâœ… å‘é‡åŒ– RAG æ¸¬è©¦å®Œæˆï¼")

if __name__ == "__main__":
    main()