#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Dify çŸ¥è­˜åº« API æ¸¬è©¦ç¨‹å¼ - ä½¿ç”¨ Dataset Token
ç¾åœ¨å¯ä»¥è‡ªå‹•ç®¡ç†çŸ¥è­˜åº«äº†ï¼
"""

import requests
import json
import sqlite3
import time
import os
from datetime import datetime
from typing import List, Dict, Any, Optional

# Dify API é…ç½®
DIFY_CONFIG = {
    # èŠå¤©æ‡‰ç”¨ API (ç”¨æ–¼æŸ¥è©¢)
    'chat_api_url': 'http://10.10.172.37/v1/chat-messages',
    'chat_api_key': 'app-R5nTTZw6jSQX75sDvyUMxLgo',
    
    # çŸ¥è­˜åº« API (ç”¨æ–¼ç®¡ç†è³‡æ–™é›†)
    'dataset_api_key': 'dataset-JLa32OwILQHkgPqYStTCW4sC',
    'base_url': 'http://10.10.172.37'
}

class DifyFullKnowledgeBaseTester:
    """å®Œæ•´çš„ Dify çŸ¥è­˜åº«æ¸¬è©¦å™¨ - ä½¿ç”¨ Dataset Token"""
    
    def __init__(self):
        self.employees_data = self.load_employee_data()
        self.dataset_id = None
        self.document_ids = []
        
        # ä¸åŒ API çš„è«‹æ±‚é ­
        self.dataset_headers = {
            'Authorization': f'Bearer {DIFY_CONFIG["dataset_api_key"]}',
            'Content-Type': 'application/json'
        }
        
        self.chat_headers = {
            'Authorization': f'Bearer {DIFY_CONFIG["chat_api_key"]}',
            'Content-Type': 'application/json'
        }
    
    def load_employee_data(self):
        """è¼‰å…¥å“¡å·¥è³‡æ–™"""
        try:
            conn = sqlite3.connect('tests/test_dify_integration/company.db')
            cursor = conn.cursor()
            cursor.execute("SELECT * FROM employees")
            columns = [description[0] for description in cursor.description]
            rows = cursor.fetchall()
            conn.close()
            
            employees = []
            for row in rows:
                employee = dict(zip(columns, row))
                employees.append(employee)
            
            print(f"ğŸ“Š è¼‰å…¥ {len(employees)} ä½å“¡å·¥è³‡æ–™")
            return employees
        except Exception as e:
            print(f"âŒ è¼‰å…¥è³‡æ–™å¤±æ•—: {e}")
            return []
    
    def create_dataset(self) -> bool:
        """ä½¿ç”¨ Dataset API å»ºç«‹è³‡æ–™é›†"""
        print("\nğŸ“š ä½¿ç”¨ Dataset API å»ºç«‹è³‡æ–™é›†...")
        
        dataset_data = {
            'name': f'å“¡å·¥è³‡æ–™åº«_å®Œæ•´ç‰ˆ_{int(time.time())}',
            'description': 'ä½¿ç”¨ Dataset API å»ºç«‹çš„å®Œæ•´å“¡å·¥è³‡æ–™åº«ï¼Œæ”¯æ´è‡ªå‹•ç®¡ç†å’ŒæŸ¥è©¢'
        }
        
        try:
            response = requests.post(
                f'{DIFY_CONFIG["base_url"]}/v1/datasets',
                headers=self.dataset_headers,
                json=dataset_data,
                timeout=30
            )
            
            print(f"ğŸ“¥ å»ºç«‹è³‡æ–™é›†å›æ‡‰: HTTP {response.status_code}")
            
            if response.status_code == 200:
                result = response.json()
                self.dataset_id = result.get('id')
                print(f"âœ… è³‡æ–™é›†å»ºç«‹æˆåŠŸï¼")
                print(f"   ğŸ“‹ è³‡æ–™é›† ID: {self.dataset_id}")
                print(f"   ğŸ“ è³‡æ–™é›†åç¨±: {result.get('name')}")
                return True
            else:
                print(f"âŒ å»ºç«‹è³‡æ–™é›†å¤±æ•—: {response.text}")
                return False
                
        except Exception as e:
            print(f"âŒ å»ºç«‹è³‡æ–™é›†ç•°å¸¸: {e}")
            return False
    
    def upload_employee_documents(self) -> bool:
        """ä¸Šå‚³å“¡å·¥æ–‡ä»¶åˆ°çŸ¥è­˜åº«"""
        if not self.dataset_id:
            print("âŒ æ²’æœ‰è³‡æ–™é›† IDï¼Œç„¡æ³•ä¸Šå‚³æ–‡ä»¶")
            return False
        
        print(f"\nğŸ“„ ä¸Šå‚³ {len(self.employees_data)} ä»½å“¡å·¥æ–‡ä»¶åˆ°çŸ¥è­˜åº«...")
        
        success_count = 0
        
        for i, employee in enumerate(self.employees_data, 1):
            # æ§‹å»ºè©³ç´°çš„å“¡å·¥æ–‡ä»¶å…§å®¹
            document_content = f"""# {employee['name']} - å“¡å·¥æª”æ¡ˆ

## åŸºæœ¬è³‡è¨Š
- **å“¡å·¥ç·¨è™Ÿ**: {employee['id']}
- **å§“å**: {employee['name']}
- **éƒ¨é–€**: {employee['department']}
- **è·ä½**: {employee['position']}
- **è–ªè³‡**: {employee['salary']:,} å…ƒï¼ˆæœˆè–ªï¼‰
- **å…¥è·æ—¥æœŸ**: {employee['hire_date']}
- **é›»å­éƒµä»¶**: {employee['email']}

## è©³ç´°æè¿°
{employee['name']} æ˜¯ {employee['department']} çš„ {employee['position']}ï¼Œ
å“¡å·¥ç·¨è™Ÿç‚º {employee['id']}ï¼Œæœˆè–ª {employee['salary']:,} å…ƒã€‚
æ–¼ {employee['hire_date']} å…¥è·ï¼Œè¯çµ¡ä¿¡ç®±ç‚º {employee['email']}ã€‚

## éƒ¨é–€è³‡è¨Š
æ‰€å±¬éƒ¨é–€ï¼š{employee['department']}
è·ä½å±¤ç´šï¼š{employee['position']}
è–ªè³‡æ°´æº–ï¼š{employee['salary']:,} å…ƒ

## è¯çµ¡æ–¹å¼
Email: {employee['email']}
å…¥è·æ™‚é–“: {employee['hire_date']}
"""
            
            document_data = {
                'text': document_content,
                'metadata': {
                    'employee_id': str(employee['id']),
                    'name': employee['name'],
                    'department': employee['department'],
                    'position': employee['position'],
                    'salary': str(employee['salary']),
                    'hire_date': employee['hire_date'],
                    'source': 'employee_database'
                }
            }
            
            try:
                response = requests.post(
                    f'{DIFY_CONFIG["base_url"]}/v1/datasets/{self.dataset_id}/documents',
                    headers=self.dataset_headers,
                    json=document_data,
                    timeout=30
                )
                
                if response.status_code == 200:
                    result = response.json()
                    doc_id = result.get('id')
                    self.document_ids.append(doc_id)
                    success_count += 1
                    print(f"âœ… {employee['name']} ä¸Šå‚³æˆåŠŸ ({i}/{len(self.employees_data)}) - Doc ID: {doc_id}")
                else:
                    print(f"âŒ {employee['name']} ä¸Šå‚³å¤±æ•—: HTTP {response.status_code}")
                    print(f"   å›æ‡‰: {response.text}")
                
                # é¿å…è«‹æ±‚éæ–¼é »ç¹
                time.sleep(0.3)
                
            except Exception as e:
                print(f"âŒ ä¸Šå‚³ {employee['name']} ç•°å¸¸: {e}")
        
        print(f"\nğŸ“Š ä¸Šå‚³çµæœ: {success_count}/{len(self.employees_data)} ä»½æ–‡ä»¶æˆåŠŸ")
        return success_count == len(self.employees_data)
    
    def wait_for_indexing(self):
        """ç­‰å¾…çŸ¥è­˜åº«ç´¢å¼•å»ºç«‹å®Œæˆ"""
        print("\nâ³ ç­‰å¾…çŸ¥è­˜åº«ç´¢å¼•å»ºç«‹...")
        
        for i in range(6):  # ç­‰å¾…æœ€å¤š 60 ç§’
            time.sleep(10)
            
            try:
                # æª¢æŸ¥è³‡æ–™é›†ç‹€æ…‹
                response = requests.get(
                    f'{DIFY_CONFIG["base_url"]}/v1/datasets/{self.dataset_id}',
                    headers=self.dataset_headers,
                    timeout=10
                )
                
                if response.status_code == 200:
                    result = response.json()
                    doc_count = result.get('document_count', 0)
                    print(f"   ç´¢å¼•é€²åº¦: {doc_count} æ–‡ä»¶å·²ç´¢å¼• ({(i+1)*10}s)")
                    
                    if doc_count >= len(self.employees_data):
                        print("âœ… ç´¢å¼•å»ºç«‹å®Œæˆï¼")
                        return True
                else:
                    print(f"   æª¢æŸ¥ç‹€æ…‹å¤±æ•—: HTTP {response.status_code}")
                    
            except Exception as e:
                print(f"   æª¢æŸ¥ç´¢å¼•ç‹€æ…‹ç•°å¸¸: {e}")
        
        print("âš ï¸ ç´¢å¼•å¯èƒ½é‚„åœ¨å»ºç«‹ä¸­ï¼Œç¹¼çºŒæ¸¬è©¦...")
        return False
    
    def query_with_dataset(self, question: str) -> dict:
        """ä½¿ç”¨é…ç½®äº†çŸ¥è­˜åº«çš„æ‡‰ç”¨é€²è¡ŒæŸ¥è©¢"""
        print(f"\nâ“ æŸ¥è©¢å•é¡Œ: {question}")
        
        payload = {
            'inputs': {},
            'query': question,
            'response_mode': 'blocking',
            'user': 'dataset_test',
            'conversation_id': '',
            'files': []
        }
        
        try:
            start_time = time.time()
            response = requests.post(
                DIFY_CONFIG['chat_api_url'],
                headers=self.chat_headers,
                json=payload,
                timeout=30
            )
            elapsed = time.time() - start_time
            
            print(f"ğŸ“¥ å›æ‡‰: HTTP {response.status_code} ({elapsed:.1f}s)")
            
            if response.status_code == 200:
                result = response.json()
                answer = result.get('answer', '')
                metadata = result.get('metadata', {})
                
                print(f"âœ… AI å›ç­”:")
                print(answer)
                
                # æª¢æŸ¥æ˜¯å¦ä½¿ç”¨äº†çŸ¥è­˜åº«
                retrieval_sources = metadata.get('retrieval_sources', [])
                if retrieval_sources:
                    print(f"\nğŸ“š çŸ¥è­˜åº«æª¢ç´¢çµæœ ({len(retrieval_sources)} å€‹ä¾†æº):")
                    for i, source in enumerate(retrieval_sources, 1):
                        print(f"   {i}. ç›¸ä¼¼åº¦: {source.get('score', 'N/A')}")
                        print(f"      æ–‡ä»¶ç‰‡æ®µ: {source.get('content', 'N/A')[:100]}...")
                        print()
                    
                    return {
                        'success': True,
                        'answer': answer,
                        'uses_knowledge_base': True,
                        'sources_count': len(retrieval_sources),
                        'response_time': elapsed
                    }
                else:
                    print("âš ï¸ æ²’æœ‰æª¢ç´¢åˆ°çŸ¥è­˜åº«è³‡æº")
                    return {
                        'success': True,
                        'answer': answer,
                        'uses_knowledge_base': False,
                        'sources_count': 0,
                        'response_time': elapsed
                    }
            else:
                print(f"âŒ æŸ¥è©¢å¤±æ•—: {response.text}")
                return {'success': False, 'error': response.text}
                
        except Exception as e:
            print(f"âŒ æŸ¥è©¢ç•°å¸¸: {e}")
            return {'success': False, 'error': str(e)}
    
    def run_comprehensive_test(self):
        """åŸ·è¡Œå®Œæ•´çš„çŸ¥è­˜åº«æ¸¬è©¦"""
        print("\n" + "="*60)
        print("ğŸ§  å®Œæ•´çŸ¥è­˜åº«åŠŸèƒ½æ¸¬è©¦")
        print("="*60)
        
        test_questions = [
            "æŠ€è¡“éƒ¨æœ‰å“ªäº›å“¡å·¥ï¼Ÿè«‹åˆ—å‡ºå§“åå’Œè·ä½ã€‚",
            "æç¾è¯çš„è©³ç´°è³‡è¨Šæ˜¯ä»€éº¼ï¼ŸåŒ…æ‹¬è–ªè³‡å’Œå…¥è·æ™‚é–“ã€‚",
            "è–ªè³‡æœ€é«˜çš„å“¡å·¥æ˜¯èª°ï¼Ÿå¤šå°‘éŒ¢ï¼Ÿ",
            "2022å¹´å…¥è·çš„å“¡å·¥æœ‰å“ªäº›ï¼Ÿ",
            "æŠ€è¡“éƒ¨çš„å¹³å‡è–ªè³‡æ˜¯å¤šå°‘ï¼Ÿ",
            "äººè³‡éƒ¨å’Œè²¡å‹™éƒ¨å“ªå€‹éƒ¨é–€è–ªè³‡è¼ƒé«˜ï¼Ÿ",
            "å…¥è·æœ€æ—©å’Œæœ€æ™šçš„å“¡å·¥åˆ†åˆ¥æ˜¯èª°ï¼Ÿ",
            "email åœ°å€åŒ…å« 'li' çš„å“¡å·¥æ˜¯èª°ï¼Ÿ"
        ]
        
        results = []
        kb_usage_count = 0
        total_response_time = 0
        
        for i, question in enumerate(test_questions, 1):
            print(f"\n--- æ¸¬è©¦ {i}/{len(test_questions)} ---")
            
            result = self.query_with_dataset(question)
            results.append({
                'question': question,
                'result': result
            })
            
            if result.get('success') and result.get('uses_knowledge_base'):
                kb_usage_count += 1
            
            if result.get('response_time'):
                total_response_time += result['response_time']
            
            # é¿å…è«‹æ±‚éæ–¼é »ç¹
            time.sleep(1)
        
        # çµ±è¨ˆçµæœ
        print("\n" + "="*60)
        print("ğŸ“Š å®Œæ•´æ¸¬è©¦çµæœçµ±è¨ˆ")
        print("="*60)
        
        successful_queries = sum(1 for r in results if r['result'].get('success'))
        avg_response_time = total_response_time / len(test_questions) if test_questions else 0
        
        print(f"âœ… æˆåŠŸæŸ¥è©¢: {successful_queries}/{len(test_questions)}")
        print(f"ğŸ“š ä½¿ç”¨çŸ¥è­˜åº«: {kb_usage_count}/{len(test_questions)}")
        print(f"â±ï¸ å¹³å‡å›æ‡‰æ™‚é–“: {avg_response_time:.1f}s")
        
        if kb_usage_count >= len(test_questions) * 0.7:  # 70% ä»¥ä¸Šä½¿ç”¨çŸ¥è­˜åº«
            print("\nğŸ‰ çŸ¥è­˜åº«æ•´åˆå¤§æˆåŠŸï¼")
            print("   ğŸ“‹ AI èƒ½è‡ªå‹•æª¢ç´¢å“¡å·¥è³‡æ–™")
            print("   ğŸ§  å¯¦ç¾äº†çœŸæ­£çš„æ°¸ä¹…è¨˜æ†¶")
            print("   ğŸš€ å¯ä»¥ç”¨æ–¼ç”Ÿç”¢ç’°å¢ƒ")
        elif kb_usage_count > 0:
            print("\nğŸ‘ çŸ¥è­˜åº«éƒ¨åˆ†æˆåŠŸ")
            print("   âš ï¸ å¯èƒ½éœ€è¦èª¿æ•´æ‡‰ç”¨é…ç½®")
            print("   ğŸ”§ æˆ–ç­‰å¾…æ›´é•·æ™‚é–“è®“ç´¢å¼•å®Œæˆ")
        else:
            print("\nâŒ çŸ¥è­˜åº«æ•´åˆå¤±æ•—")
            print("   ğŸ” è«‹æª¢æŸ¥æ‡‰ç”¨æ˜¯å¦é…ç½®ä½¿ç”¨è³‡æ–™é›†")
            print("   â° æˆ–éœ€è¦ç­‰å¾…ç´¢å¼•å»ºç«‹å®Œæˆ")
        
        return results
    
    def list_datasets_info(self):
        """åˆ—å‡ºè³‡æ–™é›†è³‡è¨Š"""
        print("\nğŸ“‹ åˆ—å‡ºæ‰€æœ‰è³‡æ–™é›†...")
        
        try:
            response = requests.get(
                f'{DIFY_CONFIG["base_url"]}/v1/datasets',
                headers=self.dataset_headers,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                datasets = result.get('data', [])
                
                print(f"ğŸ“š æ‰¾åˆ° {len(datasets)} å€‹è³‡æ–™é›†:")
                for dataset in datasets:
                    print(f"   - {dataset['name']} (ID: {dataset['id']})")
                    print(f"     æ–‡ä»¶æ•¸: {dataset.get('document_count', 0)}")
                    print(f"     å»ºç«‹æ™‚é–“: {dataset.get('created_at', 'N/A')}")
                    print()
                
                return datasets
            else:
                print(f"âŒ åˆ—å‡ºè³‡æ–™é›†å¤±æ•—: HTTP {response.status_code}")
                return []
                
        except Exception as e:
            print(f"âŒ åˆ—å‡ºè³‡æ–™é›†ç•°å¸¸: {e}")
            return []
    
    def cleanup_test_dataset(self):
        """æ¸…ç†æ¸¬è©¦è³‡æ–™é›†"""
        if not self.dataset_id:
            return
        
        print(f"\nğŸ§¹ æ¸…ç†æ¸¬è©¦è³‡æ–™é›†...")
        
        try:
            response = requests.delete(
                f'{DIFY_CONFIG["base_url"]}/v1/datasets/{self.dataset_id}',
                headers=self.dataset_headers,
                timeout=30
            )
            
            if response.status_code == 200:
                print("âœ… æ¸¬è©¦è³‡æ–™é›†æ¸…ç†å®Œæˆ")
            else:
                print(f"âš ï¸ æ¸…ç†å¤±æ•—: HTTP {response.status_code}")
                print(f"   å¯èƒ½éœ€è¦æ‰‹å‹•æ¸…ç†è³‡æ–™é›† ID: {self.dataset_id}")
                
        except Exception as e:
            print(f"âŒ æ¸…ç†ç•°å¸¸: {e}")

def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    print("ğŸš€ Dify å®Œæ•´çŸ¥è­˜åº« API æ¸¬è©¦ç³»çµ±")
    print("=" * 60)
    print("ğŸ¯ ç›®æ¨™ï¼šä½¿ç”¨ Dataset Token å¯¦ç¾å®Œæ•´è‡ªå‹•åŒ–")
    print("ğŸ”‘ Dataset Token: dataset-JLa32OwILQHkgPqYStTCW4sC")
    print("=" * 60)
    
    tester = DifyFullKnowledgeBaseTester()
    
    if not tester.employees_data:
        print("âŒ ç„¡æ³•è¼‰å…¥å“¡å·¥è³‡æ–™ï¼Œæ¸¬è©¦çµ‚æ­¢")
        return
    
    try:
        # 1. åˆ—å‡ºç¾æœ‰è³‡æ–™é›†
        tester.list_datasets_info()
        
        # 2. å»ºç«‹æ–°è³‡æ–™é›†
        if not tester.create_dataset():
            print("âŒ ç„¡æ³•å»ºç«‹è³‡æ–™é›†ï¼Œæ¸¬è©¦çµ‚æ­¢")
            return
        
        # 3. ä¸Šå‚³å“¡å·¥æ–‡ä»¶
        if not tester.upload_employee_documents():
            print("âŒ æ–‡ä»¶ä¸Šå‚³ä¸å®Œæ•´ï¼Œç¹¼çºŒæ¸¬è©¦...")
        
        # 4. ç­‰å¾…ç´¢å¼•å»ºç«‹
        tester.wait_for_indexing()
        
        # 5. åŸ·è¡Œå®Œæ•´æ¸¬è©¦
        results = tester.run_comprehensive_test()
        
        # 6. é¡¯ç¤ºæœ€çµ‚çµæœ
        print("\n" + "="*60)
        print("ğŸ‰ æ¸¬è©¦å®Œæˆï¼Dataset Token åŠŸèƒ½é©—è­‰")
        print("="*60)
        print(f"ğŸ“‹ è³‡æ–™é›† ID: {tester.dataset_id}")
        print(f"ğŸ“„ ä¸Šå‚³æ–‡ä»¶æ•¸: {len(tester.document_ids)}")
        
        kb_usage = sum(1 for r in results if r['result'].get('uses_knowledge_base'))
        print(f"ğŸ“š çŸ¥è­˜åº«ä½¿ç”¨ç‡: {kb_usage}/{len(results)} ({kb_usage/len(results)*100:.1f}%)")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ æ¸¬è©¦è¢«ä¸­æ–·")
    finally:
        # è©¢å•æ˜¯å¦æ¸…ç†
        cleanup = input("\næ˜¯å¦æ¸…ç†æ¸¬è©¦è³‡æ–™é›†ï¼Ÿ(y/N): ").strip().lower()
        if cleanup in ['y', 'yes']:
            tester.cleanup_test_dataset()
        else:
            print(f"ğŸ’¾ è³‡æ–™é›†ä¿ç•™ï¼ŒID: {tester.dataset_id}")
            print("   æ‚¨å¯ä»¥åœ¨ Dify å¹³å°æŸ¥çœ‹æˆ–æ‰‹å‹•ç®¡ç†")

if __name__ == "__main__":
    main()