#!/usr/bin/env python3
"""
Dify AI å›ç­”å“è³ªæ¸¬è©¦
====================

æ¸¬è©¦ç›®æ¨™ï¼š
1. å‘ Dify API ç™¼é€ 10 æ¬¡ç›¸åŒå•é¡Œ
2. çµ±è¨ˆ AI å›ç­”å“è³ª
3. è¨˜éŒ„æ‰€æœ‰å›æ‡‰å…§å®¹ä¾›åˆ†æ
4. æª¢æŸ¥æ˜¯å¦ä½¿ç”¨äº†å¤–éƒ¨çŸ¥è­˜åº«

ä½¿ç”¨æ–¹æ³•ï¼š
    python tests/test_dify_answer_quality.py

è¼¸å‡ºï¼š
    - è©³ç´°çš„æ¸¬è©¦çµæœ
    - JSON æ ¼å¼çš„å®Œæ•´è¨˜éŒ„ï¼ˆå¯ç”¨æ–¼å¾ŒçºŒåˆ†æï¼‰
"""

import requests
import json
import time
from datetime import datetime
from pathlib import Path


class DifyAnswerQualityTest:
    """Dify å›ç­”å“è³ªæ¸¬è©¦é¡"""
    
    def __init__(self):
        self.api_url = "http://10.10.172.37/v1/chat-messages"
        self.api_key = "app-MgZZOhADkEmdUrj2DtQLJ23G"  # Protocol Guide API Key
        self.test_query = "crystaldiskmark å¦‚ä½•æ”¾æ¸¬"
        self.results = []
        
    def send_request(self, test_number, use_conversation_id=False, conversation_id=None):
        """
        ç™¼é€å–®æ¬¡æ¸¬è©¦è«‹æ±‚
        
        Args:
            test_number: æ¸¬è©¦ç·¨è™Ÿ
            use_conversation_id: æ˜¯å¦ä½¿ç”¨ conversation_id
            conversation_id: å°è©± ID
            
        Returns:
            dict: æ¸¬è©¦çµæœ
        """
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "inputs": {},
            "query": self.test_query,
            "response_mode": "blocking",
            "user": "test_user_quality_check",
            "retrieval_model": {
                "search_method": "semantic_search",
                "reranking_enable": False,
                "reranking_mode": None,
                "top_k": 3,
                "score_threshold_enabled": False
            }
        }
        
        # å¦‚æœéœ€è¦ä½¿ç”¨ conversation_id
        if use_conversation_id and conversation_id:
            payload["conversation_id"] = conversation_id
        
        print(f"\n{'='*60}")
        print(f"æ¸¬è©¦ #{test_number}")
        print(f"{'='*60}")
        print(f"ğŸ“¤ ç™¼é€è«‹æ±‚: {self.test_query}")
        if conversation_id:
            print(f"ğŸ”— Conversation ID: {conversation_id}")
        
        start_time = time.time()
        
        try:
            response = requests.post(
                self.api_url,
                headers=headers,
                json=payload,
                timeout=60
            )
            
            elapsed_time = time.time() - start_time
            
            if response.status_code == 200:
                data = response.json()
                return self._analyze_response(test_number, data, elapsed_time, conversation_id)
            else:
                print(f"âŒ è«‹æ±‚å¤±æ•—: {response.status_code}")
                return {
                    "test_number": test_number,
                    "success": False,
                    "error": f"HTTP {response.status_code}",
                    "timestamp": datetime.now().isoformat()
                }
                
        except Exception as e:
            print(f"âŒ è«‹æ±‚ç•°å¸¸: {str(e)}")
            return {
                "test_number": test_number,
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    def _analyze_response(self, test_number, data, elapsed_time, conversation_id):
        """åˆ†æ Dify å›æ‡‰"""
        answer = data.get("answer", "")
        metadata = data.get("metadata", {})
        retriever_resources = metadata.get("retriever_resources", [])
        new_conversation_id = data.get("conversation_id", "")
        message_id = data.get("message_id", "")
        
        # åˆ†æå›ç­”å“è³ª
        answer_length = len(answer)
        has_resources = len(retriever_resources) > 0
        resource_count = len(retriever_resources)
        
        # åˆ¤æ–·å›ç­”å“è³ªï¼ˆåŸºæ–¼é•·åº¦å’Œæ˜¯å¦ä½¿ç”¨çŸ¥è­˜åº«ï¼‰
        quality_score = self._calculate_quality_score(
            answer, 
            answer_length, 
            has_resources,
            retriever_resources
        )
        
        # æª¢æŸ¥æ˜¯å¦æ˜¯"æ‰¾ä¸åˆ°è³‡æ–™"çš„å›ç­”
        negative_keywords = [
            "ç„¡æ³•æ‰¾åˆ°", "æ‰¾ä¸åˆ°", "æœªæ‰¾åˆ°", "æ²’æœ‰æ‰¾åˆ°",
            "ç„¡æ³•åœ¨è³‡æ–™åº«", "è³‡æ–™åº«ä¸­ç„¡", "ç›®å‰ç„¡æ³•"
        ]
        is_negative_answer = any(keyword in answer for keyword in negative_keywords)
        
        # è¼¸å‡ºçµæœ
        print(f"â±ï¸  å›æ‡‰æ™‚é–“: {elapsed_time:.2f}s")
        print(f"ğŸ“ å›ç­”é•·åº¦: {answer_length} å­—å…ƒ")
        print(f"ğŸ” çŸ¥è­˜åº«ä½¿ç”¨: {'âœ… æ˜¯' if has_resources else 'âŒ å¦'} ({resource_count} æ¢)")
        
        if has_resources:
            print(f"ğŸ“š å¼•ç”¨æ–‡æª”:")
            for i, resource in enumerate(retriever_resources, 1):
                doc_name = resource.get("document_name", "Unknown")
                score = resource.get("score", 0)
                print(f"   {i}. {doc_name} (åˆ†æ•¸: {score:.4f})")
        
        print(f"ğŸ¯ å“è³ªè©•åˆ†: {quality_score}/10")
        
        if is_negative_answer and has_resources:
            print(f"âš ï¸  è­¦å‘Š: AI èªªæ‰¾ä¸åˆ°è³‡æ–™ï¼Œä½†å¯¦éš›æœ‰ {resource_count} æ¢çŸ¥è­˜åº«çµæœï¼")
        
        # é¡¯ç¤ºå›ç­”é è¦½
        answer_preview = answer[:150] + "..." if len(answer) > 150 else answer
        print(f"ğŸ’¬ å›ç­”é è¦½:\n{answer_preview}\n")
        
        # æ§‹å»ºçµæœ
        result = {
            "test_number": test_number,
            "success": True,
            "timestamp": datetime.now().isoformat(),
            "query": self.test_query,
            "conversation_id": new_conversation_id or conversation_id,
            "message_id": message_id,
            "response_time": round(elapsed_time, 2),
            "answer": {
                "content": answer,
                "length": answer_length,
                "preview": answer_preview
            },
            "knowledge_base": {
                "used": has_resources,
                "resource_count": resource_count,
                "resources": [
                    {
                        "document_name": r.get("document_name", ""),
                        "score": r.get("score", 0),
                        "position": r.get("position", 0)
                    }
                    for r in retriever_resources
                ]
            },
            "quality_analysis": {
                "score": quality_score,
                "is_negative_answer": is_negative_answer,
                "has_contradiction": is_negative_answer and has_resources,
                "is_high_quality": quality_score >= 7 and has_resources and not is_negative_answer
            }
        }
        
        return result
    
    def _calculate_quality_score(self, answer, length, has_resources, resources):
        """
        è¨ˆç®—å›ç­”å“è³ªè©•åˆ† (0-10)
        
        è©•åˆ†æ¨™æº–ï¼š
        - åŸºç¤åˆ†: 5 åˆ†
        - ä½¿ç”¨çŸ¥è­˜åº«: +2 åˆ†
        - å›ç­”é•·åº¦ > 500 å­—: +2 åˆ†
        - å›ç­”é•·åº¦ 200-500 å­—: +1 åˆ†
        - çŸ¥è­˜åº«åˆ†æ•¸ > 0.8: +1 åˆ†
        - åŒ…å«ã€Œæ‰¾ä¸åˆ°ã€ç­‰è² é¢è©: -3 åˆ†
        """
        score = 5  # åŸºç¤åˆ†
        
        # ä½¿ç”¨çŸ¥è­˜åº«
        if has_resources:
            score += 2
        else:
            score -= 2
        
        # å›ç­”é•·åº¦
        if length > 500:
            score += 2
        elif length > 200:
            score += 1
        elif length < 100:
            score -= 1
        
        # çŸ¥è­˜åº«åˆ†æ•¸
        if resources:
            max_score = max([r.get("score", 0) for r in resources])
            if max_score > 0.8:
                score += 1
        
        # è² é¢é—œéµå­—
        negative_keywords = [
            "ç„¡æ³•æ‰¾åˆ°", "æ‰¾ä¸åˆ°", "æœªæ‰¾åˆ°", "æ²’æœ‰æ‰¾åˆ°",
            "ç„¡æ³•åœ¨è³‡æ–™åº«", "è³‡æ–™åº«ä¸­ç„¡", "ç›®å‰ç„¡æ³•"
        ]
        if any(keyword in answer for keyword in negative_keywords):
            score -= 3
        
        return max(0, min(10, score))  # é™åˆ¶åœ¨ 0-10 ç¯„åœ
    
    def run_tests(self, num_tests=10, use_conversation=True):
        """
        åŸ·è¡Œå¤šæ¬¡æ¸¬è©¦
        
        Args:
            num_tests: æ¸¬è©¦æ¬¡æ•¸
            use_conversation: æ˜¯å¦ä½¿ç”¨å°è©±æ¨¡å¼ï¼ˆæ‰€æœ‰è«‹æ±‚å…±ç”¨ä¸€å€‹ conversation_idï¼‰
        """
        print(f"\nğŸš€ é–‹å§‹ Dify AI å›ç­”å“è³ªæ¸¬è©¦")
        print(f"ğŸ“Š æ¸¬è©¦æ¬¡æ•¸: {num_tests}")
        print(f"ğŸ”— å°è©±æ¨¡å¼: {'å•Ÿç”¨' if use_conversation else 'åœç”¨'}")
        print(f"â“ æ¸¬è©¦å•é¡Œ: {self.test_query}")
        print(f"ğŸŒ API ç«¯é»: {self.api_url}")
        
        conversation_id = None
        
        for i in range(1, num_tests + 1):
            result = self.send_request(
                test_number=i,
                use_conversation_id=use_conversation and conversation_id is not None,
                conversation_id=conversation_id
            )
            
            self.results.append(result)
            
            # å¦‚æœæ˜¯å°è©±æ¨¡å¼ï¼Œå„²å­˜ conversation_id
            if use_conversation and result.get("success"):
                conversation_id = result.get("conversation_id")
            
            # é¿å…è«‹æ±‚éå¿«
            if i < num_tests:
                time.sleep(1)
        
        # çµ±è¨ˆå’Œåˆ†æ
        self._print_summary()
        self._save_results()
    
    def _print_summary(self):
        """è¼¸å‡ºæ¸¬è©¦æ‘˜è¦"""
        print(f"\n{'='*60}")
        print(f"ğŸ“Š æ¸¬è©¦æ‘˜è¦")
        print(f"{'='*60}")
        
        successful_tests = [r for r in self.results if r.get("success")]
        total_tests = len(self.results)
        
        if not successful_tests:
            print("âŒ æ‰€æœ‰æ¸¬è©¦éƒ½å¤±æ•—äº†")
            return
        
        # åŸºæœ¬çµ±è¨ˆ
        print(f"\nâœ… æˆåŠŸæ¸¬è©¦: {len(successful_tests)}/{total_tests}")
        
        # çŸ¥è­˜åº«ä½¿ç”¨çµ±è¨ˆ
        kb_used_count = sum(1 for r in successful_tests if r.get("knowledge_base", {}).get("used"))
        print(f"ğŸ“š ä½¿ç”¨çŸ¥è­˜åº«: {kb_used_count}/{len(successful_tests)} ({kb_used_count/len(successful_tests)*100:.1f}%)")
        
        # å›ç­”é•·åº¦çµ±è¨ˆ
        lengths = [r["answer"]["length"] for r in successful_tests]
        avg_length = sum(lengths) / len(lengths)
        print(f"ğŸ“ å¹³å‡å›ç­”é•·åº¦: {avg_length:.0f} å­—å…ƒ")
        print(f"   æœ€çŸ­: {min(lengths)} å­—å…ƒ")
        print(f"   æœ€é•·: {max(lengths)} å­—å…ƒ")
        
        # å“è³ªè©•åˆ†çµ±è¨ˆ
        scores = [r["quality_analysis"]["score"] for r in successful_tests]
        avg_score = sum(scores) / len(scores)
        high_quality_count = sum(1 for r in successful_tests if r["quality_analysis"]["is_high_quality"])
        
        print(f"\nğŸ¯ å“è³ªè©•åˆ†:")
        print(f"   å¹³å‡åˆ†æ•¸: {avg_score:.1f}/10")
        print(f"   æœ€ä½åˆ†æ•¸: {min(scores)}/10")
        print(f"   æœ€é«˜åˆ†æ•¸: {max(scores)}/10")
        print(f"   é«˜å“è³ªå›ç­”: {high_quality_count}/{len(successful_tests)} ({high_quality_count/len(successful_tests)*100:.1f}%)")
        
        # çŸ›ç›¾æƒ…æ³çµ±è¨ˆ
        contradictions = sum(1 for r in successful_tests if r["quality_analysis"]["has_contradiction"])
        if contradictions > 0:
            print(f"\nâš ï¸  ç™¼ç¾ {contradictions} æ¬¡çŸ›ç›¾ï¼ˆAI èªªæ‰¾ä¸åˆ°ä½†å¯¦éš›æœ‰çŸ¥è­˜åº«çµæœï¼‰")
        
        # å›æ‡‰æ™‚é–“çµ±è¨ˆ
        times = [r["response_time"] for r in successful_tests]
        avg_time = sum(times) / len(times)
        print(f"\nâ±ï¸  å¹³å‡å›æ‡‰æ™‚é–“: {avg_time:.2f}s")
        print(f"   æœ€å¿«: {min(times):.2f}s")
        print(f"   æœ€æ…¢: {max(times):.2f}s")
        
        # å“è³ªåˆ†ä½ˆ
        print(f"\nğŸ“ˆ å“è³ªåˆ†ä½ˆ:")
        score_ranges = [
            ("å„ªç§€ (8-10åˆ†)", 8, 10),
            ("è‰¯å¥½ (6-7åˆ†)", 6, 7),
            ("ä¸­ç­‰ (4-5åˆ†)", 4, 5),
            ("è¼ƒå·® (0-3åˆ†)", 0, 3)
        ]
        for label, min_score, max_score in score_ranges:
            count = sum(1 for s in scores if min_score <= s <= max_score)
            percentage = count / len(scores) * 100
            print(f"   {label}: {count}/{len(scores)} ({percentage:.1f}%)")
    
    def _save_results(self):
        """å„²å­˜æ¸¬è©¦çµæœåˆ°æª”æ¡ˆ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"dify_quality_test_{timestamp}.json"
        
        # å‰µå»ºæ¸¬è©¦çµæœç›®éŒ„
        output_dir = Path(__file__).parent / "test_results"
        output_dir.mkdir(exist_ok=True)
        
        filepath = output_dir / filename
        
        # æº–å‚™å®Œæ•´å ±å‘Š
        report = {
            "test_info": {
                "timestamp": datetime.now().isoformat(),
                "query": self.test_query,
                "api_url": self.api_url,
                "total_tests": len(self.results)
            },
            "summary": self._generate_summary(),
            "detailed_results": self.results
        }
        
        # å„²å­˜ç‚º JSON
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ æ¸¬è©¦çµæœå·²å„²å­˜: {filepath}")
        
        # åŒæ™‚å„²å­˜ä¸€ä»½æœ€æ–°çš„
        latest_filepath = output_dir / "latest_quality_test.json"
        with open(latest_filepath, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ æœ€æ–°çµæœå‰¯æœ¬: {latest_filepath}")
    
    def _generate_summary(self):
        """ç”Ÿæˆçµ±è¨ˆæ‘˜è¦"""
        successful_tests = [r for r in self.results if r.get("success")]
        
        if not successful_tests:
            return {"error": "æ‰€æœ‰æ¸¬è©¦éƒ½å¤±æ•—"}
        
        kb_used_count = sum(1 for r in successful_tests if r.get("knowledge_base", {}).get("used"))
        lengths = [r["answer"]["length"] for r in successful_tests]
        scores = [r["quality_analysis"]["score"] for r in successful_tests]
        times = [r["response_time"] for r in successful_tests]
        high_quality_count = sum(1 for r in successful_tests if r["quality_analysis"]["is_high_quality"])
        contradictions = sum(1 for r in successful_tests if r["quality_analysis"]["has_contradiction"])
        
        return {
            "total_tests": len(self.results),
            "successful_tests": len(successful_tests),
            "knowledge_base_usage": {
                "count": kb_used_count,
                "percentage": round(kb_used_count / len(successful_tests) * 100, 1)
            },
            "answer_length": {
                "average": round(sum(lengths) / len(lengths), 0),
                "min": min(lengths),
                "max": max(lengths)
            },
            "quality_score": {
                "average": round(sum(scores) / len(scores), 1),
                "min": min(scores),
                "max": max(scores),
                "high_quality_count": high_quality_count,
                "high_quality_percentage": round(high_quality_count / len(successful_tests) * 100, 1)
            },
            "response_time": {
                "average": round(sum(times) / len(times), 2),
                "min": round(min(times), 2),
                "max": round(max(times), 2)
            },
            "contradictions": contradictions
        }


def main():
    """ä¸»å‡½æ•¸"""
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘         Dify AI å›ç­”å“è³ªæ¸¬è©¦å·¥å…·                          â•‘
    â•‘         Protocol Assistant - CrystalDiskMark æ¸¬è©¦         â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # å‰µå»ºæ¸¬è©¦å¯¦ä¾‹
    tester = DifyAnswerQualityTest()
    
    # åŸ·è¡Œæ¸¬è©¦
    # use_conversation=True: æ‰€æœ‰è«‹æ±‚ä½¿ç”¨åŒä¸€å€‹ conversation_idï¼ˆæ¨¡æ“¬é€£çºŒå°è©±ï¼‰
    # use_conversation=False: æ¯æ¬¡éƒ½æ˜¯æ–°å°è©±
    tester.run_tests(num_tests=10, use_conversation=False)
    
    print("\nâœ… æ¸¬è©¦å®Œæˆï¼")
    print("ğŸ“ è©³ç´°çµæœå·²å„²å­˜è‡³ tests/test_results/ ç›®éŒ„")


if __name__ == "__main__":
    main()
