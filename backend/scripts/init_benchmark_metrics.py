"""
åˆå§‹åŒ–æœå°‹è·‘åˆ†ç³»çµ±çš„é è¨­è©•åˆ†ç¶­åº¦
Date: 2025-11-21
"""
import os
import sys
import django

# è¨­å®š Django ç’°å¢ƒ
sys.path.append("/app")
os.environ.setdefault("DJANGO_SETTINGS_MODULE", "ai_platform.settings")
django.setup()

from api.models import BenchmarkMetric


def init_metrics():
    """åˆå§‹åŒ–é è¨­è©•åˆ†ç¶­åº¦"""
    
    metrics = [
        {
            "metric_name": "ç²¾æº–åº¦ (Precision)",
            "metric_key": "precision",
            "metric_type": "precision",
            "description": "å›å‚³çµæœä¸­æ­£ç¢ºç­”æ¡ˆçš„æ¯”ä¾‹",
            "calculation_method": "TP / (TP + FP)",
            "weight": 0.35,
            "display_order": 1
        },
        {
            "metric_name": "å¬å›ç‡ (Recall)",
            "metric_key": "recall",
            "metric_type": "recall",
            "description": "æ­£ç¢ºç­”æ¡ˆè¢«æ‰¾å›çš„æ¯”ä¾‹",
            "calculation_method": "TP / (TP + FN)",
            "weight": 0.30,
            "display_order": 2
        },
        {
            "metric_name": "F1 åˆ†æ•¸ (F1-Score)",
            "metric_key": "f1_score",
            "metric_type": "quality",
            "description": "ç²¾æº–åº¦å’Œå¬å›ç‡çš„èª¿å’Œå¹³å‡æ•¸",
            "calculation_method": "2 * (Precision * Recall) / (Precision + Recall)",
            "weight": 0.20,
            "display_order": 3
        },
        {
            "metric_name": "å¹³å‡éŸ¿æ‡‰æ™‚é–“ (Avg Response Time)",
            "metric_key": "avg_response_time",
            "metric_type": "speed",
            "description": "æœå°‹æŸ¥è©¢çš„å¹³å‡è™•ç†æ™‚é–“ (ms)",
            "calculation_method": "sum(response_times) / count",
            "weight": 0.10,
            "display_order": 4
        },
        {
            "metric_name": "NDCG@5",
            "metric_key": "ndcg_at_5",
            "metric_type": "quality",
            "description": "è€ƒæ…®æ’åºçš„æœå°‹å“è³ªæŒ‡æ¨™",
            "calculation_method": "DCG / IDCG (å‰5å€‹çµæœ)",
            "weight": 0.05,
            "display_order": 5
        }
    ]
    
    print("ğŸš€ é–‹å§‹åˆå§‹åŒ–è©•åˆ†ç¶­åº¦...")
    print(f"   ç¸½å…± {len(metrics)} å€‹ç¶­åº¦\n")
    
    for metric_data in metrics:
        metric, created = BenchmarkMetric.objects.update_or_create(
            metric_key=metric_data['metric_key'],
            defaults=metric_data
        )
        
        status = "âœ… å‰µå»º" if created else "âœ… æ›´æ–°"
        print(f"{status}: {metric.metric_name} (æ¬Šé‡: {metric.weight * 100}%)")
    
    print(f"\nâœ… é è¨­è©•åˆ†ç¶­åº¦åˆå§‹åŒ–å®Œæˆï¼")
    
    # é©—è­‰
    total = BenchmarkMetric.objects.filter(is_active=True).count()
    total_weight = sum(m.weight for m in BenchmarkMetric.objects.filter(is_active=True))
    
    print(f"\nğŸ“Š é©—è­‰çµæœ:")
    print(f"   å•Ÿç”¨çš„ç¶­åº¦æ•¸é‡: {total}")
    print(f"   ç¸½æ¬Šé‡: {total_weight:.2f} (æ‡‰ç‚º 1.00)")
    
    if abs(float(total_weight) - 1.0) < 0.01:
        print("   âœ… æ¬Šé‡ç¸½å’Œæ­£ç¢º")
    else:
        print(f"   âš ï¸  è­¦å‘Šï¼šæ¬Šé‡ç¸½å’Œä¸ç‚º 1.00")


if __name__ == '__main__':
    init_metrics()
