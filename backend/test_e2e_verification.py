#!/usr/bin/env python
"""
ç«¯åˆ°ç«¯é©—è­‰æ¸¬è©¦
============

ç›®çš„ï¼š
1. æ¸¬è©¦æ–°ç­–ç•¥å¼•æ“å¯åŸ·è¡Œï¼ˆV3 æ··åˆ 70-30ï¼‰
2. å°æ¯” Baseline (ID=3) å’Œ V3 (ID=7) çš„çµæœå·®ç•°
3. ç¢ºèª Protocol Assistant ä¸å—å½±éŸ¿ï¼ˆé€é API æ¸¬è©¦ï¼‰

é©—è­‰æµç¨‹ï¼š
1. ä½¿ç”¨ Baseline åŸ·è¡Œ 3 å€‹æ¸¬è©¦æ¡ˆä¾‹
2. ä½¿ç”¨ V3 (æ··åˆ 70-30) åŸ·è¡Œç›¸åŒæ¸¬è©¦æ¡ˆä¾‹
3. å°æ¯”çµæœå·®ç•°
4. é©—è­‰ç­–ç•¥å¼•æ“æ—¥èªŒ
"""

import os
import sys
import django
import json

# è¨­ç½® Django ç’°å¢ƒ
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'ai_platform.settings')
django.setup()

from api.models import SearchAlgorithmVersion, BenchmarkTestCase
from library.benchmark.test_runner import BenchmarkTestRunner
import logging

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s'
)

def run_version_test(version_id, version_name, test_cases):
    """åŸ·è¡Œç‰¹å®šç‰ˆæœ¬çš„æ¸¬è©¦"""
    print(f"\n{'='*80}")
    print(f"ğŸ§ª æ¸¬è©¦ç‰ˆæœ¬: {version_name} (ID={version_id})")
    print(f"{'='*80}")
    
    try:
        # è¼‰å…¥ç‰ˆæœ¬
        version = SearchAlgorithmVersion.objects.get(id=version_id)
        params = version.parameters or {}
        use_engine = params.get('use_strategy_engine', False)
        strategy = params.get('strategy', 'N/A')
        
        print(f"\nğŸ“‹ ç‰ˆæœ¬è³‡è¨Š:")
        print(f"   - ç‰ˆæœ¬åç¨±: {version.version_name}")
        print(f"   - ç‰ˆæœ¬ä»£ç¢¼: {version.version_code}")
        print(f"   - ç®—æ³•é¡å‹: {version.algorithm_type}")
        print(f"   - ä½¿ç”¨ç­–ç•¥å¼•æ“: {use_engine}")
        print(f"   - ç­–ç•¥: {strategy}")
        
        if use_engine:
            print(f"   - ç­–ç•¥åƒæ•¸: {json.dumps(params, ensure_ascii=False, indent=6)}")
        
        # åŸ·è¡Œæ¸¬è©¦
        runner = BenchmarkTestRunner(version_id=version_id, verbose=False)
        
        results = []
        for i, test_case in enumerate(test_cases, 1):
            print(f"\n[{i}/{len(test_cases)}] æ¸¬è©¦: {test_case.question[:50]}...")
            
            result = runner.run_single_test(test_case, save_to_db=False)
            
            # é¡¯ç¤ºçµæœ
            print(f"   âœ… è¿”å›: {len(result['returned_document_ids'])} å€‹çµæœ")
            print(f"   - Document IDs: {result['returned_document_ids'][:5]}")
            print(f"   - Response Time: {result['response_time']:.2f} ms")
            print(f"   - Precision: {result.get('precision', 0):.2%}")
            print(f"   - Recall: {result.get('recall', 0):.2%}")
            print(f"   - F1 Score: {result.get('f1_score', 0):.2%}")
            print(f"   - NDCG: {result.get('ndcg', 0):.4f}")
            
            results.append(result)
        
        # è¨ˆç®—å¹³å‡æŒ‡æ¨™
        n = len(results)
        avg_precision = sum(r.get('precision', 0) for r in results) / n
        avg_recall = sum(r.get('recall', 0) for r in results) / n
        avg_f1 = sum(r.get('f1_score', 0) for r in results) / n
        avg_ndcg = sum(r.get('ndcg', 0) for r in results) / n
        avg_time = sum(r.get('response_time', 0) for r in results) / n
        
        print(f"\n{'='*80}")
        print(f"ğŸ“Š å¹³å‡æŒ‡æ¨™:")
        print(f"   - Precision: {avg_precision:.2%}")
        print(f"   - Recall: {avg_recall:.2%}")
        print(f"   - F1 Score: {avg_f1:.2%}")
        print(f"   - NDCG: {avg_ndcg:.4f}")
        print(f"   - Response Time: {avg_time:.2f} ms")
        print(f"{'='*80}")
        
        return {
            'version_id': version_id,
            'version_name': version_name,
            'results': results,
            'avg_precision': avg_precision,
            'avg_recall': avg_recall,
            'avg_f1': avg_f1,
            'avg_ndcg': avg_ndcg,
            'avg_time': avg_time,
        }
        
    except Exception as e:
        print(f"\nâŒ æ¸¬è©¦å¤±æ•—: {str(e)}")
        import traceback
        traceback.print_exc()
        return None


def compare_results(baseline_result, new_result):
    """å°æ¯”å…©å€‹ç‰ˆæœ¬çš„çµæœ"""
    print("\n" + "="*80)
    print("ğŸ“Š çµæœå°æ¯”")
    print("="*80)
    
    if not baseline_result or not new_result:
        print("âŒ ç„¡æ³•å°æ¯”ï¼ˆæŸå€‹ç‰ˆæœ¬æ¸¬è©¦å¤±æ•—ï¼‰")
        return
    
    # å°æ¯”è¡¨æ ¼
    print(f"\n{'æŒ‡æ¨™':<15} | {'Baseline':>12} | {'V3 (70-30)':>12} | {'å·®ç•°':>12}")
    print("-" * 80)
    
    metrics = [
        ('Precision', 'avg_precision', '%'),
        ('Recall', 'avg_recall', '%'),
        ('F1 Score', 'avg_f1', '%'),
        ('NDCG', 'avg_ndcg', ''),
        ('Response Time', 'avg_time', 'ms'),
    ]
    
    for metric_name, metric_key, unit in metrics:
        baseline_val = baseline_result[metric_key]
        new_val = new_result[metric_key]
        
        if unit == '%':
            diff = new_val - baseline_val
            diff_str = f"{diff:+.2%}"
            baseline_str = f"{baseline_val:.2%}"
            new_str = f"{new_val:.2%}"
        elif unit == 'ms':
            diff = new_val - baseline_val
            diff_str = f"{diff:+.2f} ms"
            baseline_str = f"{baseline_val:.2f}"
            new_str = f"{new_val:.2f}"
        else:
            diff = new_val - baseline_val
            diff_str = f"{diff:+.4f}"
            baseline_str = f"{baseline_val:.4f}"
            new_str = f"{new_val:.4f}"
        
        print(f"{metric_name:<15} | {baseline_str:>12} | {new_str:>12} | {diff_str:>12}")
    
    print("="*80)
    
    # åˆ†æçµè«–
    print("\nğŸ“ åˆ†æçµè«–:")
    
    precision_diff = new_result['avg_precision'] - baseline_result['avg_precision']
    recall_diff = new_result['avg_recall'] - baseline_result['avg_recall']
    f1_diff = new_result['avg_f1'] - baseline_result['avg_f1']
    
    if precision_diff > 0:
        print(f"   âœ… ç²¾æº–åº¦æå‡: {precision_diff:+.2%}")
    elif precision_diff < 0:
        print(f"   âš ï¸  ç²¾æº–åº¦ä¸‹é™: {precision_diff:.2%}")
    else:
        print(f"   â– ç²¾æº–åº¦æŒå¹³")
    
    if recall_diff > 0:
        print(f"   âœ… å¬å›ç‡æå‡: {recall_diff:+.2%}")
    elif recall_diff < 0:
        print(f"   âš ï¸  å¬å›ç‡ä¸‹é™: {recall_diff:.2%}")
    else:
        print(f"   â– å¬å›ç‡æŒå¹³")
    
    if f1_diff > 0:
        print(f"   âœ… F1 Score æå‡: {f1_diff:+.2%}")
    elif f1_diff < 0:
        print(f"   âš ï¸  F1 Score ä¸‹é™: {f1_diff:.2%}")
    else:
        print(f"   â– F1 Score æŒå¹³")
    
    # æ•´é«”è©•åƒ¹
    print("\nğŸ¯ æ•´é«”è©•åƒ¹:")
    if f1_diff > 0.05:
        print("   ğŸŒŸ æ–°ç­–ç•¥é¡¯è‘—å„ªæ–¼ Baselineï¼")
    elif f1_diff > 0:
        print("   âœ… æ–°ç­–ç•¥ç•¥å„ªæ–¼ Baseline")
    elif f1_diff > -0.05:
        print("   â– æ–°ç­–ç•¥èˆ‡ Baseline ç›¸ç•¶")
    else:
        print("   âš ï¸  æ–°ç­–ç•¥ä¸å¦‚ Baselineï¼Œéœ€è¦èª¿æ•´åƒæ•¸")


def test_protocol_assistant_api():
    """æ¸¬è©¦ Protocol Assistant API æ˜¯å¦æ­£å¸¸ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰"""
    print("\n" + "="*80)
    print("ğŸŒ Protocol Assistant API æ¸¬è©¦")
    print("="*80)
    
    try:
        from library.protocol_guide.search_service import ProtocolGuideSearchService
        
        search_service = ProtocolGuideSearchService()
        
        # åŸ·è¡Œä¸€å€‹ç°¡å–®çš„æœå°‹
        test_query = "ULINK æ¸¬è©¦"
        print(f"\nğŸ” æ¸¬è©¦æŸ¥è©¢: '{test_query}'")
        
        results = search_service.search_knowledge(
            query=test_query,
            limit=5,
            use_vector=True
        )
        
        print(f"   âœ… è¿”å› {len(results)} å€‹çµæœ")
        
        if results:
            print(f"   - ç¬¬ä¸€å€‹çµæœ ID: {results[0].get('id')}")
            print(f"   - ç¬¬ä¸€å€‹çµæœåˆ†æ•¸: {results[0].get('score', 0):.4f}")
        
        print("\nâœ… Protocol Assistant API æ­£å¸¸é‹ä½œï¼")
        print("   ï¼ˆä½¿ç”¨æ¨™æº– search_knowledge è·¯å¾‘ï¼Œä¸å—ç­–ç•¥å¼•æ“å½±éŸ¿ï¼‰")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ API æ¸¬è©¦å¤±æ•—: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»æ¸¬è©¦æµç¨‹"""
    print("\n" + "="*80)
    print("ğŸš€ ç«¯åˆ°ç«¯é©—è­‰æ¸¬è©¦")
    print("="*80)
    
    print("\nç›®æ¨™:")
    print("1. æ¸¬è©¦æ–°ç­–ç•¥å¼•æ“å¯åŸ·è¡Œï¼ˆV3 æ··åˆ 70-30ï¼‰")
    print("2. å°æ¯” Baseline å’Œ V3 çš„çµæœå·®ç•°")
    print("3. ç¢ºèª Protocol Assistant ä¸å—å½±éŸ¿")
    
    # ç²å–æ¸¬è©¦æ¡ˆä¾‹
    test_cases = BenchmarkTestCase.objects.filter(
        is_active=True
    ).order_by('id')[:3]
    
    if not test_cases:
        print("\nâŒ æ²’æœ‰å¯ç”¨çš„æ¸¬è©¦æ¡ˆä¾‹")
        return 1
    
    print(f"\nğŸ“ ä½¿ç”¨ {len(test_cases)} å€‹æ¸¬è©¦æ¡ˆä¾‹")
    
    # 1. æ¸¬è©¦ Baseline
    baseline_result = run_version_test(
        version_id=3,
        version_name="Baseline",
        test_cases=test_cases
    )
    
    # 2. æ¸¬è©¦ V3 (æ··åˆ 70-30)
    v3_result = run_version_test(
        version_id=7,
        version_name="V3 - æ··åˆæ¬Šé‡ 70-30",
        test_cases=test_cases
    )
    
    # 3. å°æ¯”çµæœ
    compare_results(baseline_result, v3_result)
    
    # 4. æ¸¬è©¦ Protocol Assistant API
    api_ok = test_protocol_assistant_api()
    
    # ç¸½çµ
    print("\n" + "="*80)
    print("âœ… ç«¯åˆ°ç«¯é©—è­‰å®Œæˆï¼")
    print("="*80)
    
    print("\nğŸ“‹ é©—è­‰çµæœ:")
    print(f"   {'âœ…' if baseline_result else 'âŒ'} Baseline æ¸¬è©¦æˆåŠŸ")
    print(f"   {'âœ…' if v3_result else 'âŒ'} V3 (æ··åˆ 70-30) æ¸¬è©¦æˆåŠŸ")
    print(f"   {'âœ…' if api_ok else 'âŒ'} Protocol Assistant API æ­£å¸¸")
    
    all_ok = baseline_result and v3_result and api_ok
    
    if all_ok:
        print("\nğŸ‰ æ‰€æœ‰é©—è­‰é€šéï¼")
        print("âœ… æ–°ç­–ç•¥å¼•æ“å¯ä»¥å®‰å…¨ä½¿ç”¨")
        print("âœ… ç¾æœ‰åŠŸèƒ½ä¸å—å½±éŸ¿")
        print("âœ… æº–å‚™å¥½é€²è¡Œå®Œæ•´ Benchmark æ¸¬è©¦ï¼")
    else:
        print("\nâš ï¸  éƒ¨åˆ†é©—è­‰å¤±æ•—ï¼Œéœ€è¦æª¢æŸ¥å•é¡Œ")
    
    print("="*80 + "\n")
    
    return 0 if all_ok else 1


if __name__ == '__main__':
    sys.exit(main())
