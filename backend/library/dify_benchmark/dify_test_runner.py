"""
Dify Test Runner - å–®ç‰ˆæœ¬æ¸¬è©¦åŸ·è¡Œfrom .dify_api_client import DifyAPIClient
from .keyword_evaluator import KeywordEvaluator
from .progress_tracker import BatchTestProgressTracker

logger = logging.getLogger(__name__)
progress_tracker = BatchTestProgressTracker()  # âœ… åˆå§‹åŒ–é€²åº¦è¿½è¹¤å™¨é€”ï¼š
1. åŸ·è¡Œå–®ä¸€ Dify ç‰ˆæœ¬çš„æ‰€æœ‰æ¸¬è©¦æ¡ˆä¾‹
2. å‘¼å« Dify API ç²å–ç­”æ¡ˆ
3. ä½¿ç”¨ KeywordEvaluator é€²è¡Œè©•åˆ†
4. è¨˜éŒ„æ¸¬è©¦çµæœåˆ°è³‡æ–™åº«

æµç¨‹ï¼š
Question â†’ Dify API (with RAG) â†’ Answer â†’ KeywordEvaluator â†’ Score â†’ Database
"""

import logging
import time
import json
import concurrent.futures
from threading import Lock
from typing import List, Dict, Any, Optional
from datetime import datetime
from django.db import transaction
from django.utils import timezone

from api.models import (
    DifyConfigVersion,
    DifyBenchmarkTestCase,
    DifyTestRun,
    DifyTestResult,
    DifyAnswerEvaluation
)
from .dify_api_client import DifyAPIClient
from .evaluators import KeywordEvaluator
from .progress_tracker import BatchTestProgressTracker

logger = logging.getLogger(__name__)
progress_tracker = BatchTestProgressTracker()  # âœ… å…¨å±€å¯¦ä¾‹


class DifyTestRunner:
    """
    å–®ç‰ˆæœ¬æ¸¬è©¦åŸ·è¡Œå™¨
    
    è² è²¬åŸ·è¡Œå–®ä¸€ Dify é…ç½®ç‰ˆæœ¬çš„æ‰€æœ‰æ¸¬è©¦æ¡ˆä¾‹
    
    ä½¿ç”¨æ–¹å¼ï¼š
        runner = DifyTestRunner(
            version=version_instance,
            use_ai_evaluator=False
        )
        
        test_run = runner.run_batch_tests(
            test_cases=test_cases,
            run_name="æ¸¬è©¦æ‰¹æ¬¡ #1",
            batch_id="batch_001"
        )
        
        # è¿”å›ï¼šDifyTestRun å¯¦ä¾‹
    """
    
    def __init__(
        self,
        version: DifyConfigVersion,
        use_ai_evaluator: bool = False,
        api_timeout: int = 75,
        max_workers: int = 10
    ):
        """
        åˆå§‹åŒ–æ¸¬è©¦åŸ·è¡Œå™¨
        
        Args:
            version: Dify é…ç½®ç‰ˆæœ¬
            use_ai_evaluator: æ˜¯å¦ä½¿ç”¨ AI è©•åˆ†å™¨
            api_timeout: API è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰
            max_workers: æœ€å¤§ä¸¦è¡Œç·šç¨‹æ•¸
        """
        # âœ… å¼·åˆ¶é¡å‹è½‰æ›ï¼šç¢ºä¿ max_workers æ˜¯æ•´æ•¸
        self.max_workers = int(max_workers) if max_workers else 10
        
        # é©—è­‰ max_workers ç¯„åœ
        if self.max_workers <= 0:
            logger.warning(f"âš ï¸ max_workers={max_workers} ç„¡æ•ˆï¼Œä½¿ç”¨é è¨­å€¼ 10")
            self.max_workers = 10
        elif self.max_workers > 20:
            logger.warning(f"âš ï¸ max_workers={max_workers} éå¤§ï¼Œé™åˆ¶ç‚º 20")
            self.max_workers = 20
        
        logger.info(f"ğŸ”§ [DifyTestRunner] max_workers å·²è¨­å®šç‚º: {self.max_workers} (type: {type(self.max_workers).__name__})")
        
        self.version = version
        self.use_ai_evaluator = use_ai_evaluator
        
        # âœ… v1.2: æº–å‚™ç‰ˆæœ¬é…ç½®ï¼ˆç”¨æ–¼å¾Œç«¯æœå°‹ï¼‰
        self.version_config = {
            'version_code': version.version_code,
            'version_name': version.version_name,
            'rag_settings': version.rag_settings
        }
        logger.info(
            f"ğŸ“‹ [DifyTestRunner] ç‰ˆæœ¬é…ç½®å·²è¼‰å…¥: "
            f"version={version.version_code}, "
            f"retrieval_mode={version.rag_settings.get('retrieval_mode', 'unknown')}"
        )
        
        # åˆå§‹åŒ– Dify API Client
        self.api_client = DifyAPIClient(timeout=api_timeout)
        
        # åˆå§‹åŒ–è©•åˆ†å™¨
        self.keyword_evaluator = KeywordEvaluator()
        
        # ç·šç¨‹å®‰å…¨çš„è¨ˆæ•¸å™¨ï¼ˆç”¨æ–¼å¤šç·šç¨‹çµ±è¨ˆï¼‰
        self._lock = Lock()
        self._passed_count = 0
        self._failed_count = 0
        self._total_score = 0
        
        logger.info(
            f"DifyTestRunner åˆå§‹åŒ–å®Œæˆ: "
            f"version={version.version_name}, "
            f"max_workers={self.max_workers}, "
            f"evaluator={'AI' if use_ai_evaluator else 'Keyword'}"
        )
    
    def run_batch_tests(
        self,
        test_cases: List[DifyBenchmarkTestCase],
        run_name: str = None,
        batch_id: str = None,
        description: str = None
    ) -> DifyTestRun:
        """
        åŸ·è¡Œæ‰¹é‡æ¸¬è©¦
        
        Args:
            test_cases: æ¸¬è©¦æ¡ˆä¾‹åˆ—è¡¨
            run_name: æ¸¬è©¦æ‰¹æ¬¡åç¨±ï¼ˆå¯é¸ï¼‰
            batch_id: æ‰¹æ¬¡ IDï¼ˆå¯é¸ï¼Œç”¨æ–¼å¤šç‰ˆæœ¬å°æ¯”ï¼‰
            description: æ¸¬è©¦æè¿°ï¼ˆå¯é¸ï¼‰
        
        Returns:
            DifyTestRun å¯¦ä¾‹
        """
        try:
            # 1. å‰µå»º Test Run è¨˜éŒ„
            test_run = self._create_test_run(
                test_cases=test_cases,
                run_name=run_name,
                batch_id=batch_id,
                description=description
            )
            
            logger.info(
                f"é–‹å§‹åŸ·è¡Œæ¸¬è©¦: "
                f"run_id={test_run.id}, "
                f"version={self.version.version_name}, "
                f"total_cases={len(test_cases)}"
            )
            
            # 2. åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦æ¡ˆä¾‹
            passed_count = 0
            failed_count = 0
            total_score = 0
            
            for i, test_case in enumerate(test_cases, 1):
                try:
                    logger.info(f"åŸ·è¡Œæ¸¬è©¦æ¡ˆä¾‹ {i}/{len(test_cases)}: {test_case.question[:50]}")
                    
                    # åŸ·è¡Œå–®å€‹æ¸¬è©¦
                    result = self._run_single_test(test_run, test_case)
                    
                    # çµ±è¨ˆçµæœ
                    if result.is_passed:
                        passed_count += 1
                    else:
                        failed_count += 1
                    
                    total_score += result.score
                    
                    logger.info(
                        f"æ¸¬è©¦æ¡ˆä¾‹å®Œæˆ: "
                        f"score={result.score}, "
                        f"passed={'âœ…' if result.is_passed else 'âŒ'}"
                    )
                    
                except Exception as e:
                    logger.error(f"æ¸¬è©¦æ¡ˆä¾‹åŸ·è¡Œå¤±æ•— (æ¡ˆä¾‹ {i}): {str(e)}", exc_info=True)
                    failed_count += 1
            
            # 3. æ›´æ–° Test Run çµ±è¨ˆ
            self._update_test_run_statistics(
                test_run=test_run,
                passed_count=passed_count,
                failed_count=failed_count,
                total_score=total_score
            )
            
            logger.info(
                f"æ¸¬è©¦åŸ·è¡Œå®Œæˆ: "
                f"passed={passed_count}/{len(test_cases)}, "
                f"avg_score={test_run.average_score:.2f}, "
                f"pass_rate={test_run.pass_rate:.2f}%"
            )
            
            return test_run
            
        except Exception as e:
            logger.error(f"æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {str(e)}", exc_info=True)
            raise
    
    def run_batch_tests_parallel(
        self,
        test_cases: List[DifyBenchmarkTestCase],
        run_name: str = None,
        batch_id: str = None,
        description: str = None
    ) -> DifyTestRun:
        """
        ã€å¤šç·šç¨‹ä¸¦è¡Œã€‘åŸ·è¡Œæ‰¹é‡æ¸¬è©¦
        
        ä½¿ç”¨ ThreadPoolExecutor ä¸¦è¡ŒåŸ·è¡Œæ¸¬è©¦æ¡ˆä¾‹ï¼Œå¤§å¹…æå‡æ¸¬è©¦é€Ÿåº¦ã€‚
        æ¯å€‹æ¸¬è©¦ä½¿ç”¨ç¨ç«‹çš„ conversation_idï¼Œç¢ºä¿æ¸¬è©¦éš”é›¢ã€‚
        
        Args:
            test_cases: æ¸¬è©¦æ¡ˆä¾‹åˆ—è¡¨
            run_name: æ¸¬è©¦æ‰¹æ¬¡åç¨±ï¼ˆå¯é¸ï¼‰
            batch_id: æ‰¹æ¬¡ IDï¼ˆå¯é¸ï¼Œç”¨æ–¼å¤šç‰ˆæœ¬å°æ¯”ï¼‰
            description: æ¸¬è©¦æè¿°ï¼ˆå¯é¸ï¼‰
        
        Returns:
            DifyTestRun å¯¦ä¾‹
        
        æ•ˆèƒ½æå‡ï¼š
            - 10 å€‹æ¸¬è©¦ï¼š30 ç§’ â†’ 6 ç§’ï¼ˆ80% æå‡ï¼‰
            - 50 å€‹æ¸¬è©¦ï¼š150 ç§’ â†’ 30 ç§’ï¼ˆ80% æå‡ï¼‰
        """
        try:
            # 1. å‰µå»º Test Run è¨˜éŒ„
            test_run = self._create_test_run(
                test_cases=test_cases,
                run_name=run_name,
                batch_id=batch_id,
                description=description
            )
            
            # é‡ç½®ç·šç¨‹å®‰å…¨è¨ˆæ•¸å™¨
            with self._lock:
                self._passed_count = 0
                self._failed_count = 0
                self._total_score = 0
            
            logger.info(
                f"é–‹å§‹ä¸¦è¡Œæ¸¬è©¦: "
                f"run_id={test_run.id}, "
                f"version={self.version.version_name}, "
                f"total_cases={len(test_cases)}, "
                f"max_workers={self.max_workers}"
            )
            
            start_time = time.time()
            
            # 2. ä½¿ç”¨ ThreadPoolExecutor ä¸¦è¡ŒåŸ·è¡Œ
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                # æäº¤æ‰€æœ‰æ¸¬è©¦ä»»å‹™
                future_to_case = {
                    executor.submit(
                        self._run_single_test_thread_safe,
                        test_run,
                        test_case,
                        i
                    ): test_case
                    for i, test_case in enumerate(test_cases, 1)
                }
                
                # ç­‰å¾…æ‰€æœ‰ä»»å‹™å®Œæˆ
                for future in concurrent.futures.as_completed(future_to_case):
                    test_case = future_to_case[future]
                    try:
                        result = future.result()
                        logger.info(
                            f"æ¸¬è©¦æ¡ˆä¾‹å®Œæˆ: "
                            f"question={test_case.question[:30]}..., "
                            f"score={result.score}, "
                            f"passed={'âœ…' if result.is_passed else 'âŒ'}"
                        )
                    except Exception as e:
                        logger.error(
                            f"æ¸¬è©¦æ¡ˆä¾‹åŸ·è¡Œå¤±æ•—: "
                            f"question={test_case.question[:30]}..., "
                            f"error={str(e)}"
                        )
            
            execution_time = time.time() - start_time
            
            # 3. æ›´æ–° Test Run çµ±è¨ˆ
            self._update_test_run_statistics(
                test_run=test_run,
                passed_count=self._passed_count,
                failed_count=self._failed_count,
                total_score=self._total_score
            )
            
            logger.info(
                f"ä¸¦è¡Œæ¸¬è©¦å®Œæˆ: "
                f"passed={self._passed_count}/{len(test_cases)}, "
                f"avg_score={test_run.average_score:.2f}, "
                f"pass_rate={test_run.pass_rate:.2f}%, "
                f"execution_time={execution_time:.2f}s"
            )
            
            return test_run
            
        except Exception as e:
            logger.error(f"ä¸¦è¡Œæ¸¬è©¦åŸ·è¡Œå¤±æ•—: {str(e)}", exc_info=True)
            raise
    
    def _run_single_test_thread_safe(
        self,
        test_run: DifyTestRun,
        test_case: DifyBenchmarkTestCase,
        index: int
    ) -> DifyTestResult:
        """
        ã€ç·šç¨‹å®‰å…¨ã€‘åŸ·è¡Œå–®å€‹æ¸¬è©¦æ¡ˆä¾‹
        
        é—œéµç‰¹æ€§ï¼š
        1. ä½¿ç”¨å”¯ä¸€çš„ user_idï¼ˆåŒ…å«æ¸¬è©¦æ‰¹æ¬¡å’Œåºè™Ÿï¼‰
        2. æ¯æ¬¡ä½¿ç”¨æ–°çš„ conversation_idï¼ˆNoneï¼‰
        3. ç·šç¨‹å®‰å…¨çš„çµ±è¨ˆæ›´æ–°ï¼ˆä½¿ç”¨ Lockï¼‰
        4. å®Œå…¨éš”é›¢ï¼Œä¸å½±éŸ¿ Protocol Assistant
        5. âœ… v1.3: æ”¯æ´ SmartSearchRouterï¼ˆèˆ‡ Web ä¸€è‡´çš„å…©éšæ®µæœå°‹ï¼‰
        
        Args:
            test_run: æ¸¬è©¦æ‰¹æ¬¡å¯¦ä¾‹
            test_case: æ¸¬è©¦æ¡ˆä¾‹å¯¦ä¾‹
            index: æ¸¬è©¦æ¡ˆä¾‹åºè™Ÿï¼ˆ1-basedï¼‰
        
        Returns:
            DifyTestResult å¯¦ä¾‹
        """
        
        # ç”Ÿæˆå”¯ä¸€çš„ user_idï¼ˆå€åˆ†æ¸¬è©¦èˆ‡æ­£å¸¸ç”¨æˆ¶ï¼‰
        unique_user_id = f"benchmark_test_{test_run.id}_{index}"
        
        logger.info(
            f"[Thread {index}] é–‹å§‹æ¸¬è©¦: "
            f"question={test_case.question[:50]}..., "
            f"user_id={unique_user_id}"
        )
        
        try:
            # âœ… v1.3: æª¢æŸ¥æ˜¯å¦ä½¿ç”¨ SmartSearchRouterï¼ˆèˆ‡ Web å®Œå…¨ä¸€è‡´ï¼‰
            use_smart_router = self.version_config.get('rag_settings', {}).get('use_smart_router', False)
            
            if use_smart_router:
                # ä½¿ç”¨ SmartSearchRouterï¼ˆèˆ‡ Web Protocol Assistant å®Œå…¨ä¸€è‡´ï¼‰
                logger.info(f"[Thread {index}] ğŸ”„ ä½¿ç”¨ SmartSearchRouterï¼ˆèˆ‡ Web ä¸€è‡´ï¼‰")
                api_response = self.api_client.send_question_with_smart_router(
                    question=test_case.question,
                    user_id=unique_user_id,
                    conversation_id=None
                )
            else:
                # âœ… v1.2: å‘¼å« Dify APIï¼ˆå‚³éç‰ˆæœ¬é…ç½®ä»¥ä½¿ç”¨å¾Œç«¯æœå°‹ï¼‰
                api_response = self.api_client.send_question(
                    question=test_case.question,
                    user_id=unique_user_id,      # âœ… å”¯ä¸€ user_id
                    conversation_id=None,        # âœ… æ¯æ¬¡æ–°å°è©±
                    version_config=self.version_config  # âœ… v1.2 æ–°å¢ï¼šå‚³éç‰ˆæœ¬é…ç½®
                )
            
            # æå–è³‡è¨Š
            actual_answer = api_response.get('answer', '')
            response_time = api_response.get('response_time', 0)
            dify_conversation_id = api_response.get('conversation_id', '')
            dify_message_id = api_response.get('message_id', '')
            retrieved_documents = api_response.get('retrieved_documents', [])
            backend_search_used = api_response.get('backend_search_used', False)  # âœ… v1.2 æ–°å¢
            search_results_count = api_response.get('search_results_count', 0)  # âœ… v1.2 æ–°å¢
            smart_router_used = api_response.get('smart_router_used', False)  # âœ… v1.3 æ–°å¢
            
            # âœ… v1.3: è¨˜éŒ„ SmartRouter ä½¿ç”¨ç‹€æ…‹
            if smart_router_used:
                logger.info(
                    f"[Thread {index}] ğŸŒŸ ä½¿ç”¨ SmartRouter: "
                    f"mode={api_response.get('search_mode', 'unknown')}, "
                    f"stage={api_response.get('search_stage', 0)}, "
                    f"fallback={api_response.get('is_fallback', False)}"
                )
            # âœ… v1.2: è¨˜éŒ„å¾Œç«¯æœå°‹ç‹€æ…‹
            elif backend_search_used:
                logger.info(
                    f"[Thread {index}] ğŸŒŸ ä½¿ç”¨å¾Œç«¯æœå°‹: "
                    f"results={search_results_count}, "
                    f"version={self.version.version_code}"
                )
            
            # 2. ä½¿ç”¨ KeywordEvaluator è©•åˆ†
            keywords = test_case.answer_keywords  # âœ… ç›´æ¥è¨ªå• JSONField æ¬„ä½
            
            evaluation_result = self.keyword_evaluator.evaluate(
                question=test_case.question,
                expected_answer=test_case.expected_answer,
                actual_answer=actual_answer,
                keywords=keywords
            )
            
            score = evaluation_result['score']
            is_passed = evaluation_result['is_passed']
            matched_keywords = evaluation_result['matched_keywords']
            missing_keywords = evaluation_result['missing_keywords']
            
            # 3. å„²å­˜ TestResultï¼ˆDjango ORM æ˜¯ç·šç¨‹å®‰å…¨çš„ï¼‰
            test_result = DifyTestResult.objects.create(
                test_run=test_run,
                test_case=test_case,
                dify_answer=actual_answer,  # âœ… æ­£ç¢ºæ¬„ä½å
                dify_message_id=dify_message_id,
                score=score,
                is_passed=is_passed,
                response_time=response_time,
                matched_keywords=matched_keywords,
                missing_keywords=missing_keywords
                # dify_conversation_id å’Œ retrieved_documents_count æ¬„ä½ä¸å­˜åœ¨ï¼Œç§»é™¤
            )
            
            # 4. å„²å­˜ AnswerEvaluation
            DifyAnswerEvaluation.objects.create(
                test_result=test_result,
                question=test_case.question,
                expected_answer=test_case.expected_answer or "",
                actual_answer=actual_answer,
                evaluator_model='keyword_only',
                scores={
                    'overall_score': score,
                    'is_passed': is_passed,
                    'matched_keywords': matched_keywords,
                    'missing_keywords': missing_keywords,
                    'total_keywords': len(keywords),
                    'matched_count': len(matched_keywords),
                    'missing_count': len(missing_keywords)
                },
                feedback=f"é—œéµå­—åŒ¹é…: {len(matched_keywords)}/{len(keywords)}"
            )
            
            # 5. ç·šç¨‹å®‰å…¨åœ°æ›´æ–°çµ±è¨ˆï¼ˆä½¿ç”¨ Lockï¼‰
            with self._lock:
                if is_passed:
                    self._passed_count += 1
                else:
                    self._failed_count += 1
                self._total_score += score
            
            # 6. âœ… æ›´æ–°é€²åº¦è¿½è¹¤å™¨ï¼ˆæ¯å€‹æ¸¬è©¦å®Œæˆå¾Œç«‹å³æ›´æ–°ï¼‰
            # ğŸ” èª¿è©¦ï¼šæª¢æŸ¥ batch_id ç‹€æ…‹
            logger.info(
                f"ğŸ” [DEBUG] test_run.batch_id ç‹€æ…‹: "
                f"hasattr={hasattr(test_run, 'batch_id')}, "
                f"value='{test_run.batch_id if hasattr(test_run, 'batch_id') else 'N/A'}', "
                f"type={type(test_run.batch_id) if hasattr(test_run, 'batch_id') else 'N/A'}"
            )
            
            if hasattr(test_run, 'batch_id') and test_run.batch_id:
                # ä½¿ç”¨ completed_tests=1 è¡¨ç¤ºå®Œæˆ 1 å€‹æ¸¬è©¦ï¼ˆæœƒè‡ªå‹•ç´¯åŠ ï¼‰
                progress_tracker.update_progress(
                    batch_id=test_run.batch_id,
                    completed_tests=1,  # æ¯æ¬¡å¢åŠ  1
                    failed_tests=1 if not is_passed else 0,
                    current_test_case=test_case.question[:50]  # é¡¯ç¤ºç•¶å‰æ¸¬è©¦æ¡ˆä¾‹
                )
                
                logger.info(
                    f"[Thread {index}] ğŸ“Š é€²åº¦å·²æ›´æ–°: "
                    f"batch_id={test_run.batch_id}, "
                    f"test_case={test_case.question[:30]}..."
                )
            else:
                logger.warning(
                    f"âš ï¸ [Thread {index}] batch_id ç‚ºç©ºæˆ–ä¸å­˜åœ¨ï¼Œç„¡æ³•æ›´æ–°é€²åº¦ï¼"
                )
            
            logger.info(
                f"[Thread {index}] æ¸¬è©¦å®Œæˆ: "
                f"score={score}, "
                f"passed={'âœ…' if is_passed else 'âŒ'}, "
                f"response_time={response_time:.2f}s"
            )
            
            return test_result
            
        except Exception as e:
            logger.error(f"[Thread {index}] æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {str(e)}", exc_info=True)
            
            # çµ±è¨ˆå¤±æ•—æ¬¡æ•¸
            with self._lock:
                self._failed_count += 1
            
            raise
    
    def _create_test_run(
        self,
        test_cases: List[DifyBenchmarkTestCase],
        run_name: str,
        batch_id: str,
        description: str
    ) -> DifyTestRun:
        """å‰µå»º Test Run è¨˜éŒ„"""
        
        # ç”Ÿæˆé è¨­åç¨±
        if not run_name:
            run_name = f"{self.version.version_name} - {datetime.now().strftime('%Y-%m-%d %H:%M')}"
        
        # å‰µå»ºæ¸¬è©¦åŸ·è¡Œè¨˜éŒ„ï¼ˆä½¿ç”¨æ­£ç¢ºçš„æ¬„ä½åç¨±ï¼‰
        test_run = DifyTestRun.objects.create(
            version=self.version,
            run_name=run_name,
            batch_id=batch_id or '',
            total_test_cases=len(test_cases),  # âœ… æ­£ç¢ºæ¬„ä½å
            # description å’Œ status æ¬„ä½ä¸å­˜åœ¨ï¼Œç§»é™¤
        )
        
        return test_run
    
    def _run_single_test(
        self,
        test_run: DifyTestRun,
        test_case: DifyBenchmarkTestCase
    ) -> DifyTestResult:
        """
        åŸ·è¡Œå–®å€‹æ¸¬è©¦æ¡ˆä¾‹
        
        æµç¨‹ï¼š
        1. å‘¼å« Dify API ç²å–ç­”æ¡ˆï¼ˆæ”¯æ´ SmartSearchRouterï¼‰
        2. ä½¿ç”¨ KeywordEvaluator è©•åˆ†
        3. å„²å­˜ TestResult å’Œ AnswerEvaluation
        
        âœ… v1.3: æ”¯æ´ use_smart_router é¸é …
        """
        
        # âœ… v1.3: æª¢æŸ¥æ˜¯å¦ä½¿ç”¨ SmartSearchRouter
        use_smart_router = self.version_config.get('rag_settings', {}).get('use_smart_router', False)
        
        # 1. å‘¼å« Dify API
        if use_smart_router:
            # ä½¿ç”¨ SmartSearchRouterï¼ˆèˆ‡ Web Protocol Assistant å®Œå…¨ä¸€è‡´ï¼‰
            logger.info(f"ğŸ”„ ä½¿ç”¨ SmartSearchRouterï¼ˆèˆ‡ Web ä¸€è‡´ï¼‰")
            api_response = self.api_client.send_question_with_smart_router(
                question=test_case.question,
                user_id=f"test_run_{test_run.id}",
                conversation_id=None
            )
        else:
            # ä½¿ç”¨åŸæœ‰çš„ send_question æ–¹æ³•
            api_response = self.api_client.send_question(
                question=test_case.question,
                user_id=f"test_run_{test_run.id}",
                conversation_id=None  # æ¯å€‹æ¸¬è©¦æ¡ˆä¾‹ä½¿ç”¨ç¨ç«‹å°è©±
            )
        
        # æå–è³‡è¨Š
        actual_answer = api_response.get('answer', '')
        response_time = api_response.get('response_time', 0)
        dify_conversation_id = api_response.get('conversation_id', '')
        dify_message_id = api_response.get('message_id', '')
        retrieved_documents = api_response.get('retrieved_documents', [])
        
        # âœ… v1.3: è¨˜éŒ„ SmartRouter è³‡è¨Š
        if use_smart_router:
            logger.info(
                f"SmartRouter çµæœ: "
                f"mode={api_response.get('search_mode', 'unknown')}, "
                f"stage={api_response.get('search_stage', 0)}, "
                f"fallback={api_response.get('is_fallback', False)}"
            )
        
        # 2. ä½¿ç”¨ KeywordEvaluator è©•åˆ†
        keywords = test_case.answer_keywords  # âœ… ç›´æ¥è¨ªå• JSONField æ¬„ä½
        
        evaluation_result = self.keyword_evaluator.evaluate(
            question=test_case.question,
            expected_answer=test_case.expected_answer,
            actual_answer=actual_answer,
            keywords=keywords
        )
        
        score = evaluation_result['score']
        is_passed = evaluation_result['is_passed']
        matched_keywords = evaluation_result['matched_keywords']
        missing_keywords = evaluation_result['missing_keywords']
        
        # 3. å„²å­˜ TestResult
        test_result = DifyTestResult.objects.create(
            test_run=test_run,
            test_case=test_case,
            dify_answer=actual_answer,  # âœ… æ­£ç¢ºæ¬„ä½å
            dify_message_id=dify_message_id,
            score=score,
            is_passed=is_passed,
            response_time=response_time,
            matched_keywords=matched_keywords,
            missing_keywords=missing_keywords
            # dify_conversation_id å’Œ retrieved_documents_count æ¬„ä½ä¸å­˜åœ¨ï¼Œç§»é™¤
        )
        
        # 4. å„²å­˜ AnswerEvaluation
        DifyAnswerEvaluation.objects.create(
            test_result=test_result,
            question=test_case.question,
            expected_answer=test_case.expected_answer or "",
            actual_answer=actual_answer,
            evaluator_model='keyword_only',
            scores={
                'overall_score': score,
                'is_passed': is_passed,
                'matched_keywords': matched_keywords,
                'missing_keywords': missing_keywords,
                'match_details': evaluation_result.get('match_details', {}),
                'total_keywords': len(keywords),
                'matched_count': len(matched_keywords),
                'missing_count': len(missing_keywords)
            },
            feedback=f"é—œéµå­—åŒ¹é…: {len(matched_keywords)}/{len(keywords)}"
        )
        
        return test_result
    
    def _update_test_run_statistics(
        self,
        test_run: DifyTestRun,
        passed_count: int,
        failed_count: int,
        total_score: int
    ):
        """æ›´æ–° Test Run çµ±è¨ˆè³‡æ–™"""
        
        total_cases = passed_count + failed_count
        
        test_run.passed_cases = passed_count
        test_run.failed_cases = failed_count
        test_run.pass_rate = (passed_count / total_cases * 100) if total_cases > 0 else 0
        test_run.average_score = (total_score / total_cases) if total_cases > 0 else 0
        # status æ¬„ä½ä¸å­˜åœ¨æ–¼ Modelï¼Œç§»é™¤
        test_run.completed_at = timezone.now()
        test_run.save()
    
    def get_test_summary(self, test_run: DifyTestRun) -> Dict[str, Any]:
        """
        ç²å–æ¸¬è©¦æ‘˜è¦
        
        Returns:
            {
                'run_id': int,
                'version_name': str,
                'total_cases': int,
                'passed_cases': int,
                'failed_cases': int,
                'pass_rate': float,
                'average_score': float,
                'duration': float (seconds)
            }
        """
        duration = 0
        if test_run.completed_at and test_run.started_at:
            duration = (test_run.completed_at - test_run.started_at).total_seconds()
        
        return {
            'run_id': test_run.id,
            'version_name': self.version.version_name,
            'total_cases': test_run.total_test_cases,  # âœ… ä½¿ç”¨æ­£ç¢ºæ¬„ä½å
            'passed_cases': test_run.passed_cases,
            'failed_cases': test_run.failed_cases,
            'pass_rate': test_run.pass_rate,
            'average_score': test_run.average_score,
            'duration': round(duration, 2),
            # status æ¬„ä½ä¸å­˜åœ¨ï¼Œç§»é™¤
        }
